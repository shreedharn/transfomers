# Mathematical Quick Reference for Neural Networks

A comprehensive reference table of core mathematical concepts used in neural networks and deep learning, ordered from foundational to advanced topics.

| Mathematical Category | Concept | Example Formula | Purpose & Why It Makes Sense in Neural Networks |
|----------------------|---------|----------------|--------------------------------------------------|
| **Linear Algebra** | Matrix Multiplication | $\mathbf{C} = \mathbf{A}\mathbf{B}$ where $C_{ij} = \sum_k A_{ik}B_{kj}$ | The fundamental operation of neural networks. Each row of $\mathbf{A}$ represents neuron weights, each column of $\mathbf{B}$ represents input vectors. The result computes weighted sums for all neurons simultaneously, enabling parallel computation. This is why a single matrix multiplication can represent an entire layer's forward pass. |
| **Linear Algebra** | Matrix Transpose | $(\mathbf{A})^T_{ij} = \mathbf{A}_{ji}$ | Essential for backpropagation. When gradients flow backward through a layer with weights $\mathbf{W}$, we need $\mathbf{W}^T$ to properly route the gradient signals back to the previous layer. The transpose "reverses" the forward direction of information flow. |
| **Linear Algebra** | Matrix Inverse | $\mathbf{A}\mathbf{A}^{-1} = \mathbf{I}$ | Used in analytical solutions for least squares (normal equations) and understanding linear transformations. In neural networks, helps analyze layer transformations and appears in second-order optimization methods like Newton's method. |
| **Linear Algebra** | Eigenvalues & Eigenvectors | $\mathbf{A}\mathbf{v} = \lambda\mathbf{v}$ | Reveals principal directions of data variation (PCA), helps analyze gradient flow and conditioning of weight matrices. Large eigenvalues can indicate exploding gradients, while small ones suggest vanishing gradients. Critical for understanding optimization landscapes. |
| **Linear Algebra** | Singular Value Decomposition | $\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T$ | Decomposes any matrix into orthogonal transformations and scaling. Used in dimensionality reduction, weight initialization, and analyzing the effective rank of learned representations. Helps understand what transformations neural network layers are actually learning. |
| **Vectors & Geometry** | Dot Product | $\mathbf{a} \cdot \mathbf{b} = \sum_i a_i b_i = \|\mathbf{a}\|\|\mathbf{b}\|\cos(\theta)$ | Measures how "aligned" two vectors are. In neurons, the dot product between input $\mathbf{x}$ and weights $\mathbf{w}$ gives the raw activation strength—high when input pattern matches what the neuron is looking for. This is the core operation that determines neuron firing. |
| **Vectors & Geometry** | Cosine Similarity | $\cos(\theta) = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\|\|\mathbf{b}\|}$ | Measures similarity independent of magnitude. Used in attention mechanisms, word embeddings, and similarity-based learning. Helps neural networks focus on directional patterns rather than absolute magnitudes, making them more robust to scaling variations. |
| **Vectors & Geometry** | Euclidean Distance | $d(\mathbf{a}, \mathbf{b}) = \|\mathbf{a} - \mathbf{b}\|_2 = \sqrt{\sum_i (a_i - b_i)^2}$ | Measures how "far apart" two points are in feature space. Used in loss functions (MSE), clustering, and nearest neighbor methods. In neural networks, helps measure prediction errors and similarity between representations. |
| **Vectors & Geometry** | $L_p$ Norms | $\|\mathbf{x}\|_p = \left(\sum_i |x_i|^p\right)^{1/p}$ | Measures vector "size" in different ways. $L_1$ promotes sparsity (many weights become zero), $L_2$ promotes smoothness (weights stay small). Used in regularization to control model complexity and prevent overfitting by penalizing large weights. |
| **Calculus** | Derivative | $f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$ | Measures how fast a function changes. In neural networks, tells us how much the loss changes when we tweak a parameter. This is the foundation of gradient-based learning—we follow the derivative to find better parameter values. |
| **Calculus** | Partial Derivative | $\frac{\partial f}{\partial x_i}$ | Derivative with respect to one variable while holding others constant. Neural networks have millions of parameters, so we need partial derivatives to see how the loss changes with respect to each individual weight or bias. |
| **Calculus** | Chain Rule | $\frac{d}{dx}f(g(x)) = f'(g(x)) \cdot g'(x)$ | The mathematical foundation of backpropagation. Neural networks are compositions of functions (layer after layer), so to compute gradients we multiply derivatives along the chain from output back to input. This is why it's called "backpropagation"—propagating derivatives backward through the composition. |
| **Calculus** | Gradient | $\nabla f = \left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n}\right]$ | Points in the direction of steepest increase. In neural network training, we move parameters in the negative gradient direction (steepest decrease) to minimize the loss function. The gradient tells us both direction and magnitude of the best parameter update. |
| **Calculus** | Hessian | $\mathbf{H}_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$ | Matrix of second derivatives that describes the curvature of the loss surface. Helps understand convergence behavior and is used in advanced optimization methods like Newton's method and natural gradients. High curvature areas require smaller learning rates. |
| **Calculus** | Jacobian | $\mathbf{J}_{ij} = \frac{\partial f_i}{\partial x_j}$ | Matrix of first derivatives for vector-valued functions. Essential for backpropagation through layers that output vectors (like hidden layers). Each element shows how one output component changes with respect to one input component, enabling gradient flow through complex architectures. |
| **Differential Equations** | Ordinary Differential Equations (ODEs) | $\frac{dy}{dt} = f(y, t)$ | Models how quantities change over time. Neural ODEs treat layer depth as continuous time, allowing adaptive depth and memory-efficient training. ResNets approximate the solution to ODEs, explaining why skip connections work so well for deep networks. |
| **Differential Equations** | Partial Differential Equations (PDEs) | $\frac{\partial u}{\partial t} = f\left(u, \frac{\partial u}{\partial x}, \frac{\partial^2 u}{\partial x^2}, \ldots\right)$ | Models complex spatiotemporal phenomena. Physics-informed neural networks (PINNs) embed PDE constraints directly into the loss function, allowing neural networks to solve scientific computing problems while respecting physical laws. |
| **Nonlinear Functions** | Hyperbolic Tangent | $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ | Activation function that squashes inputs to $(-1, 1)$. Provides nonlinearity needed for complex patterns while keeping outputs bounded. The S-shape introduces smooth nonlinear decision boundaries, and its zero-centered output helps with gradient flow compared to sigmoid. |
| **Nonlinear Functions** | Sigmoid | $\sigma(x) = \frac{1}{1 + e^{-x}}$ | Squashes inputs to $(0, 1)$, naturally interpreted as probabilities. Used in binary classification and gating mechanisms (LSTM gates). However, suffers from vanishing gradients at extremes, which is why ReLU became more popular in deep networks. |
| **Nonlinear Functions** | ReLU | $\text{ReLU}(x) = \max(0, x)$ | Simple nonlinearity that sets negative values to zero. Solves vanishing gradient problem because gradient is either 0 or 1. Promotes sparsity (many neurons inactive) which makes networks more interpretable and efficient. Biologically inspired by neuron firing thresholds. |
| **Probability & Statistics** | Expectation | $\mathbb{E}[X] = \sum_x x \cdot P(X = x)$ | Average value of a random variable. In neural networks, we often work with expected loss over data distributions. Batch statistics, dropout, and stochastic optimization all rely on expectation to handle randomness in training and make models robust to unseen data. |
| **Probability & Statistics** | Variance | $\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]$ | Measures spread of a distribution. Critical for weight initialization (to prevent vanishing/exploding gradients) and batch normalization (to stabilize training). Understanding variance helps design networks that maintain good signal propagation through many layers. |
| **Probability & Statistics** | Softmax | $\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$ | Converts real values to probability distribution. Essential for multi-class classification and attention mechanisms. The exponential amplifies differences while ensuring outputs sum to 1, creating a "soft" version of selecting the maximum value that's differentiable for gradient-based learning. |
| **Probability & Statistics** | Cross-Entropy Loss | $\mathcal{L} = -\sum_i y_i \log(\hat{y}_i)$ | Measures difference between predicted and true probability distributions. Natural loss function for classification because it heavily penalizes confident wrong predictions. Mathematically connected to maximum likelihood estimation and information theory—minimizing cross-entropy maximizes the likelihood of correct predictions. |
| **Probability & Statistics** | KL Divergence | $D_{KL}(P \| Q) = \sum_i P(i) \log\frac{P(i)}{Q(i)}$ | Measures how different two probability distributions are. Used in variational autoencoders, regularization, and knowledge distillation. Helps neural networks match target distributions and quantifies the information lost when approximating one distribution with another. |
| **Optimization** | Gradient Descent | $\theta_{t+1} = \theta_t - \alpha \nabla_\theta \mathcal{L}(\theta_t)$ | Iteratively moves parameters in direction of steepest loss decrease. The fundamental learning algorithm for neural networks. The learning rate $\alpha$ controls step size—too large causes instability, too small causes slow convergence. Modern variants (Adam, RMSprop) adapt the learning rate automatically. |
| **Optimization** | Mean Squared Error | $\text{MSE} = \frac{1}{n}\sum_i (y_i - \hat{y}_i)^2$ | Measures average squared difference between predictions and targets. Natural for regression problems because it's differentiable and penalizes large errors more than small ones. Related to Gaussian likelihood assumption and provides smooth gradients for optimization. |
| **Optimization** | L1 Regularization | $\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{data}} + \lambda \sum_i |\theta_i|$ | Adds penalty proportional to absolute value of parameters. Promotes sparsity—many weights become exactly zero, effectively performing feature selection. Creates "sharp" penalty that encourages simple models and improves interpretability by eliminating unimportant connections. |
| **Optimization** | L2 Regularization | $\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{data}} + \lambda \sum_i \theta_i^2$ | Adds penalty proportional to squared parameters. Keeps weights small and smooth, preventing overfitting. Mathematically equivalent to Gaussian prior on weights and provides smooth gradients. Unlike L1, doesn't force weights to exactly zero but shrinks them toward zero proportionally. |
| **Information Theory** | Entropy | $H(X) = -\sum_i P(x_i) \log P(x_i)$ | Measures uncertainty or information content in a distribution. Lower entropy means more predictable. In neural networks, we often want to minimize prediction entropy (be confident) while maximizing representation entropy (capture diverse patterns). Connects machine learning to fundamental information theory. |
| **Information Theory** | Mutual Information | $I(X;Y) = \sum_{x,y} P(x,y) \log\frac{P(x,y)}{P(x)P(y)}$ | Measures how much information one variable provides about another. Used in representation learning to ensure learned features capture relevant information about targets while being diverse. Helps design neural networks that learn maximally informative representations without redundancy. |
| **Attention Mechanisms** | Scaled Dot-Product Attention | $\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$ | The core operation of transformers. Queries (Q) search through keys (K) to find relevant information, then retrieve corresponding values (V). The scaling by $\sqrt{d_k}$ prevents attention from becoming too sharp/peaked as dimensions grow, maintaining good gradient flow and distributed attention weights. |
| **Attention Mechanisms** | Multi-Head Attention | $\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$ where $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$ | Allows the model to attend to different types of information simultaneously. Each head can learn different attention patterns (syntax, semantics, etc.). Like having multiple specialized search engines running in parallel, then combining their results for a richer understanding. |
| **Transformer Components** | Layer Normalization | $\text{LayerNorm}(x) = \frac{x - \mu}{\sigma} \odot \gamma + \beta$ where $\mu, \sigma$ computed per sample | Normalizes activations within each sample to have zero mean and unit variance. Unlike batch normalization, works independently for each sample, making it stable for variable sequence lengths and small batch sizes. Helps with gradient flow and training stability. |
| **Transformer Components** | Residual Connections | $\mathbf{h}_{l+1} = \mathbf{h}_l + F(\mathbf{h}_l)$ | Creates "gradient highways" that allow information to flow directly through the network. Essential for training very deep networks (like transformers with many layers) by preventing vanishing gradients. Acts like a safety net - even if some layers learn poorly, information can still reach the output. |
| **Transformer Components** | Positional Encoding | $PE_{(pos,2i)} = \sin(pos/10000^{2i/d})$ and $PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d})$ | Injects information about token position into the model since attention is permutation-invariant. Uses sinusoidal functions so the model can learn to attend by relative position. Without this, "cat sat on mat" would be processed identically to "mat on sat cat". |
| **Optimization** | Adam Optimizer | $m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$, $v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$, $\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{v_t} + \epsilon}\hat{m}_t$ | Adaptive learning rate optimizer that maintains running averages of gradients (momentum) and squared gradients (variance). Automatically adjusts learning rates per parameter based on historical gradients. Like cruise control for optimization - speeds up in flat areas, slows down in steep areas. |
| **Optimization** | Learning Rate Scheduling | $\eta_t = \eta_0 \cdot \min\left(t^{-0.5}, t \cdot \text{warmup}^{-1.5}\right)$ (warmup schedule) | Gradually increases learning rate during initial training (warmup) then decreases it. Prevents early instability when weights are randomly initialized while allowing fast learning once the model stabilizes. Critical for training large transformer models successfully. |
| **Regularization** | Dropout | $\text{Dropout}(x) = x \odot \text{Bernoulli}(1-p) / (1-p)$ | Randomly sets elements to zero during training to prevent overfitting. Forces the model to not rely too heavily on any single neuron or connection. Like cross-training in sports - if some players are "injured" (dropped out), others must step up, making the whole team more robust. |
| **Regularization** | Attention Dropout | Applied to $\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)$ before multiplying by V | Randomly zeros out some attention weights during training. Prevents the model from becoming over-reliant on specific attention patterns and encourages learning more diverse attention strategies. Particularly important for transformer generalization. |
| **Optimization** | Weight Initialization | Xavier/Glorot: $\mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)$, He: $\mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)$ | Sets initial weights to maintain proper signal propagation through the network. Xavier for linear/tanh layers, He for ReLU layers. Wrong initialization can cause vanishing/exploding gradients from the very first forward pass, preventing learning entirely. |
| **Advanced Concepts** | Temperature Scaling | $\text{softmax}(z/\tau)$ where $\tau$ is temperature | Controls the "sharpness" of probability distributions. Lower temperature makes the distribution more peaked (confident), higher temperature makes it more uniform (uncertain). Used in generation for creativity control and in calibration to match predicted confidence with actual accuracy. |