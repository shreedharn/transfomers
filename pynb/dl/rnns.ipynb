{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs, LSTMs, GRUs in PyTorch\n",
    "\n",
    "This notebook demonstrates recurrent neural networks and their variants.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Why Gating Mechanisms?](#why-gating-mechanisms)\n",
    "2. [Minimal Sequence Classifier with LSTM](#minimal-sequence-classifier-with-lstm)\n",
    "3. [Variable Length Sequence Handling](#variable-length-sequence-handling)\n",
    "4. [RNN vs LSTM vs GRU Comparison](#rnn-vs-lstm-vs-gru-comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Gating Mechanisms?\n",
    "\n",
    "Gating mechanisms help RNNs remember long-term dependencies and avoid vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple vanilla RNN implementation\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # RNN cell parameters\n",
    "        self.W_ih = nn.Linear(input_size, hidden_size)   # input to hidden\n",
    "        self.W_hh = nn.Linear(hidden_size, hidden_size)  # hidden to hidden\n",
    "        self.W_ho = nn.Linear(hidden_size, output_size)  # hidden to output\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            # h_t = tanh(W_ih * x_t + W_hh * h_{t-1})\n",
    "            hidden = torch.tanh(self.W_ih(x[:, t, :]) + self.W_hh(hidden))\n",
    "            output = self.W_ho(hidden)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1), hidden\n",
    "\n",
    "# LSTM implementation (simplified)\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # LSTM gates\n",
    "        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.candidate_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, states=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        if states is None:\n",
    "            h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "            c = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        else:\n",
    "            h, c = states\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            # Concatenate input and hidden state\n",
    "            combined = torch.cat([x[:, t, :], h], dim=1)\n",
    "            \n",
    "            # LSTM gates\n",
    "            forget = torch.sigmoid(self.forget_gate(combined))\n",
    "            input_g = torch.sigmoid(self.input_gate(combined))\n",
    "            candidate = torch.tanh(self.candidate_gate(combined))\n",
    "            output_g = torch.sigmoid(self.output_gate(combined))\n",
    "            \n",
    "            # Update cell state\n",
    "            c = forget * c + input_g * candidate\n",
    "            \n",
    "            # Update hidden state\n",
    "            h = output_g * torch.tanh(c)\n",
    "            \n",
    "            # Output\n",
    "            output = self.output_layer(h)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1), (h, c)\n",
    "\n",
    "# Test both models\n",
    "input_size, hidden_size, output_size = 10, 32, 5\n",
    "seq_len, batch_size = 15, 3\n",
    "\n",
    "rnn_model = VanillaRNN(input_size, hidden_size, output_size)\n",
    "lstm_model = SimpleLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "rnn_out, rnn_hidden = rnn_model(x)\n",
    "lstm_out, lstm_states = lstm_model(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"RNN output shape: {rnn_out.shape}\")\n",
    "print(f\"LSTM output shape: {lstm_out.shape}\")\n",
    "print(f\"\\nRNN has {sum(p.numel() for p in rnn_model.parameters())} parameters\")\n",
    "print(f\"LSTM has {sum(p.numel() for p in lstm_model.parameters())} parameters\")\n",
    "print(\"LSTM has more parameters due to gating mechanisms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Sequence Classifier with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1):\n",
    "        super(SequenceClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # LSTM returns output for all time steps and final hidden state\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Use the last hidden state for classification\n",
    "        # hidden shape: (n_layers, batch, hidden_dim)\n",
    "        last_hidden = hidden[-1]  # Take last layer: (batch, hidden_dim)\n",
    "        \n",
    "        # Apply dropout and classify\n",
    "        output = self.fc(self.dropout(last_hidden))\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create synthetic sequence data for sentiment analysis\n",
    "def create_synthetic_sequences(vocab_size=100, n_samples=1000, min_len=5, max_len=20):\n",
    "    \"\"\"Create synthetic sequences with binary sentiment labels\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Random sequence length\n",
    "        length = torch.randint(min_len, max_len + 1, (1,)).item()\n",
    "        \n",
    "        # Generate random sequence\n",
    "        seq = torch.randint(1, vocab_size, (length,))  # Start from 1 (0 is padding)\n",
    "        \n",
    "        # Simple rule: if sum of tokens is even -> positive (1), else negative (0)\n",
    "        label = int(seq.sum().item() % 2)\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return sequences, torch.tensor(labels)\n",
    "\n",
    "# Generate data\n",
    "vocab_size = 50\n",
    "sequences, labels = create_synthetic_sequences(vocab_size, n_samples=1000)\n",
    "\n",
    "print(f\"Generated {len(sequences)} sequences\")\n",
    "print(f\"Example sequence: {sequences[0]}\")\n",
    "print(f\"Example label: {labels[0]}\")\n",
    "print(f\"Label distribution: {torch.bincount(labels)}\")\n",
    "\n",
    "# Pad sequences to same length\n",
    "def collate_batch(sequences, labels, pad_token=0):\n",
    "    \"\"\"Pad sequences and create batch\"\"\"\n",
    "    # Pad sequences\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=pad_token)\n",
    "    return padded_sequences, labels\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(sequences))\n",
    "train_sequences = sequences[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "val_sequences = sequences[train_size:]\n",
    "val_labels = labels[train_size:]\n",
    "\n",
    "# Create batches\n",
    "train_X, train_y = collate_batch(train_sequences, train_labels)\n",
    "val_X, val_y = collate_batch(val_sequences, val_labels)\n",
    "\n",
    "print(f\"\\nTrain data shape: {train_X.shape}, {train_y.shape}\")\n",
    "print(f\"Val data shape: {val_X.shape}, {val_y.shape}\")\n",
    "\n",
    "# Create model\n",
    "model = SequenceClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=32,\n",
    "    hidden_dim=64,\n",
    "    output_dim=2,  # Binary classification\n",
    "    n_layers=2\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_X, train_y, val_X, val_y, epochs=30):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(train_X)\n",
    "        loss = criterion(outputs, train_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        with torch.no_grad():\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_acc = (predicted == train_y).sum().item() / len(train_y)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(val_X)\n",
    "            val_loss = criterion(val_outputs, val_y)\n",
    "            \n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc = (val_predicted == val_y).sum().item() / len(val_y)\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch:2d}: Train Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "    model, train_X, train_y, val_X, val_y, epochs=30\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal validation accuracy: {val_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Length Sequence Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient variable length sequence processing\n",
    "def create_variable_length_batch():\n",
    "    \"\"\"Create a batch with different sequence lengths\"\"\"\n",
    "    sequences = [\n",
    "        torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]),      # length 8\n",
    "        torch.tensor([10, 11, 12, 13]),               # length 4  \n",
    "        torch.tensor([20, 21, 22, 23, 24, 25]),      # length 6\n",
    "        torch.tensor([30, 31]),                       # length 2\n",
    "        torch.tensor([40, 41, 42, 43, 44])           # length 5\n",
    "    ]\n",
    "    \n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    return sequences, lengths\n",
    "\n",
    "sequences, lengths = create_variable_length_batch()\n",
    "print(\"Original sequences:\")\n",
    "for i, (seq, length) in enumerate(zip(sequences, lengths)):\n",
    "    print(f\"Seq {i}: {seq.tolist()} (length: {length})\")\n",
    "\n",
    "# Method 1: Simple padding (inefficient)\n",
    "print(\"\\n=== Method 1: Simple Padding ===\")\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "print(f\"Padded sequences shape: {padded_sequences.shape}\")\n",
    "print(\"Padded sequences:\")\n",
    "print(padded_sequences.numpy())\n",
    "\n",
    "# Method 2: Packed sequences (efficient)\n",
    "print(\"\\n=== Method 2: Packed Sequences ===\")\n",
    "\n",
    "class EfficientLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(EfficientLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, sequences, lengths):\n",
    "        # Pack padded sequences\n",
    "        packed = pack_padded_sequence(\n",
    "            sequences, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Run LSTM on packed sequences\n",
    "        packed_output, (hidden, cell) = self.lstm(packed)\n",
    "        \n",
    "        # Unpack sequences\n",
    "        output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Use the last valid output for each sequence\n",
    "        batch_size = output.size(0)\n",
    "        last_outputs = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Get the last valid time step for this sequence\n",
    "            last_idx = output_lengths[i] - 1\n",
    "            last_outputs.append(output[i, last_idx, :])\n",
    "        \n",
    "        last_outputs = torch.stack(last_outputs)\n",
    "        \n",
    "        # Final classification\n",
    "        return self.fc(last_outputs)\n",
    "\n",
    "# Create embeddings for our sequences (treat as token indices)\n",
    "vocab_size = 50\n",
    "embed_dim = 16\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "# Convert sequences to embeddings\n",
    "embedded_sequences = [embedding(seq) for seq in sequences]\n",
    "embedded_padded = pad_sequence(embedded_sequences, batch_first=True)\n",
    "\n",
    "print(f\"Embedded padded shape: {embedded_padded.shape}\")  # (batch, max_seq_len, embed_dim)\n",
    "\n",
    "# Test efficient LSTM\n",
    "efficient_model = EfficientLSTM(embed_dim, 32, 2)\n",
    "output = efficient_model(embedded_padded, lengths)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")  # (batch_size, output_size)\n",
    "print(\"Efficient processing completed - no computation wasted on padding!\")\n",
    "\n",
    "# Show the difference in computation\n",
    "print(f\"\\nTotal padded length: {padded_sequences.shape[0] * padded_sequences.shape[1]}\")\n",
    "print(f\"Total actual length: {sum(lengths)}\")\n",
    "print(f\"Efficiency gain: {(1 - sum(lengths)/(padded_sequences.shape[0] * padded_sequences.shape[1]))*100:.1f}% less computation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN vs LSTM vs GRU Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different RNN architectures\n",
    "class RNNComparison(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, rnn_type='LSTM'):\n",
    "        super(RNNComparison, self).__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "        if rnn_type == 'RNN':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            output, (hidden, cell) = self.rnn(x)\n",
    "            last_hidden = hidden[-1]\n",
    "        else:  # RNN or GRU\n",
    "            output, hidden = self.rnn(x)\n",
    "            last_hidden = hidden[-1]\n",
    "        \n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "# Create test models\n",
    "input_size, hidden_size, output_size = 32, 64, 2\n",
    "\n",
    "models = {\n",
    "    'RNN': RNNComparison(input_size, hidden_size, output_size, 'RNN'),\n",
    "    'LSTM': RNNComparison(input_size, hidden_size, output_size, 'LSTM'),\n",
    "    'GRU': RNNComparison(input_size, hidden_size, output_size, 'GRU')\n",
    "}\n",
    "\n",
    "# Compare parameter counts\n",
    "print(\"=== Parameter Comparison ===\")\n",
    "for name, model in models.items():\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name}: {param_count:,} parameters\")\n",
    "\n",
    "# Test on long sequence (to show vanishing gradient problem)\n",
    "seq_len = 100\n",
    "batch_size = 16\n",
    "x = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "print(f\"\\n=== Forward Pass Test ===\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    output = model(x)\n",
    "    print(f\"{name} output shape: {output.shape}\")\n",
    "\n",
    "# Gradient analysis\n",
    "print(f\"\\n=== Gradient Flow Analysis ===\")\n",
    "\n",
    "# Create a simple task: remember the first input value\n",
    "def create_memory_task(seq_len=50, batch_size=32):\n",
    "    \"\"\"Create a task that requires remembering the first time step\"\"\"\n",
    "    x = torch.randn(batch_size, seq_len, 1)\n",
    "    # Target is based on the sign of the first time step\n",
    "    y = (x[:, 0, 0] > 0).long()\n",
    "    return x, y\n",
    "\n",
    "# Test gradient flow\n",
    "x, y = create_memory_task(seq_len=50, batch_size=32)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Reset model\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "    \n",
    "    # Forward and backward\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check gradient norms\n",
    "    total_norm = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            param_norm = param.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    \n",
    "    print(f\"{name}: Loss = {loss.item():.4f}, Gradient norm = {total_norm:.4f}\")\n",
    "\n",
    "print(\"\\n=== Architecture Summary ===\")\n",
    "print(\"RNN: Simple, fast, but suffers from vanishing gradients\")\n",
    "print(\"LSTM: Complex gating, best for long sequences, most parameters\")\n",
    "print(\"GRU: Simplified gating, good compromise between RNN and LSTM\")\n",
    "print(\"\\nRule of thumb:\")\n",
    "print(\"- Short sequences (<20): RNN might be sufficient\")\n",
    "print(\"- Long sequences (>50): LSTM or GRU\")\n",
    "print(\"- When in doubt: try GRU first (good balance of performance/complexity)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}