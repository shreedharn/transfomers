{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPs in PyTorch\n",
    "\n",
    "This notebook demonstrates Multi-Layer Perceptron (MLP) implementation and training.\n",
    "\n",
    "## Table of Contents\n",
    "1. [From Equation to Code](#from-equation-to-code)\n",
    "2. [Training MLP on Synthetic Data](#training-mlp-on-synthetic-data)\n",
    "3. [Common Gotchas](#common-gotchas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Equation to Code\n",
    "\n",
    "Mathematical form: $\\mathbf{h}_1 = \\sigma(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1)$, $\\mathbf{h}_2 = \\sigma(\\mathbf{W}_2 \\mathbf{h}_1 + \\mathbf{b}_2)$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        # First layer\n",
    "        layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n",
    "        \n",
    "        # Output layer (no activation - we'll apply it later)\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Manual implementation to show the math explicitly\n",
    "class ExplicitMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ExplicitMLP, self).__init__()\n",
    "        \n",
    "        # Explicitly define each layer\n",
    "        self.W1 = nn.Linear(input_size, hidden_size)   # W1*x + b1\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size)  # W2*h1 + b2\n",
    "        self.W3 = nn.Linear(hidden_size, output_size)  # W3*h2 + b3\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # h1 = σ(W1*x + b1)\n",
    "        h1 = F.relu(self.W1(x))\n",
    "        \n",
    "        # h2 = σ(W2*h1 + b2)\n",
    "        h2 = F.relu(self.W2(h1))\n",
    "        \n",
    "        # output = W3*h2 + b3 (no activation)\n",
    "        output = self.W3(h2)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Compare the two approaches\n",
    "input_size, hidden_size, output_size = 10, 64, 3\n",
    "model1 = SimpleMLP(input_size, hidden_size, output_size, num_layers=3)\n",
    "model2 = ExplicitMLP(input_size, hidden_size, output_size)\n",
    "\n",
    "print(f\"SimpleMLP parameters: {sum(p.numel() for p in model1.parameters()):,}\")\n",
    "print(f\"ExplicitMLP parameters: {sum(p.numel() for p in model2.parameters()):,}\")\n",
    "\n",
    "# Test with sample input\n",
    "x = torch.randn(5, input_size)  # Batch of 5 samples\n",
    "out1 = model1(x)\n",
    "out2 = model2(x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape (both models): {out1.shape}, {out2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MLP on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification data\n",
    "def generate_synthetic_data(n_samples=1000, n_features=20, n_classes=3):\n",
    "    \"\"\"Generate synthetic classification data\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Generate features\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    \n",
    "    # Create non-linear decision boundary\n",
    "    # Use a simple polynomial to generate labels\n",
    "    weights = torch.randn(n_features, 1)\n",
    "    linear_combo = X @ weights\n",
    "    \n",
    "    # Add some non-linearity\n",
    "    scores = linear_combo.squeeze() + 0.5 * (X[:, 0] * X[:, 1]) + 0.3 * (X[:, 2] ** 2)\n",
    "    \n",
    "    # Convert to class labels\n",
    "    percentiles = torch.quantile(scores, torch.tensor([1/3, 2/3]))\n",
    "    y = torch.zeros(n_samples, dtype=torch.long)\n",
    "    y[scores > percentiles[0]] = 1\n",
    "    y[scores > percentiles[1]] = 2\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_synthetic_data(n_samples=2000, n_features=20, n_classes=3)\n",
    "print(f\"Data shape: {X.shape}, Labels shape: {y.shape}\")\n",
    "print(f\"Class distribution: {torch.bincount(y)}\")\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}\")\n",
    "\n",
    "# Create model\n",
    "model = SimpleMLP(input_size=20, hidden_size=128, output_size=3, num_layers=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, X, y, optimizer, criterion, batch_size=64):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Simple batching (normally you'd use DataLoader)\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_X = X[i:i+batch_size]\n",
    "        batch_y = y[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    return total_loss / (len(X) // batch_size), correct / len(X)\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == y).sum().item() / len(y)\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "# Training loop\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "\n",
    "for epoch in range(50):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, X_train, y_train, optimizer, criterion)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc = evaluate(model, X_val, y_val)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:2d}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal validation accuracy: {val_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "ax1.plot(val_losses, label='Val Loss', color='red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(train_accs, label='Train Acc', color='blue')\n",
    "ax2.plot(val_accs, label='Val Acc', color='red')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Gotchas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== COMMON MLP GOTCHAS ===\")\n",
    "\n",
    "# 1. Forgetting to set model.train() / model.eval()\n",
    "print(\"\\n1. Train/Eval Mode:\")\n",
    "model_with_dropout = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),  # 50% dropout\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "model_with_dropout.train()\n",
    "out_train = model_with_dropout(x)\n",
    "\n",
    "model_with_dropout.eval()\n",
    "out_eval = model_with_dropout(x)\n",
    "\n",
    "print(f\"Output in train mode: {out_train.item():.4f}\")\n",
    "print(f\"Output in eval mode: {out_eval.item():.4f}\")\n",
    "print(\"Notice how outputs can be different due to dropout!\")\n",
    "\n",
    "# 2. Wrong loss function for the task\n",
    "print(\"\\n2. Loss Function Selection:\")\n",
    "# For classification: CrossEntropyLoss\n",
    "# For regression: MSELoss or L1Loss\n",
    "# For binary classification: BCEWithLogitsLoss\n",
    "\n",
    "logits = torch.randn(3, 5)  # 3 samples, 5 classes\n",
    "targets = torch.tensor([0, 2, 4])  # Class indices\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "print(f\"Correct (CrossEntropy): {ce_loss(logits, targets).item():.4f}\")\n",
    "\n",
    "# Wrong: using MSE for classification\n",
    "mse_loss = nn.MSELoss()\n",
    "# This would be wrong: mse_loss(logits, targets)  # Shape mismatch!\n",
    "print(\"MSE for classification would cause shape errors!\")\n",
    "\n",
    "# 3. Gradient explosion/vanishing\n",
    "print(\"\\n3. Gradient Issues:\")\n",
    "\n",
    "# Very deep network without proper initialization\n",
    "deep_model = nn.Sequential()\n",
    "for i in range(20):  # 20 layers!\n",
    "    deep_model.add_module(f'linear_{i}', nn.Linear(64, 64))\n",
    "    deep_model.add_module(f'sigmoid_{i}', nn.Sigmoid())  # Sigmoid can cause vanishing gradients\n",
    "deep_model.add_module('final', nn.Linear(64, 1))\n",
    "\n",
    "# Check gradient norms\n",
    "x = torch.randn(10, 64)\n",
    "y = torch.randn(10, 1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "output = deep_model(x)\n",
    "loss = criterion(output, y)\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients in first vs last layer\n",
    "first_layer_grad = deep_model[0].weight.grad\n",
    "last_layer_grad = deep_model[-1].weight.grad\n",
    "\n",
    "if first_layer_grad is not None and last_layer_grad is not None:\n",
    "    print(f\"First layer gradient norm: {first_layer_grad.norm().item():.8f}\")\n",
    "    print(f\"Last layer gradient norm: {last_layer_grad.norm().item():.8f}\")\n",
    "    print(\"Notice how gradients can vanish in deep networks with sigmoid!\")\n",
    "\n",
    "# 4. Learning rate too high/low\n",
    "print(\"\\n4. Learning Rate Effects:\")\n",
    "print(\"Too high: Loss explodes or oscillates\")\n",
    "print(\"Too low: Very slow convergence\")\n",
    "print(\"Rule of thumb: Start with 1e-3 for Adam, 1e-2 for SGD\")\n",
    "\n",
    "# 5. Batch size effects\n",
    "print(\"\\n5. Batch Size Considerations:\")\n",
    "print(\"Small batches: Noisy gradients, may help with generalization\")\n",
    "print(\"Large batches: Smooth gradients, faster training, may overfit\")\n",
    "print(\"Typical range: 16-256 for most problems\")\n",
    "\n",
    "print(\"\\n=== END OF GOTCHAS ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}