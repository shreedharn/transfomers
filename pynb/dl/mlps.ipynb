{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Multi-Layer Perceptrons: Deep Learning Foundations\n\n## üéØ Introduction\n\nWelcome to the building blocks of deep learning! This notebook will take you from simple linear transformations to sophisticated neural networks that can learn complex patterns. MLPs are the foundation that everything else builds upon - understanding them deeply is crucial for mastering modern AI.\n\n### üß† What You'll Master\n\nThis comprehensive guide covers:\n- **From linear to nonlinear**: How activation functions enable complex learning\n- **Universal approximation**: Why MLPs can learn any continuous function\n- **Deep architectures**: Building networks with multiple hidden layers\n- **Training dynamics**: Understanding how MLPs learn through backpropagation\n- **Modern patterns**: Batch normalization, dropout, and residual connections\n\n### üéì Prerequisites\n\n- Solid understanding of PyTorch tensors and basic operations\n- Familiarity with linear algebra (matrix multiplication, vectors)\n- Basic calculus concepts (derivatives, chain rule)\n- Understanding of gradient descent optimization\n\n### üöÄ Why MLPs Matter\n\nMLPs are fundamental because they:\n- **Prove universal approximation**: Can theoretically learn any continuous function\n- **Form building blocks**: Every complex architecture contains MLP components\n- **Enable nonlinear learning**: Activation functions make complex patterns possible\n- **Scale effectively**: From tiny networks to massive language models\n- **Transfer broadly**: Same principles work across vision, NLP, and beyond\n\n---\n\n## üìö Table of Contents\n\n1. **[From Linear to Nonlinear](#from-linear-to-nonlinear)** - The power of activation functions\n2. **[Deep Architecture Design](#deep-architecture-design)** - Building effective multi-layer networks\n3. **[Universal Approximation Demo](#universal-approximation-demo)** - Seeing MLPs learn complex functions\n4. **[Modern MLP Patterns](#modern-mlp-patterns)** - Batch norm, dropout, and residuals\n5. **[Training Dynamics](#training-dynamics)** - Understanding how MLPs learn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## From Linear to Nonlinear\n\n### ‚ö° The Activation Function Revolution\n\nThe difference between a linear transformation and a neural network is the activation function. Without activations, even the deepest network is just a complex matrix multiplication. Let's see how nonlinearity enables universal learning!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# THE POWER OF NONLINEARITY\n# =============================================================================\n\nprint(\"‚ö° From Linear to Nonlinear Transformation\")\nprint(\"=\" * 50)\n\n# Demonstrate why activation functions are crucial\ndef demonstrate_linearity_problem():\n    \"\"\"Show why pure linear layers can't learn complex patterns.\"\"\"\n    \n    print(\"üîç The Linear Limitation\")\n    print(\"-\" * 30)\n    \n    # Create a \"deep\" network with NO activation functions\n    class PureLinearNetwork(nn.Module):\n        def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3):\n            super().__init__()\n            \n            # Multiple linear layers - but NO activations!\n            layers = []\n            layers.append(nn.Linear(input_dim, hidden_dim))\n            \n            for _ in range(num_layers - 2):\n                layers.append(nn.Linear(hidden_dim, hidden_dim))\n            \n            layers.append(nn.Linear(hidden_dim, output_dim))\n            \n            self.network = nn.Sequential(*layers)\n        \n        def forward(self, x):\n            return self.network(x)\n    \n    # Create pure linear network\n    linear_net = PureLinearNetwork(2, 64, 1, num_layers=5)\n    \n    print(f\"Created 'deep' network with {len(list(linear_net.parameters()))} parameter tensors\")\n    print(f\"Layers: 2 ‚Üí 64 ‚Üí 64 ‚Üí 64 ‚Üí 1\")\n    \n    # Test with simple input\n    x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n    output = linear_net(x)\n    \n    print(f\"Input: {x}\")\n    print(f\"Output: {output.detach()}\")\n    \n    # The crucial insight: This is just ONE big matrix multiplication!\n    print(f\"\\nüí° Key Insight:\")\n    print(f\"No matter how many linear layers you stack,\")\n    print(f\"the result is mathematically equivalent to a SINGLE linear transformation!\")\n    print(f\"W5 @ W4 @ W3 @ W2 @ W1 = W_combined\")\n    \n    # Prove this by computing the equivalent single matrix\n    with torch.no_grad():\n        # Get all weight matrices\n        W1 = linear_net.network[0].weight  # [64, 2]\n        W2 = linear_net.network[1].weight  # [64, 64]\n        W3 = linear_net.network[2].weight  # [64, 64]\n        W4 = linear_net.network[3].weight  # [1, 64]\n        \n        # Compose them into single matrix (note reversed order for matrix mult)\n        W_combined = W4 @ W3 @ W2 @ W1  # [1, 2]\n        \n        # Test equivalence\n        single_matrix_output = x @ W_combined.T\n        \n        print(f\"\\nProof - Single matrix multiplication gives same result:\")\n        print(f\"Deep network output: {output.detach()}\")\n        print(f\"Single matrix output: {single_matrix_output}\")\n        print(f\"Difference: {torch.abs(output - single_matrix_output).max().item():.10f}\")\n\ndemonstrate_linearity_problem()\n\nprint(f\"\\n‚ú® Enter Activation Functions\")\nprint(\"=\" * 50)\n\n# Compare different activation functions\nclass ActivationComparison(nn.Module):\n    \"\"\"Network to compare different activation functions.\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, output_dim, activation='relu'):\n        super().__init__()\n        \n        self.linear1 = nn.Linear(input_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n        self.linear3 = nn.Linear(hidden_dim, output_dim)\n        \n        # Different activation functions\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'tanh':\n            self.activation = nn.Tanh()\n        elif activation == 'sigmoid':\n            self.activation = nn.Sigmoid()\n        elif activation == 'gelu':\n            self.activation = nn.GELU()\n        else:\n            raise ValueError(f\"Unknown activation: {activation}\")\n            \n        self.activation_name = activation\n    \n    def forward(self, x):\n        # Now we have: Linear ‚Üí Activation ‚Üí Linear ‚Üí Activation ‚Üí Linear\n        x = self.linear1(x)\n        x = self.activation(x)  # This breaks the linearity!\n        x = self.linear2(x)\n        x = self.activation(x)  # Each activation enables new expressiveness\n        x = self.linear3(x)\n        return x\n\n# Create networks with different activations\nactivations = ['relu', 'tanh', 'sigmoid', 'gelu']\nnetworks = {}\n\nfor act in activations:\n    networks[act] = ActivationComparison(2, 32, 1, activation=act)\n\nprint(\"Created networks with different activations:\")\nfor name, net in networks.items():\n    param_count = sum(p.numel() for p in net.parameters())\n    print(f\"  {name.upper():8}: {param_count:,} parameters\")\n\n# Test how different activations handle the same input\ntest_input = torch.tensor([[-2.0, -1.0], [0.0, 0.0], [1.0, 2.0]])\nprint(f\"\\nTesting with input: {test_input}\")\n\nprint(f\"\\nActivation function responses:\")\nfor name, net in networks.items():\n    with torch.no_grad():\n        output = net(test_input)\n        print(f\"{name.upper():8}: {output.squeeze().tolist()}\")\n\nprint(f\"\\nüéØ Activation Function Properties\")\nprint(\"=\" * 50)\n\n# Visualize activation function shapes\nx_range = torch.linspace(-3, 3, 100)\n\nactivations_funcs = {\n    'ReLU': nn.ReLU(),\n    'Tanh': nn.Tanh(), \n    'Sigmoid': nn.Sigmoid(),\n    'GELU': nn.GELU()\n}\n\nprint(\"Activation function characteristics:\")\nprint(\"Function | Range       | Zero-Centered | Differentiable | Modern Use\")\nprint(\"---------|-------------|---------------|----------------|------------\")\n\nfor name, func in activations_funcs.items():\n    with torch.no_grad():\n        y = func(x_range)\n        range_str = f\"({y.min().item():.1f}, {y.max().item():.1f})\"\n        zero_centered = \"Yes\" if y.mean().abs().item() < 0.1 else \"No\"\n        \n        if name == 'ReLU':\n            diff = \"At x‚â†0\"\n            use = \"CNNs, MLPs\"\n        elif name == 'Tanh':\n            diff = \"Everywhere\"\n            use = \"RNNs, Classic\"\n        elif name == 'Sigmoid':\n            diff = \"Everywhere\" \n            use = \"Output layer\"\n        else:  # GELU\n            diff = \"Everywhere\"\n            use = \"Transformers\"\n            \n        print(f\"{name:8} | {range_str:11} | {zero_centered:13} | {diff:14} | {use}\")\n\nprint(f\"\\nüí° Why Activations Enable Universal Approximation\")\nprint(\"=\" * 50)\nprint(\"1. **Break linearity**: Prevent network collapse to single matrix\")\nprint(\"2. **Create boundaries**: ReLU creates piecewise linear regions\") \nprint(\"3. **Enable compositions**: Stack simple nonlinear transforms\")\nprint(\"4. **Approximate any function**: Universal approximation theorem\")\nprint(\"5. **Learn complex patterns**: XOR, circles, arbitrary decision boundaries\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MLP on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification data\n",
    "def generate_synthetic_data(n_samples=1000, n_features=20, n_classes=3):\n",
    "    \"\"\"Generate synthetic classification data\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Generate features\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    \n",
    "    # Create non-linear decision boundary\n",
    "    # Use a simple polynomial to generate labels\n",
    "    weights = torch.randn(n_features, 1)\n",
    "    linear_combo = X @ weights\n",
    "    \n",
    "    # Add some non-linearity\n",
    "    scores = linear_combo.squeeze() + 0.5 * (X[:, 0] * X[:, 1]) + 0.3 * (X[:, 2] ** 2)\n",
    "    \n",
    "    # Convert to class labels\n",
    "    percentiles = torch.quantile(scores, torch.tensor([1/3, 2/3]))\n",
    "    y = torch.zeros(n_samples, dtype=torch.long)\n",
    "    y[scores > percentiles[0]] = 1\n",
    "    y[scores > percentiles[1]] = 2\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_synthetic_data(n_samples=2000, n_features=20, n_classes=3)\n",
    "print(f\"Data shape: {X.shape}, Labels shape: {y.shape}\")\n",
    "print(f\"Class distribution: {torch.bincount(y)}\")\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}\")\n",
    "\n",
    "# Create model\n",
    "model = SimpleMLP(input_size=20, hidden_size=128, output_size=3, num_layers=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, X, y, optimizer, criterion, batch_size=64):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Simple batching (normally you'd use DataLoader)\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_X = X[i:i+batch_size]\n",
    "        batch_y = y[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    return total_loss / (len(X) // batch_size), correct / len(X)\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == y).sum().item() / len(y)\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "# Training loop\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "\n",
    "for epoch in range(50):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, X_train, y_train, optimizer, criterion)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc = evaluate(model, X_val, y_val)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:2d}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal validation accuracy: {val_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "ax1.plot(val_losses, label='Val Loss', color='red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(train_accs, label='Train Acc', color='blue')\n",
    "ax2.plot(val_accs, label='Val Acc', color='red')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Gotchas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== COMMON MLP GOTCHAS ===\")\n",
    "\n",
    "# 1. Forgetting to set model.train() / model.eval()\n",
    "print(\"\\n1. Train/Eval Mode:\")\n",
    "model_with_dropout = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),  # 50% dropout\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "model_with_dropout.train()\n",
    "out_train = model_with_dropout(x)\n",
    "\n",
    "model_with_dropout.eval()\n",
    "out_eval = model_with_dropout(x)\n",
    "\n",
    "print(f\"Output in train mode: {out_train.item():.4f}\")\n",
    "print(f\"Output in eval mode: {out_eval.item():.4f}\")\n",
    "print(\"Notice how outputs can be different due to dropout!\")\n",
    "\n",
    "# 2. Wrong loss function for the task\n",
    "print(\"\\n2. Loss Function Selection:\")\n",
    "# For classification: CrossEntropyLoss\n",
    "# For regression: MSELoss or L1Loss\n",
    "# For binary classification: BCEWithLogitsLoss\n",
    "\n",
    "logits = torch.randn(3, 5)  # 3 samples, 5 classes\n",
    "targets = torch.tensor([0, 2, 4])  # Class indices\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "print(f\"Correct (CrossEntropy): {ce_loss(logits, targets).item():.4f}\")\n",
    "\n",
    "# Wrong: using MSE for classification\n",
    "mse_loss = nn.MSELoss()\n",
    "# This would be wrong: mse_loss(logits, targets)  # Shape mismatch!\n",
    "print(\"MSE for classification would cause shape errors!\")\n",
    "\n",
    "# 3. Gradient explosion/vanishing\n",
    "print(\"\\n3. Gradient Issues:\")\n",
    "\n",
    "# Very deep network without proper initialization\n",
    "deep_model = nn.Sequential()\n",
    "for i in range(20):  # 20 layers!\n",
    "    deep_model.add_module(f'linear_{i}', nn.Linear(64, 64))\n",
    "    deep_model.add_module(f'sigmoid_{i}', nn.Sigmoid())  # Sigmoid can cause vanishing gradients\n",
    "deep_model.add_module('final', nn.Linear(64, 1))\n",
    "\n",
    "# Check gradient norms\n",
    "x = torch.randn(10, 64)\n",
    "y = torch.randn(10, 1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "output = deep_model(x)\n",
    "loss = criterion(output, y)\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients in first vs last layer\n",
    "first_layer_grad = deep_model[0].weight.grad\n",
    "last_layer_grad = deep_model[-1].weight.grad\n",
    "\n",
    "if first_layer_grad is not None and last_layer_grad is not None:\n",
    "    print(f\"First layer gradient norm: {first_layer_grad.norm().item():.8f}\")\n",
    "    print(f\"Last layer gradient norm: {last_layer_grad.norm().item():.8f}\")\n",
    "    print(\"Notice how gradients can vanish in deep networks with sigmoid!\")\n",
    "\n",
    "# 4. Learning rate too high/low\n",
    "print(\"\\n4. Learning Rate Effects:\")\n",
    "print(\"Too high: Loss explodes or oscillates\")\n",
    "print(\"Too low: Very slow convergence\")\n",
    "print(\"Rule of thumb: Start with 1e-3 for Adam, 1e-2 for SGD\")\n",
    "\n",
    "# 5. Batch size effects\n",
    "print(\"\\n5. Batch Size Considerations:\")\n",
    "print(\"Small batches: Noisy gradients, may help with generalization\")\n",
    "print(\"Large batches: Smooth gradients, faster training, may overfit\")\n",
    "print(\"Typical range: 16-256 for most problems\")\n",
    "\n",
    "print(\"\\n=== END OF GOTCHAS ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}