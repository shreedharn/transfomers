{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra for Neural Networks\n",
    "\n",
    "This notebook contains PyTorch examples demonstrating linear algebra concepts essential for understanding neural networks.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Matrix Multiplication](#matrix-multiplication)\n",
    "2. [Matrix Transpose](#matrix-transpose)\n",
    "3. [Matrix Inverse](#matrix-inverse)\n",
    "4. [Eigenvalues & Eigenvectors](#eigenvalues--eigenvectors)\n",
    "5. [Singular Value Decomposition (SVD)](#singular-value-decomposition-svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "**Formula:** $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$ where $C_{ij} = \\sum_k A_{ik}B_{kj}$\n",
    "\n",
    "Matrix multiplication is the fundamental operation of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network layer: y = Wx + b\n",
    "batch_size, input_dim, output_dim = 32, 784, 128\n",
    "x = torch.randn(batch_size, input_dim)  # Input batch\n",
    "W = torch.randn(output_dim, input_dim)  # Weight matrix\n",
    "b = torch.randn(output_dim)             # Bias vector\n",
    "\n",
    "# Forward pass - matrix multiplication\n",
    "y = x @ W.T + b  # Shape: (32, 128)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Weight shape: {W.shape}\")  \n",
    "print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "# Each row of y contains weighted sums for all neurons for one sample\n",
    "print(f\"Output for first sample: {y[0][:5]}...\")  # First 5 neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Transpose\n",
    "\n",
    "**Formula:** $(\\mathbf{A})^T_{ij} = \\mathbf{A}_{ji}$\n",
    "\n",
    "Essential for backpropagation - the transpose \"reverses\" the forward direction of information flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass: y = x @ W.T\n",
    "# Backward pass: dx = dy @ W (using transpose automatically)\n",
    "x = torch.randn(32, 784, requires_grad=True)\n",
    "W = torch.randn(128, 784, requires_grad=True)\n",
    "y = x @ W.T\n",
    "\n",
    "# Create dummy loss and backpropagate\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Forward: x {x.shape} @ W.T {W.T.shape} = y {y.shape}\")\n",
    "print(f\"Gradient flows back through transpose automatically\")\n",
    "print(f\"x.grad shape: {x.grad.shape}\")  # Same as x.shape\n",
    "print(f\"W.grad shape: {W.grad.shape}\")  # Same as W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Inverse\n",
    "\n",
    "**Formula:** $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}$\n",
    "\n",
    "Used in analytical solutions and understanding linear transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal equations for linear regression: Î¸ = (X.T @ X)^(-1) @ X.T @ y\n",
    "n_samples, n_features = 100, 10\n",
    "X = torch.randn(n_samples, n_features)\n",
    "true_theta = torch.randn(n_features)\n",
    "y = X @ true_theta + 0.1 * torch.randn(n_samples)\n",
    "\n",
    "# Analytical solution using matrix inverse\n",
    "XtX = X.T @ X\n",
    "XtX_inv = torch.inverse(XtX)\n",
    "theta_analytical = XtX_inv @ X.T @ y\n",
    "\n",
    "print(f\"True theta: {true_theta[:3]}\")\n",
    "print(f\"Estimated theta: {theta_analytical[:3]}\")\n",
    "print(f\"Error: {torch.norm(true_theta - theta_analytical):.6f}\")\n",
    "\n",
    "# Note: In practice, use torch.linalg.lstsq for numerical stability\n",
    "theta_stable = torch.linalg.lstsq(X, y).solution\n",
    "print(f\"Stable solution: {theta_stable[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues & Eigenvectors\n",
    "\n",
    "**Formula:** $\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}$\n",
    "\n",
    "Reveals principal directions of data variation and helps analyze gradient flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing weight matrix conditioning\n",
    "W = torch.randn(100, 100)\n",
    "eigenvals, eigenvecs = torch.linalg.eig(W @ W.T)  # Eigendecomposition\n",
    "eigenvals = eigenvals.real  # Take real part\n",
    "\n",
    "condition_number = eigenvals.max() / eigenvals.min()\n",
    "print(f\"Condition number: {condition_number:.2f}\")\n",
    "print(f\"Max eigenvalue: {eigenvals.max():.2f}\")\n",
    "print(f\"Min eigenvalue: {eigenvals.min():.2f}\")\n",
    "\n",
    "# PCA example - find principal components\n",
    "data = torch.randn(1000, 50)  # 1000 samples, 50 features\n",
    "centered_data = data - data.mean(dim=0)\n",
    "cov_matrix = (centered_data.T @ centered_data) / (len(data) - 1)\n",
    "\n",
    "eigenvals, eigenvecs = torch.linalg.eigh(cov_matrix)  # For symmetric matrices\n",
    "# Sort by eigenvalue magnitude\n",
    "sorted_indices = torch.argsort(eigenvals, descending=True)\n",
    "principal_components = eigenvecs[:, sorted_indices]\n",
    "\n",
    "print(f\"Explained variance ratios: {eigenvals[sorted_indices][:5] / eigenvals.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "**Formula:** $\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T$\n",
    "\n",
    "Decomposes any matrix into orthogonal transformations and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD for dimensionality reduction and analysis\n",
    "data = torch.randn(1000, 100)  # High-dimensional data\n",
    "\n",
    "# Perform SVD\n",
    "U, S, Vt = torch.linalg.svd(data, full_matrices=False)\n",
    "\n",
    "# Analyze the singular values\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"U shape: {U.shape}\")    # Left singular vectors\n",
    "print(f\"S shape: {S.shape}\")    # Singular values\n",
    "print(f\"Vt shape: {Vt.shape}\")  # Right singular vectors\n",
    "\n",
    "# Reconstruct with fewer components (dimensionality reduction)\n",
    "k = 20  # Keep top 20 components\n",
    "data_reduced = U[:, :k] @ torch.diag(S[:k]) @ Vt[:k, :]\n",
    "\n",
    "reconstruction_error = torch.norm(data - data_reduced)\n",
    "compression_ratio = (k * (U.shape[0] + Vt.shape[1])) / (data.shape[0] * data.shape[1])\n",
    "\n",
    "print(f\"Reconstruction error: {reconstruction_error:.2f}\")\n",
    "print(f\"Compression ratio: {compression_ratio:.2%}\")\n",
    "print(f\"Variance explained by top {k} components: {(S[:k]**2).sum() / (S**2).sum():.2%}\")\n",
    "\n",
    "# SVD for weight initialization (orthogonal initialization)\n",
    "def svd_init(tensor):\n",
    "    \"\"\"Initialize weights using SVD for orthogonal matrices\"\"\"\n",
    "    if tensor.dim() >= 2:\n",
    "        U, _, Vt = torch.linalg.svd(tensor, full_matrices=False)\n",
    "        return U if U.shape == tensor.shape else Vt\n",
    "    return tensor\n",
    "\n",
    "weight = torch.empty(128, 64)\n",
    "orthogonal_weight = svd_init(weight)\n",
    "print(f\"Orthogonality check: {torch.norm(orthogonal_weight @ orthogonal_weight.T - torch.eye(128)):.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}