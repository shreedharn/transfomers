{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors & Geometry for Neural Networks\n",
    "\n",
    "This notebook contains PyTorch examples demonstrating vector and geometric concepts essential for understanding neural networks.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Dot Product](#dot-product)\n",
    "2. [Cosine Similarity](#cosine-similarity)\n",
    "3. [Euclidean Distance](#euclidean-distance)\n",
    "4. [Lp Norms](#lp-norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "\n",
    "**Formula:** $\\mathbf{a} \\cdot \\mathbf{b} = \\sum_i a_i b_i = \\|\\mathbf{a}\\|\\|\\mathbf{b}\\|\\cos(\\theta)$\n",
    "\n",
    "Measures how \"aligned\" two vectors are. Core operation for neuron activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron activation using dot product\n",
    "input_vector = torch.tensor([1.0, 0.5, -0.2, 0.8])\n",
    "weight_vector = torch.tensor([0.3, 0.7, -0.1, 0.4])\n",
    "\n",
    "# Raw activation (dot product)\n",
    "activation = torch.dot(input_vector, weight_vector)\n",
    "print(f\"Activation: {activation:.3f}\")\n",
    "\n",
    "# Understanding geometric interpretation\n",
    "magnitude_a = torch.norm(input_vector)\n",
    "magnitude_b = torch.norm(weight_vector)\n",
    "cosine_angle = activation / (magnitude_a * magnitude_b)\n",
    "angle_degrees = torch.acos(cosine_angle) * 180 / torch.pi\n",
    "\n",
    "print(f\"Input magnitude: {magnitude_a:.3f}\")\n",
    "print(f\"Weight magnitude: {magnitude_b:.3f}\")\n",
    "print(f\"Cosine of angle: {cosine_angle:.3f}\")\n",
    "print(f\"Angle between vectors: {angle_degrees:.1f}°\")\n",
    "\n",
    "# Batch processing - multiple samples\n",
    "batch_inputs = torch.randn(32, 4)  # 32 samples, 4 features\n",
    "batch_activations = batch_inputs @ weight_vector\n",
    "print(f\"Batch activations shape: {batch_activations.shape}\")\n",
    "print(f\"Sample activations: {batch_activations[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "**Formula:** $\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|}$\n",
    "\n",
    "Measures similarity independent of magnitude. Used in attention and word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding similarity\n",
    "word1 = torch.tensor([0.2, 0.8, -0.1, 0.3, 0.7])  # \"king\"\n",
    "word2 = torch.tensor([0.1, 0.7, -0.2, 0.2, 0.6])  # \"queen\"\n",
    "word3 = torch.tensor([-0.5, 0.1, 0.8, -0.3, 0.2])  # \"apple\"\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return torch.dot(a, b) / (torch.norm(a) * torch.norm(b))\n",
    "\n",
    "sim_king_queen = cosine_similarity(word1, word2)\n",
    "sim_king_apple = cosine_similarity(word1, word3)\n",
    "sim_queen_apple = cosine_similarity(word2, word3)\n",
    "\n",
    "print(f\"King-Queen similarity: {sim_king_queen:.3f}\")\n",
    "print(f\"King-Apple similarity: {sim_king_apple:.3f}\")\n",
    "print(f\"Queen-Apple similarity: {sim_queen_apple:.3f}\")\n",
    "\n",
    "# Attention mechanism using cosine similarity\n",
    "query = torch.randn(1, 64)\n",
    "keys = torch.randn(10, 64)  # 10 different keys\n",
    "\n",
    "# Compute attention weights using cosine similarity\n",
    "similarities = torch.nn.functional.cosine_similarity(query, keys, dim=1)\n",
    "attention_weights = torch.softmax(similarities, dim=0)\n",
    "\n",
    "print(f\"Attention weights: {attention_weights}\")\n",
    "print(f\"Max attention to key: {attention_weights.argmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance\n",
    "\n",
    "**Formula:** $d(\\mathbf{a}, \\mathbf{b}) = \\|\\mathbf{a} - \\mathbf{b}\\|_2 = \\sqrt{\\sum_i (a_i - b_i)^2}$\n",
    "\n",
    "Measures how \"far apart\" two points are in feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Nearest Neighbors in embedding space\n",
    "embeddings = torch.randn(100, 50)  # 100 data points, 50-dim embeddings\n",
    "query_point = torch.randn(1, 50)\n",
    "\n",
    "# Compute distances to all points\n",
    "distances = torch.norm(embeddings - query_point, dim=1)\n",
    "k = 5\n",
    "nearest_k = torch.topk(distances, k, largest=False)\n",
    "\n",
    "print(f\"Distances to {k} nearest neighbors: {nearest_k.values}\")\n",
    "print(f\"Indices of {k} nearest neighbors: {nearest_k.indices}\")\n",
    "\n",
    "# Mean Squared Error loss using Euclidean distance\n",
    "predictions = torch.randn(32, 10)  # 32 samples, 10 classes\n",
    "targets = torch.randn(32, 10)\n",
    "\n",
    "mse_loss = torch.mean((predictions - targets) ** 2)\n",
    "# Equivalent to: torch.mean(torch.norm(predictions - targets, dim=1) ** 2)\n",
    "euclidean_based_loss = torch.mean(torch.norm(predictions - targets, dim=1) ** 2)\n",
    "\n",
    "print(f\"MSE loss: {mse_loss:.4f}\")\n",
    "print(f\"Euclidean-based loss: {euclidean_based_loss:.4f}\")\n",
    "\n",
    "# Clustering: assign points to nearest centroid\n",
    "centroids = torch.randn(3, 50)  # 3 cluster centers\n",
    "data_points = torch.randn(20, 50)  # 20 data points\n",
    "\n",
    "# Compute distance from each point to each centroid\n",
    "distances_to_centroids = torch.cdist(data_points, centroids)  # Shape: (20, 3)\n",
    "cluster_assignments = torch.argmin(distances_to_centroids, dim=1)\n",
    "\n",
    "print(f\"Cluster assignments: {cluster_assignments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lp Norms\n",
    "\n",
    "**Formula:** $\\|\\mathbf{x}\\|_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}$\n",
    "\n",
    "Measures vector \"size\" in different ways. Used for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different norms and their effects\n",
    "weights = torch.tensor([3.0, -1.0, 0.5, -2.0, 0.1])\n",
    "\n",
    "l1_norm = torch.norm(weights, p=1)  # Sum of absolute values\n",
    "l2_norm = torch.norm(weights, p=2)  # Euclidean norm\n",
    "l_inf_norm = torch.norm(weights, p=float('inf'))  # Maximum absolute value\n",
    "\n",
    "print(f\"Original weights: {weights}\")\n",
    "print(f\"L1 norm: {l1_norm:.3f}\")\n",
    "print(f\"L2 norm: {l2_norm:.3f}\")\n",
    "print(f\"L∞ norm: {l_inf_norm:.3f}\")\n",
    "\n",
    "# L1 vs L2 regularization effects\n",
    "def train_with_regularization(reg_type='l2', reg_strength=0.01):\n",
    "    model_weights = torch.tensor([2.0, -1.5, 0.3, -0.8], requires_grad=True)\n",
    "    optimizer = torch.optim.SGD([model_weights], lr=0.1)\n",
    "    \n",
    "    for _ in range(100):\n",
    "        # Dummy loss (normally would be your actual loss)\n",
    "        loss = torch.sum(model_weights ** 2)  # Dummy objective\n",
    "        \n",
    "        # Add regularization\n",
    "        if reg_type == 'l1':\n",
    "            loss += reg_strength * torch.norm(model_weights, p=1)\n",
    "        elif reg_type == 'l2':\n",
    "            loss += reg_strength * torch.norm(model_weights, p=2) ** 2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model_weights.detach()\n",
    "\n",
    "# Compare effects\n",
    "weights_no_reg = train_with_regularization(reg_type=None, reg_strength=0)\n",
    "weights_l1 = train_with_regularization(reg_type='l1', reg_strength=0.1)\n",
    "weights_l2 = train_with_regularization(reg_type='l2', reg_strength=0.1)\n",
    "\n",
    "print(f\"No regularization: {weights_no_reg}\")\n",
    "print(f\"L1 regularization: {weights_l1}\")\n",
    "print(f\"L2 regularization: {weights_l2}\")\n",
    "\n",
    "# Count near-zero weights (sparsity)\n",
    "def count_sparse(weights, threshold=0.01):\n",
    "    return (torch.abs(weights) < threshold).sum().item()\n",
    "\n",
    "print(f\"Sparse weights (L1): {count_sparse(weights_l1)}/4\")\n",
    "print(f\"Sparse weights (L2): {count_sparse(weights_l2)}/4\")\n",
    "\n",
    "# Gradient norms for training stability\n",
    "gradients = torch.randn(1000)  # Simulated gradients\n",
    "grad_l1_norm = torch.norm(gradients, p=1)\n",
    "grad_l2_norm = torch.norm(gradients, p=2)\n",
    "\n",
    "print(f\"Gradient L1 norm: {grad_l1_norm:.2f}\")\n",
    "print(f\"Gradient L2 norm: {grad_l2_norm:.2f}\")\n",
    "\n",
    "# Gradient clipping using L2 norm\n",
    "max_norm = 1.0\n",
    "if grad_l2_norm > max_norm:\n",
    "    gradients = gradients * (max_norm / grad_l2_norm)\n",
    "    print(f\"Gradients clipped to norm: {torch.norm(gradients, p=2):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}