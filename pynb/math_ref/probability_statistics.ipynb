{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability & Statistics for Neural Networks\n",
    "\n",
    "This notebook contains PyTorch examples demonstrating probability and statistics concepts essential for understanding neural networks.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Expectation](#expectation)\n",
    "2. [Variance](#variance)\n",
    "3. [Softmax](#softmax)\n",
    "4. [Cross-Entropy Loss](#cross-entropy-loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation\n",
    "\n",
    "**Formula:** $\\mathbb{E}[X] = \\sum_x x \\cdot P(X = x)$\n",
    "\n",
    "Average value of a random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo estimation of expectation\n",
    "def estimate_expectation(samples):\n",
    "    return torch.mean(samples)\n",
    "\n",
    "# Expectation in neural network training\n",
    "data_loader = [(torch.randn(32, 10), torch.randint(0, 2, (32,))) for _ in range(100)]\n",
    "model = torch.nn.Linear(10, 2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute expected loss over dataset\n",
    "total_loss = 0\n",
    "num_batches = 0\n",
    "for x, y in data_loader:\n",
    "    loss = criterion(model(x), y)\n",
    "    total_loss += loss.item()\n",
    "    num_batches += 1\n",
    "\n",
    "expected_loss = total_loss / num_batches\n",
    "print(f\"Expected loss over dataset: {expected_loss:.4f}\")\n",
    "\n",
    "# Batch normalization uses expectation\n",
    "x = torch.randn(100, 50)\n",
    "batch_mean = torch.mean(x, dim=0)  # E[X] per feature\n",
    "print(f\"Feature means: {batch_mean[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance\n",
    "\n",
    "**Formula:** $\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]$\n",
    "\n",
    "Measures spread of a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance in weight initialization\n",
    "def xavier_init(input_dim, output_dim):\n",
    "    \"\"\"Xavier initialization maintains variance across layers\"\"\"\n",
    "    variance = 2.0 / (input_dim + output_dim)\n",
    "    return torch.randn(output_dim, input_dim) * torch.sqrt(torch.tensor(variance))\n",
    "\n",
    "def he_init(input_dim, output_dim):\n",
    "    \"\"\"He initialization for ReLU networks\"\"\"\n",
    "    variance = 2.0 / input_dim\n",
    "    return torch.randn(output_dim, input_dim) * torch.sqrt(torch.tensor(variance))\n",
    "\n",
    "# Compare initialization schemes\n",
    "x = torch.randn(1000, 100)\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        w_xavier = xavier_init(x.shape[1], 100)\n",
    "        w_he = he_init(x.shape[1], 100)\n",
    "        w_random = torch.randn(100, x.shape[1]) * 0.1\n",
    "    else:\n",
    "        w_xavier = xavier_init(100, 100)\n",
    "        w_he = he_init(100, 100)\n",
    "        w_random = torch.randn(100, 100) * 0.1\n",
    "    \n",
    "    x_xavier = torch.relu(x @ w_xavier.T)\n",
    "    x_he = torch.relu(x @ w_he.T)\n",
    "    x_random = torch.relu(x @ w_random.T)\n",
    "    \n",
    "    print(f\"Layer {i+1} variance - Xavier: {torch.var(x_xavier):.4f}, He: {torch.var(x_he):.4f}, Random: {torch.var(x_random):.4f}\")\n",
    "    \n",
    "    x = x_xavier  # Continue with Xavier for next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "**Formula:** $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$\n",
    "\n",
    "Converts real values to probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class classification with softmax\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "probabilities = torch.softmax(logits, dim=0)\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Probabilities: {probabilities}\")\n",
    "print(f\"Sum: {probabilities.sum()}\")\n",
    "\n",
    "# Temperature effects on softmax\n",
    "def softmax_with_temperature(logits, temperature=1.0):\n",
    "    return torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "temps = [0.1, 1.0, 10.0]\n",
    "for temp in temps:\n",
    "    probs = softmax_with_temperature(logits, temp)\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-8))\n",
    "    print(f\"Temperature {temp}: {probs.numpy()} (entropy: {entropy:.3f})\")\n",
    "\n",
    "# Attention mechanism using softmax\n",
    "query = torch.randn(1, 64)\n",
    "keys = torch.randn(10, 64)\n",
    "scores = query @ keys.T  # Attention scores\n",
    "attention_weights = torch.softmax(scores, dim=-1)\n",
    "print(f\"Attention weights sum: {attention_weights.sum():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy Loss\n",
    "\n",
    "**Formula:** $\\mathcal{L} = -\\sum_i y_i \\log(\\hat{y}_i)$\n",
    "\n",
    "Measures difference between predicted and true probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy for classification\n",
    "predictions = torch.tensor([[0.1, 0.8, 0.1], [0.7, 0.2, 0.1]])  # Predicted probabilities\n",
    "targets = torch.tensor([1, 0])  # True class indices\n",
    "\n",
    "ce_loss = torch.nn.functional.cross_entropy(torch.log(predictions), targets)\n",
    "manual_ce = -torch.mean(torch.log(predictions[range(len(targets)), targets]))\n",
    "\n",
    "print(f\"Cross-entropy loss: {ce_loss:.4f}\")\n",
    "print(f\"Manual calculation: {manual_ce:.4f}\")\n",
    "\n",
    "# Confidence and loss relationship\n",
    "confident_wrong = torch.tensor([[0.05, 0.05, 0.9]])  # Confident but wrong\n",
    "uncertain = torch.tensor([[0.4, 0.3, 0.3]])         # Uncertain\n",
    "target = torch.tensor([0])  # True class is 0\n",
    "\n",
    "loss_confident = torch.nn.functional.cross_entropy(torch.log(confident_wrong), target)\n",
    "loss_uncertain = torch.nn.functional.cross_entropy(torch.log(uncertain), target)\n",
    "\n",
    "print(f\"Confident wrong prediction loss: {loss_confident:.4f}\")\n",
    "print(f\"Uncertain prediction loss: {loss_uncertain:.4f}\")\n",
    "print(\"Cross-entropy heavily penalizes confident wrong predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}