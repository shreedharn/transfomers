{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus for Neural Networks\n",
    "\n",
    "This notebook contains PyTorch examples demonstrating calculus concepts essential for understanding neural networks.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Derivatives](#derivatives)\n",
    "2. [Partial Derivatives](#partial-derivatives)\n",
    "3. [Chain Rule](#chain-rule)\n",
    "4. [Gradient](#gradient)\n",
    "5. [Hessian](#hessian)\n",
    "6. [Jacobian](#jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives\n",
    "\n",
    "**Formula:** $f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$\n",
    "\n",
    "Foundation of gradient-based learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic differentiation in PyTorch\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3 + 2*x**2 + x + 1  # f(x) = x³ + 2x² + x + 1\n",
    "\n",
    "# Compute derivative automatically\n",
    "y.backward()\n",
    "print(f\"f({x.item()}) = {y.item()}\")\n",
    "print(f\"f'({x.item()}) = {x.grad.item()}\")\n",
    "\n",
    "# Manual verification: f'(x) = 3x² + 4x + 1\n",
    "manual_derivative = 3 * x.item()**2 + 4 * x.item() + 1\n",
    "print(f\"Manual calculation: f'(2) = {manual_derivative}\")\n",
    "\n",
    "# Loss function derivative example\n",
    "def simple_loss(w, x, y_true):\n",
    "    y_pred = w * x\n",
    "    return (y_pred - y_true)**2\n",
    "\n",
    "w = torch.tensor(1.5, requires_grad=True)\n",
    "x_val, y_true = 3.0, 10.0\n",
    "\n",
    "loss = simple_loss(w, x_val, y_true)\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nLoss: {loss.item():.3f}\")\n",
    "print(f\"Gradient w.r.t. weight: {w.grad.item():.3f}\")\n",
    "print(f\"Direction to move weight: {'decrease' if w.grad > 0 else 'increase'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Derivatives\n",
    "\n",
    "**Formula:** $\\frac{\\partial f}{\\partial x_i}$\n",
    "\n",
    "Derivative with respect to one variable while holding others constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-variable function\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "z = x**2 * y + x * y**2  # f(x,y) = x²y + xy²\n",
    "\n",
    "z.backward()\n",
    "print(f\"f({x.item()}, {y.item()}) = {z.item()}\")\n",
    "print(f\"∂f/∂x = {x.grad.item()}\")  # Should be 2xy + y²\n",
    "print(f\"∂f/∂y = {y.grad.item()}\")  # Should be x² + 2xy\n",
    "\n",
    "# Neural network layer with multiple parameters\n",
    "batch_size, input_dim, output_dim = 4, 3, 2\n",
    "X = torch.randn(batch_size, input_dim)\n",
    "W = torch.randn(output_dim, input_dim, requires_grad=True)\n",
    "b = torch.randn(output_dim, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "Y = X @ W.T + b\n",
    "loss = Y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nWeight gradients shape: {W.grad.shape}\")\n",
    "print(f\"Bias gradients shape: {b.grad.shape}\")\n",
    "print(f\"Each gradient shows how loss changes w.r.t. that parameter\")\n",
    "\n",
    "# Examine specific parameter gradients\n",
    "print(f\"∂loss/∂W[0,0] = {W.grad[0,0].item():.3f}\")\n",
    "print(f\"∂loss/∂b[0] = {b.grad[0].item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "**Formula:** $\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)$\n",
    "\n",
    "Mathematical foundation of backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual chain rule demonstration\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Composition: f(g(h(x))) where h(x)=x², g(u)=u+1, f(v)=v³\n",
    "h = x**2        # h(x) = x²\n",
    "g = h + 1       # g(h) = h + 1  \n",
    "f = g**3        # f(g) = g³\n",
    "\n",
    "f.backward()\n",
    "print(f\"Input: {x.item()}\")\n",
    "print(f\"h(x) = x² = {h.item()}\")\n",
    "print(f\"g(h) = h + 1 = {g.item()}\")\n",
    "print(f\"f(g) = g³ = {f.item()}\")\n",
    "print(f\"df/dx via chain rule: {x.grad.item()}\")\n",
    "\n",
    "# Manual verification: \n",
    "# df/dx = df/dg * dg/dh * dh/dx = 3g² * 1 * 2x = 3(x²+1)² * 2x\n",
    "manual = 3 * (x.item()**2 + 1)**2 * 2 * x.item()\n",
    "print(f\"Manual calculation: {manual}\")\n",
    "\n",
    "# Neural network chain rule\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, 3)\n",
    "        self.layer2 = torch.nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = torch.relu(self.layer1(x))  # First composition\n",
    "        h2 = self.layer2(h1)             # Second composition\n",
    "        return h2\n",
    "\n",
    "net = SimpleNet()\n",
    "x_input = torch.randn(1, 2)\n",
    "target = torch.randn(1, 1)\n",
    "\n",
    "output = net(x_input)\n",
    "loss = torch.nn.functional.mse_loss(output, target)\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nNetwork output: {output.item():.3f}\")\n",
    "print(f\"Loss: {loss.item():.3f}\")\n",
    "print(f\"Layer 1 weight gradients: {net.layer1.weight.grad[0][:2]}\")\n",
    "print(f\"Layer 2 weight gradients: {net.layer2.weight.grad[0][:2]}\")\n",
    "print(\"Gradients computed via automatic chain rule application\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "**Formula:** $\\nabla f = \\left[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\right]$\n",
    "\n",
    "Points in direction of steepest increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D function visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function: f(x,y) = x² + y² - 2x - 4y + 5 (has minimum at (1,2))\n",
    "def f(x, y):\n",
    "    return x**2 + y**2 - 2*x - 4*y + 5\n",
    "\n",
    "# Gradient: ∇f = [2x-2, 2y-4]\n",
    "def gradient(x, y):\n",
    "    return torch.tensor([2*x - 2, 2*y - 4])\n",
    "\n",
    "# Starting point\n",
    "position = torch.tensor([0.0, 0.0], requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "path = [position.detach().clone()]\n",
    "\n",
    "print(\"Gradient descent optimization:\")\n",
    "for step in range(10):\n",
    "    # Compute function value and gradient\n",
    "    x, y = position\n",
    "    loss = f(x, y)\n",
    "    \n",
    "    # Clear previous gradients\n",
    "    if position.grad is not None:\n",
    "        position.grad.zero_()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"Step {step}: pos=({x:.2f}, {y:.2f}), f={loss:.3f}, grad=({position.grad[0]:.2f}, {position.grad[1]:.2f})\")\n",
    "    \n",
    "    # Update position (gradient descent step)\n",
    "    with torch.no_grad():\n",
    "        position -= learning_rate * position.grad\n",
    "    \n",
    "    path.append(position.detach().clone())\n",
    "    \n",
    "    # Stop if gradient is small\n",
    "    if torch.norm(position.grad) < 0.01:\n",
    "        break\n",
    "\n",
    "print(f\"\\nFinal position: ({position[0]:.3f}, {position[1]:.3f})\")\n",
    "print(f\"Theoretical minimum: (1.000, 2.000)\")\n",
    "\n",
    "# Gradient-based feature importance\n",
    "model = torch.nn.Linear(5, 1)\n",
    "input_data = torch.randn(1, 5, requires_grad=True)\n",
    "target = torch.randn(1, 1)\n",
    "\n",
    "output = model(input_data)\n",
    "loss = torch.nn.functional.mse_loss(output, target)\n",
    "loss.backward()\n",
    "\n",
    "feature_importance = torch.abs(input_data.grad).squeeze()\n",
    "print(f\"\\nFeature importance (|gradient|): {feature_importance}\")\n",
    "print(f\"Most important feature: {feature_importance.argmax().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian\n",
    "\n",
    "**Formula:** $\\mathbf{H}_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$\n",
    "\n",
    "Matrix of second derivatives describing curvature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Hessian for simple function\n",
    "def quadratic_loss(x):\n",
    "    return 0.5 * (x[0]**2 + 2*x[1]**2 + x[0]*x[1])\n",
    "\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "loss = quadratic_loss(x)\n",
    "\n",
    "# Compute gradients\n",
    "grad = torch.autograd.grad(loss, x, create_graph=True)[0]\n",
    "\n",
    "# Compute Hessian (second derivatives)\n",
    "hessian = torch.zeros(2, 2)\n",
    "for i in range(2):\n",
    "    grad2 = torch.autograd.grad(grad[i], x, retain_graph=True)[0]\n",
    "    hessian[i] = grad2\n",
    "\n",
    "print(f\"Loss: {loss.item():.3f}\")\n",
    "print(f\"Gradient: {grad}\")\n",
    "print(f\"Hessian:\\n{hessian}\")\n",
    "\n",
    "# Condition number analysis\n",
    "eigenvals = torch.linalg.eigvals(hessian).real\n",
    "condition_number = eigenvals.max() / eigenvals.min()\n",
    "print(f\"Condition number: {condition_number:.2f}\")\n",
    "print(f\"Well-conditioned: {condition_number < 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian\n",
    "\n",
    "**Formula:** $\\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j}$\n",
    "\n",
    "Matrix of first derivatives for vector-valued functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector-valued function example\n",
    "def vector_function(x):\n",
    "    return torch.stack([\n",
    "        x[0]**2 + x[1],\n",
    "        x[0] * x[1],\n",
    "        torch.sin(x[0]) + torch.cos(x[1])\n",
    "    ])\n",
    "\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = vector_function(x)\n",
    "\n",
    "# Compute Jacobian\n",
    "jacobian = torch.zeros(3, 2)\n",
    "for i in range(3):\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    y[i].backward(retain_graph=True)\n",
    "    jacobian[i] = x.grad.clone()\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {y}\")\n",
    "print(f\"Jacobian:\\n{jacobian}\")\n",
    "\n",
    "# Neural network layer Jacobian\n",
    "layer = torch.nn.Linear(3, 2)\n",
    "x_batch = torch.randn(1, 3, requires_grad=True)\n",
    "y_batch = layer(x_batch)\n",
    "\n",
    "# Jacobian for neural network layer\n",
    "jac = torch.autograd.functional.jacobian(layer, x_batch)\n",
    "print(f\"NN Jacobian shape: {jac.shape}\")  # (batch, output_dim, batch, input_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}