{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch Tensors: Vectors & Matrices\n\n## üéØ Introduction\n\nWelcome to the foundational world of PyTorch tensors! This notebook will take you from complete beginner to confident tensor manipulator. Tensors are the fundamental data structure in PyTorch - they're like NumPy arrays but with superpowers: automatic differentiation, GPU acceleration, and deep integration with neural network operations.\n\n### üß† What You'll Learn\n\nBy the end of this notebook, you'll understand:\n- **Tensor fundamentals**: Creation, properties, and mental models\n- **Matrix operations**: The mathematical backbone of neural networks\n- **Broadcasting magic**: How PyTorch handles mismatched dimensions\n- **Embedding vs one-hot**: Why embeddings revolutionized NLP\n- **Batch processing**: The key to efficient neural network training\n\n### üéì Prerequisites\n\n- Basic Python knowledge (lists, functions, loops)\n- Elementary linear algebra (vectors, matrices, matrix multiplication)\n- No prior PyTorch experience needed!\n\n### üöÄ Why This Matters\n\nUnderstanding tensors is crucial because:\n- Every neural network operation is a tensor transformation\n- Efficient tensor operations = faster training and inference\n- Shape mismatches are the #1 source of PyTorch bugs\n- Proper tensor design enables scalable deep learning\n\n---\n\n## üìö Table of Contents\n\n1. **[Core Mental Model](#core-mental-model)** - Understanding what tensors really are\n2. **[Vector/Matrix Operations](#vector-matrix-operations)** - Essential operations for neural networks  \n3. **[One-Hot vs Embedding Vectors](#one-hot-vs-embedding-vectors)** - The embedding revolution explained\n4. **[Tiny Exercise: Batch Operations](#tiny-exercise-batch-operations)** - Putting it all together"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Essential imports for tensor operations\nimport torch                    # Core PyTorch library\nimport torch.nn as nn          # Neural network modules\nimport torch.nn.functional as F # Functional interface (activations, losses, etc.)\nimport numpy as np             # For comparison and some operations\n\n# Reproducibility setup - always do this first!\n# These ensure your results are consistent across runs\ntorch.manual_seed(0)    # Sets PyTorch random seed\nnp.random.seed(0)       # Sets NumPy random seed\n\n# Environment information - important for debugging\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Python executable: {__import__('sys').executable}\")\n\n# Device detection - CPU vs GPU\n# We'll use CPU for this tutorial to ensure reproducibility\navailable_device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice = torch.device('cpu')  # Force CPU for consistent results\n\nprint(f\"Available device: {available_device}\")\nprint(f\"Using device: {device}\")\n\n# Quick tensor creation test to verify everything works\ntest_tensor = torch.tensor([1.0, 2.0, 3.0])\nprint(f\"‚úì PyTorch is working! Test tensor: {test_tensor}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Core Mental Model\n\n### üß† What Is a Tensor?\n\nA **tensor** is a generalization of scalars, vectors, and matrices:\n- **Scalar** (0D): Just a number ‚Üí `5.0`\n- **Vector** (1D): Array of numbers ‚Üí `[1, 2, 3]`  \n- **Matrix** (2D): Grid of numbers ‚Üí `[[1, 2], [3, 4]]`\n- **Tensor** (3D+): Multi-dimensional array ‚Üí `[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]`\n\n### üéØ The Neural Network Connection\n\nIn neural networks, tensors represent:\n- **Inputs**: Your data (images, text, audio)\n- **Parameters**: Learnable weights and biases\n- **Activations**: Values flowing between layers\n- **Gradients**: How to update parameters\n\n### üîß PyTorch Tensors vs NumPy Arrays\n\n| Feature | NumPy Array | PyTorch Tensor |\n|---------|-------------|----------------|\n| GPU Support | ‚ùå | ‚úÖ |\n| Automatic Differentiation | ‚ùå | ‚úÖ |\n| Neural Network Integration | ‚ùå | ‚úÖ |\n| Scientific Computing | ‚úÖ | ‚úÖ |\n\n**Key insight**: PyTorch tensors are like NumPy arrays that know calculus!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TENSOR CREATION PATTERNS\n# =============================================================================\n\nprint(\"üîß Basic Tensor Creation\")\nprint(\"=\" * 50)\n\n# Method 1: From Python lists/numbers\nscalar = torch.tensor(5.0)                    # 0D tensor (scalar)\nvector = torch.tensor([1.0, 2.0, 3.0])       # 1D tensor (vector)\nmatrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])  # 2D tensor (matrix)\n\nprint(f\"Scalar: {scalar}, shape: {scalar.shape}, dimensions: {scalar.ndim}\")\nprint(f\"Vector: {vector}, shape: {vector.shape}, dimensions: {vector.ndim}\")\nprint(f\"Matrix: {matrix}, shape: {matrix.shape}, dimensions: {matrix.ndim}\")\n\nprint(\"\\nüèóÔ∏è Common Creation Patterns\")\nprint(\"=\" * 50)\n\n# Method 2: Structural creation (most common in practice)\nzeros_2d = torch.zeros(3, 4)              # 3√ó4 matrix filled with zeros\nones_3d = torch.ones(2, 3, 4)             # 2√ó3√ó4 tensor filled with ones\nrandn_2d = torch.randn(2, 5)              # Random normal (mean=0, std=1)\narange_1d = torch.arange(0, 10, 2)        # Evenly spaced: [0, 2, 4, 6, 8]\nlinspace_1d = torch.linspace(0, 1, 5)     # 5 points from 0 to 1\n\nprint(f\"Zeros matrix shape: {zeros_2d.shape}\")\nprint(f\"Ones tensor shape: {ones_3d.shape}\")\nprint(f\"Random normal shape: {randn_2d.shape}\")\nprint(f\"Arange result: {arange_1d}\")\nprint(f\"Linspace result: {linspace_1d}\")\n\nprint(\"\\nüîç Tensor Properties\")\nprint(\"=\" * 50)\n\n# Every tensor has these key properties\nexample_tensor = torch.randn(2, 3, 4)\n\nprint(f\"Shape (dimensions): {example_tensor.shape}\")      # torch.Size([2, 3, 4])\nprint(f\"Data type: {example_tensor.dtype}\")               # torch.float32 (default)\nprint(f\"Device location: {example_tensor.device}\")        # cpu or cuda\nprint(f\"Requires gradients: {example_tensor.requires_grad}\")  # False (default)\nprint(f\"Number of elements: {example_tensor.numel()}\")    # 2 √ó 3 √ó 4 = 24\nprint(f\"Memory layout: {example_tensor.stride()}\")        # How data is stored\n\nprint(\"\\nüí° Key Insight: Shape is Everything!\")\nprint(\"In deep learning, getting tensor shapes right is 80% of the battle.\")\nprint(\"Always think: [batch_size, sequence_length, feature_dimension]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Vector/Matrix Operations\n\n### üéØ The Heart of Neural Networks\n\nMatrix multiplication is the fundamental operation in neural networks. Every layer transformation, attention mechanism, and parameter update relies on efficient matrix operations. Let's master the essential patterns!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# NEURAL NETWORK MATRIX OPERATIONS\n# =============================================================================\n\nprint(\"üßÆ The Linear Layer Pattern\")\nprint(\"=\" * 50)\n\n# This is the most important pattern in deep learning!\n# Every linear layer does: output = input @ weight + bias\n\nbatch_size, d_input, d_hidden = 4, 6, 8\n\n# Typical neural network shapes\nX = torch.randn(batch_size, d_input)    # Input batch: [batch_size, input_features]\nW = torch.randn(d_input, d_hidden)      # Weight matrix: [input_features, output_features]  \nb = torch.randn(d_hidden)               # Bias vector: [output_features]\n\nprint(f\"Input X shape: {X.shape} - {batch_size} samples, {d_input} features each\")\nprint(f\"Weight W shape: {W.shape} - transforms {d_input} ‚Üí {d_hidden} features\")\nprint(f\"Bias b shape: {b.shape} - one bias per output feature\")\n\n# The fundamental neural network operation\ny = X @ W + b  # Matrix multiplication + broadcasting bias addition\n\nprint(f\"\\nOutput y shape: {y.shape} - {batch_size} samples, {d_hidden} features each\")\nprint(f\"‚úì Linear transformation complete: {d_input} ‚Üí {d_hidden} dimensions\")\n\nprint(\"\\nüéØ Why This Shape Pattern Matters\")\nprint(\"- Batch dimension (first) allows parallel processing\")\nprint(\"- Feature dimension (last) is what gets transformed\")\nprint(\"- This pattern scales from tiny MLPs to massive transformers\")\n\nprint(\"\\nüî™ Tensor Slicing and Indexing\")\nprint(\"=\" * 50)\n\n# Indexing patterns you'll use constantly\nfirst_sample = X[0]              # Get first sample: [d_input]\nfirst_two = X[:2]                # Get first two samples: [2, d_input]\nlast_feature = X[..., -1]        # Last feature across all samples: [batch_size]\nmiddle_features = X[:, 1:4]      # Features 1-3 for all samples: [batch_size, 3]\n\nprint(f\"First sample shape: {first_sample.shape}\")\nprint(f\"First two samples shape: {first_two.shape}\")\nprint(f\"Last feature across batch shape: {last_feature.shape}\")\nprint(f\"Middle features shape: {middle_features.shape}\")\n\nprint(\"\\nüèóÔ∏è Stacking vs Concatenation\")\nprint(\"=\" * 50)\n\n# Two fundamental ways to combine tensors\nX1 = torch.randn(4, 6)\nX2 = torch.randn(4, 6)\n\n# Stack: Creates NEW dimension\nstacked = torch.stack([X1, X2], dim=0)    # [2, 4, 6] - NEW first dimension\nstacked_last = torch.stack([X1, X2], dim=-1)  # [4, 6, 2] - NEW last dimension\n\n# Cat: Concatenates along EXISTING dimension  \nconcat_batch = torch.cat([X1, X2], dim=0)     # [8, 6] - double the batch size\nconcat_features = torch.cat([X1, X2], dim=1)  # [4, 12] - double the features\n\nprint(f\"Original X1, X2 shapes: {X1.shape}, {X2.shape}\")\nprint(f\"Stack (dim=0): {stacked.shape} - creates new dimension\")\nprint(f\"Stack (dim=-1): {stacked_last.shape} - creates new last dimension\")\nprint(f\"Cat (dim=0): {concat_batch.shape} - bigger batch\")\nprint(f\"Cat (dim=1): {concat_features.shape} - more features\")\n\nprint(\"\\nüí° Pro Tip:\")\nprint(\"- Use stack() when you want to create batches\")\nprint(\"- Use cat() when you want to combine features or increase batch size\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## One-Hot vs Embedding Vectors\n\n### üöÄ The Embedding Revolution\n\nOne of the biggest breakthroughs in deep learning was replacing sparse one-hot vectors with dense embedding vectors. This single change enabled the transformer revolution and modern NLP. Let's see why embeddings are so powerful!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# ONE-HOT vs EMBEDDINGS: THE GREAT TRANSFORMATION\n# =============================================================================\n\nprint(\"üî• The Old Way: One-Hot Vectors\")\nprint(\"=\" * 50)\n\n# One-hot: sparse, large, fixed representation\nvocab_size = 1000      # Typical small vocabulary\ntoken_id = 42          # Word \"hello\" might be token 42\n\n# Create one-hot vector - mostly zeros!\nonehot = torch.zeros(vocab_size)\nonehot[token_id] = 1.0\n\nprint(f\"One-hot vector size: {onehot.shape}\")\nprint(f\"Memory usage: {onehot.numel()} floats\")\nprint(f\"Mostly zeros: {torch.sum(onehot == 0).item()} zeros out of {onehot.numel()}\")\nprint(f\"Sparsity: {(onehot == 0).float().mean().item():.1%}\")\nprint(f\"Sample values: [{onehot[40]:.0f}, {onehot[41]:.0f}, {onehot[42]:.0f}, {onehot[43]:.0f}, {onehot[44]:.0f}]\")\n\nprint(\"\\n‚ú® The New Way: Dense Embeddings\")\nprint(\"=\" * 50)\n\n# Embedding: dense, compact, learnable representation\nembedding_dim = 64\nembedding_layer = nn.Embedding(vocab_size, embedding_dim)\n\n# Get embedding for the same token\nembedded = embedding_layer(torch.tensor([token_id]))\n\nprint(f\"Embedding vector size: {embedded.shape}\")\nprint(f\"Memory usage: {embedded.numel()} floats\")\nprint(f\"All values meaningful: no zeros wasted\")\nprint(f\"Density: 100% of values carry information\")\nprint(f\"Sample values: {embedded[0, :5]}\")  # First 5 dimensions\n\nprint(\"\\nüìä Efficiency Comparison\")\nprint(\"=\" * 50)\n\n# Memory efficiency\nonehot_memory = vocab_size * 4  # 4 bytes per float32\nembedding_memory = embedding_dim * 4  # 4 bytes per float32\n\nprint(f\"One-hot memory per token: {onehot_memory:,} bytes\")\nprint(f\"Embedding memory per token: {embedding_memory:,} bytes\")\nprint(f\"Memory savings: {onehot_memory / embedding_memory:.1f}x smaller\")\n\n# Computational efficiency\nprint(f\"\\nComputational efficiency:\")\nprint(f\"One-hot @ weight: {vocab_size} √ó {embedding_dim} = {vocab_size * embedding_dim:,} operations\")\nprint(f\"Embedding lookup: {embedding_dim} operations (just indexing!)\")\nprint(f\"Speed improvement: {(vocab_size * embedding_dim) // embedding_dim}x faster\")\n\nprint(\"\\nüß† Learning and Representation Power\")\nprint(\"=\" * 50)\n\n# Demonstrate batch processing\nbatch_tokens = torch.tensor([1, 5, 10, 42, 100, 999])  # Batch of different tokens\nbatch_embedded = embedding_layer(batch_tokens)\n\nprint(f\"Batch tokens: {batch_tokens}\")\nprint(f\"Batch embeddings shape: {batch_embedded.shape}\")\nprint(f\"Processing {len(batch_tokens)} tokens simultaneously\")\n\n# Show that embeddings are learnable parameters\nprint(f\"\\nEmbedding layer parameters: {embedding_layer.weight.numel():,}\")\nprint(f\"Embedding weight matrix shape: {embedding_layer.weight.shape}\")\nprint(f\"Each row is a learnable representation for one token\")\n\nprint(\"\\nüéØ Why Embeddings Won\")\nprint(\"=\" * 50)\nprint(\"1. üöÄ EFFICIENCY:\")\nprint(f\"   - Memory: {embedding_dim} floats vs {vocab_size:,} floats\")\nprint(\"   - Computation: Lookup vs matrix multiplication\")\nprint(\"   - Storage: Dense vs sparse operations\")\n\nprint(\"\\n2. üß† LEARNING:\")\nprint(\"   - One-hot: Fixed, no learning possible\")\nprint(\"   - Embeddings: Every dimension is trainable\")\nprint(\"   - Semantic relationships emerge automatically\")\n\nprint(\"\\n3. üîó COMPOSITIONALITY:\")\nprint(\"   - Similar words get similar embeddings\")\nprint(\"   - Math operations work: king - man + woman ‚âà queen\")\nprint(\"   - Transfer learning becomes possible\")\n\nprint(\"\\nüí° Modern Impact:\")\nprint(\"Embeddings enabled BERT, GPT, and all modern NLP models.\")\nprint(\"The same concept now works for images, code, and multimodal data!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Tiny Exercise: Batch Operations\n\n### üéØ Putting It All Together\n\nLet's practice the fundamental tensor operations you'll use in every neural network. We'll work with realistic batch sizes and dimensions, just like in real deep learning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# COMPREHENSIVE TENSOR EXERCISE\n# =============================================================================\n\nprint(\"üéØ Neural Network Simulation\")\nprint(\"=\" * 50)\n\n# Realistic neural network dimensions\nbatch_size = 3        # Small batch for clarity\nd_model = 4          # Feature dimension (like a tiny transformer)\n\n# Step 1: Create input batch (like tokenized text)\nprint(\"Step 1: Input Data\")\nX_batch = torch.randn(batch_size, d_model)  # [batch_size, features]\nW_linear = torch.randn(d_model, d_model)    # [input_dim, output_dim] \nb_linear = torch.randn(d_model)             # [output_dim]\n\nprint(f\"Input batch shape: {X_batch.shape}\")\nprint(f\"Weight matrix shape: {W_linear.shape}\")\nprint(f\"Bias vector shape: {b_linear.shape}\")\n\n# Step 2: Linear transformation (core of every neural layer)\nprint(f\"\\nStep 2: Linear Transformation\")\ny_batch = X_batch @ W_linear + b_linear     # Matrix multiplication + bias\n\nprint(f\"Output shape: {y_batch.shape}\")\nprint(f\"‚úì Shape check: input {X_batch.shape} ‚Üí output {y_batch.shape}\")\n\n# Verify our understanding with assertions\nassert X_batch.shape == (batch_size, d_model), f\"Expected {(batch_size, d_model)}, got {X_batch.shape}\"\nassert y_batch.shape == (batch_size, d_model), f\"Expected {(batch_size, d_model)}, got {y_batch.shape}\"\nprint(\"‚úì All shape assertions passed!\")\n\nprint(f\"\\nStep 3: Common Neural Network Operations\")\nprint(\"=\" * 50)\n\n# Element-wise operations (activations)\nrelu_output = torch.relu(y_batch)           # ReLU: max(0, x)\nsquared = torch.pow(y_batch, 2)             # Element-wise square\nexp_output = torch.exp(y_batch)             # Element-wise exponential\nsigmoid_output = torch.sigmoid(y_batch)     # Sigmoid activation\n\nprint(f\"Original output (sample):\\n{y_batch[0]}\")                    # First sample\nprint(f\"After ReLU (negatives ‚Üí 0):\\n{relu_output[0]}\")\nprint(f\"After sigmoid (0-1 range):\\n{sigmoid_output[0]}\")\n\nprint(f\"\\nStep 4: Reduction Operations\")\nprint(\"=\" * 50)\n\n# Reductions - crucial for pooling and attention\nsum_all = torch.sum(y_batch)                    # Sum all elements ‚Üí scalar\nsum_batch_dim = torch.sum(y_batch, dim=0)       # Sum across batch ‚Üí [d_model]\nsum_feature_dim = torch.sum(y_batch, dim=1)     # Sum across features ‚Üí [batch_size]\nmean_batch = torch.mean(y_batch, dim=0)         # Mean across batch ‚Üí [d_model]\n\nprint(f\"Sum all elements: {sum_all.item():.3f} (scalar)\")\nprint(f\"Sum across batch: {sum_batch_dim} (shape: {sum_batch_dim.shape})\")\nprint(f\"Sum across features: {sum_feature_dim} (shape: {sum_feature_dim.shape})\")\nprint(f\"Mean across batch: {mean_batch} (shape: {mean_batch.shape})\")\n\nprint(f\"\\nStep 5: Broadcasting Magic\")\nprint(\"=\" * 50)\n\n# Broadcasting: PyTorch's secret sauce for shape flexibility\nA = torch.randn(3, 1)    # [3, 1] - 3 rows, 1 column\nB = torch.randn(1, 4)    # [1, 4] - 1 row, 4 columns\n\nprint(f\"Tensor A shape: {A.shape}\")\nprint(f\"Tensor B shape: {B.shape}\")\n\n# Broadcasting automatically expands dimensions\nC = A + B                # [3, 1] + [1, 4] ‚Üí [3, 4]\n\nprint(f\"A + B result shape: {C.shape} (broadcasted!)\")\nprint(\"Broadcasting rule: dimensions align from the right, 1s expand to match\")\n\n# Manual verification of broadcasting\nA_expanded = A.expand(3, 4)  # [3, 1] ‚Üí [3, 4]\nB_expanded = B.expand(3, 4)  # [1, 4] ‚Üí [3, 4]\nC_manual = A_expanded + B_expanded\n\nprint(f\"Manual expansion matches: {torch.allclose(C, C_manual)}\")\n\nprint(f\"\\nStep 6: Real-World Pattern - Attention Scores\")\nprint(\"=\" * 50)\n\n# Simulate attention mechanism pattern\nseq_len = 5\nquery = torch.randn(batch_size, seq_len, d_model)    # [B, seq_len, d_model]\nkey = torch.randn(batch_size, seq_len, d_model)      # [B, seq_len, d_model]\n\n# Attention scores: Q @ K^T\nattention_scores = torch.matmul(query, key.transpose(-2, -1))  # [B, seq_len, seq_len]\n\nprint(f\"Query shape: {query.shape}\")\nprint(f\"Key shape: {key.shape}\")\nprint(f\"Attention scores shape: {attention_scores.shape}\")\nprint(f\"‚úì This is how transformers compute attention!\")\n\nprint(f\"\\nüéâ Congratulations!\")\nprint(\"=\" * 50)\nprint(\"You've mastered the fundamental tensor operations of deep learning:\")\nprint(\"‚úì Tensor creation and properties\")\nprint(\"‚úì Matrix multiplication and broadcasting\")\nprint(\"‚úì Element-wise operations and reductions\")\nprint(\"‚úì Real neural network patterns\")\nprint(\"‚úì Shape thinking and debugging\")\n\nprint(f\"\\nüöÄ Next Steps:\")\nprint(\"- Learn about automatic differentiation (autograd)\")\nprint(\"- Understand how tensors become neural network layers\")\nprint(\"- Explore GPU acceleration and optimization\")\nprint(\"- Build your first neural network!\")\n\nprint(f\"\\nüí° Key Insight:\")\nprint(\"Deep learning is just smart tensor manipulation.\")\nprint(\"Master tensors, master deep learning!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}