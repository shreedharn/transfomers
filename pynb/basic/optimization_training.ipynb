{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# PyTorch Optimization & Training: Making Neural Networks Learn\n\n## 🎯 Introduction\n\nWelcome to the heart of deep learning - the training process! This notebook will transform you from someone who can build neural networks into someone who can train them effectively. Understanding optimization is what separates toy examples from production-ready models.\n\n### 🧠 What You'll Master\n\nThis comprehensive guide covers:\n- **Training loop architecture**: The fundamental structure of neural network training\n- **Optimizer comparison**: SGD, Adam, AdamW, and when to use each\n- **Loss function selection**: Choosing the right objective for your problem\n- **Learning rate strategies**: Schedules, warmup, and adaptive methods\n- **Training stability**: Preventing divergence and ensuring convergence\n\n### 🎓 Prerequisites\n\n- Solid understanding of modules, parameters, and autograd\n- Familiarity with neural network architecture concepts\n- Basic knowledge of gradient descent and backpropagation\n\n### 🚀 Why Training Mastery Matters\n\nEffective training enables:\n- **Convergence**: Reaching optimal solutions reliably\n- **Efficiency**: Training faster with fewer resources\n- **Stability**: Avoiding common training failures\n- **Generalization**: Models that work on new data\n- **Scalability**: Techniques that work from tiny to massive models\n\n---\n\n## 📚 Table of Contents\n\n1. **[Training Loop Fundamentals](#training-loop-fundamentals)** - The core structure of neural network training\n2. **[Optimizer Deep Dive](#optimizer-deep-dive)** - Understanding and comparing optimization algorithms\n3. **[Loss Functions & Objectives](#loss-functions-objectives)** - Choosing the right training signal\n4. **[Learning Rate Strategies](#learning-rate-strategies)** - Schedules and adaptive methods\n5. **[Training Stability & Debugging](#training-stability-debugging)** - Ensuring reliable convergence"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Training Loop Fundamentals\n\n### 🔄 The Heart of Deep Learning\n\nEvery neural network training follows the same fundamental pattern: forward pass, loss computation, backward pass, parameter update. Let's master this essential cycle and understand why each step matters!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# THE COMPLETE TRAINING LOOP ARCHITECTURE\n# =============================================================================\n\nprint(\"🔄 Training Loop Mastery\")\nprint(\"=\" * 50)\n\n# Create a simple model and dataset for training demonstration\nclass SimpleRegressor(nn.Module):\n    \"\"\"Simple model for demonstrating training concepts.\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(), \n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, x):\n        return self.network(x)\n\n# Generate synthetic dataset for training\ntorch.manual_seed(42)  # For reproducibility\n\n# Create synthetic data: y = 2*x1 + 3*x2 + noise\nn_samples, input_dim = 1000, 5\nX = torch.randn(n_samples, input_dim)\n# Only first two features matter, rest are noise\ntrue_weights = torch.tensor([2.0, 3.0, 0.0, 0.0, 0.0])\ny = X @ true_weights + 0.1 * torch.randn(n_samples)  # Add noise\n\nprint(f\"Dataset created: {n_samples} samples, {input_dim} features\")\nprint(f\"True relationship: y = 2*x1 + 3*x2 + noise\")\n\n# Split into train/validation\ntrain_size = int(0.8 * n_samples)\nX_train, X_val = X[:train_size], X[train_size:]\ny_train, y_val = y[:train_size], y[train_size:]\n\nprint(f\"Train set: {X_train.shape[0]} samples\")\nprint(f\"Validation set: {X_val.shape[0]} samples\")\n\n# Initialize model, loss, and optimizer\nmodel = SimpleRegressor(input_dim=input_dim, hidden_dim=32, output_dim=1)\ncriterion = nn.MSELoss()  # Mean Squared Error for regression\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nprint(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n🎯 The Universal Training Loop\")\nprint(\"=\" * 50)\n\n# Training configuration\nnum_epochs = 100\nbatch_size = 32\n\n# Create data loaders for batching\nfrom torch.utils.data import DataLoader, TensorDataset\n\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\nval_dataset = TensorDataset(X_val, y_val)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# Training loop with detailed commentary\ntrain_losses = []\nval_losses = []\n\nprint(\"Training progress (every 20 epochs):\")\nprint(\"Epoch | Train Loss | Val Loss   | Notes\")\nprint(\"------|------------|------------|-------------\")\n\nfor epoch in range(num_epochs):\n    # =================================\n    # TRAINING PHASE\n    # =================================\n    model.train()  # Set to training mode (affects dropout, batchnorm)\n    epoch_train_loss = 0.0\n    num_train_batches = 0\n    \n    for batch_idx, (batch_X, batch_y) in enumerate(train_loader):\n        # Step 1: Zero gradients from previous iteration\n        # CRITICAL: Without this, gradients accumulate!\n        optimizer.zero_grad()\n        \n        # Step 2: Forward pass - compute predictions\n        predictions = model(batch_X)\n        \n        # Step 3: Compute loss between predictions and targets\n        loss = criterion(predictions.squeeze(), batch_y)\n        \n        # Step 4: Backward pass - compute gradients\n        loss.backward()\n        \n        # Step 5: Update parameters using computed gradients\n        optimizer.step()\n        \n        # Track training progress\n        epoch_train_loss += loss.item()\n        num_train_batches += 1\n    \n    avg_train_loss = epoch_train_loss / num_train_batches\n    train_losses.append(avg_train_loss)\n    \n    # =================================\n    # VALIDATION PHASE\n    # =================================\n    model.eval()  # Set to evaluation mode\n    epoch_val_loss = 0.0\n    num_val_batches = 0\n    \n    with torch.no_grad():  # Disable gradient computation for efficiency\n        for batch_X, batch_y in val_loader:\n            predictions = model(batch_X)\n            loss = criterion(predictions.squeeze(), batch_y)\n            epoch_val_loss += loss.item()\n            num_val_batches += 1\n    \n    avg_val_loss = epoch_val_loss / num_val_batches\n    val_losses.append(avg_val_loss)\n    \n    # Progress reporting\n    if epoch % 20 == 0 or epoch == num_epochs - 1:\n        if epoch == 0:\n            note = \"Initial random weights\"\n        elif avg_val_loss < min(val_losses[:-1]):\n            note = \"New best validation!\"\n        elif avg_train_loss < 0.01:\n            note = \"Near convergence\"\n        else:\n            note = \"Training...\"\n            \n        print(f\"{epoch:5d} | {avg_train_loss:10.6f} | {avg_val_loss:10.6f} | {note}\")\n\nprint(f\"\\n✅ Training completed!\")\nprint(f\"Final train loss: {train_losses[-1]:.6f}\")\nprint(f\"Final validation loss: {val_losses[-1]:.6f}\")\n\n# Check how well we learned the true relationship\nlearned_weights = model.network[0].weight[0].detach()  # First layer weights\nprint(f\"\\nLearned vs True weights (first 2 should be ~2.0, ~3.0):\")\nfor i, (learned, true) in enumerate(zip(learned_weights, true_weights)):\n    print(f\"  Feature {i+1}: learned={learned.item():6.3f}, true={true.item():6.3f}\")\n\nprint(f\"\\n🔍 Key Training Loop Elements\")\nprint(\"=\" * 50)\nprint(\"1. **model.train()**: Enable training mode (dropout, batchnorm active)\")\nprint(\"2. **optimizer.zero_grad()**: Clear gradients from previous iteration\")\nprint(\"3. **Forward pass**: Compute predictions from inputs\")\nprint(\"4. **Loss computation**: Measure prediction quality\")\nprint(\"5. **loss.backward()**: Compute gradients via backpropagation\")\nprint(\"6. **optimizer.step()**: Update parameters using gradients\")\nprint(\"7. **model.eval()**: Switch to evaluation mode for validation\")\nprint(\"8. **torch.no_grad()**: Disable gradients during validation\")\n\nprint(f\"\\n⚡ Performance Insights\")\nprint(\"=\" * 50)\nbest_val_epoch = val_losses.index(min(val_losses))\nprint(f\"Best validation loss: {min(val_losses):.6f} at epoch {best_val_epoch}\")\nprint(f\"Training efficiency: {'High' if train_losses[-1] < 0.01 else 'Moderate'}\")\nprint(f\"Overfitting check: {'Good' if val_losses[-1] < 2 * min(val_losses) else 'Check'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using our universal training loop\n",
    "model = SimpleClassifier(input_size=20, hidden_size=128, num_classes=5)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=10,\n",
    "    lr=0.001,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "def plot_training_curves(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    if history['val_loss']:\n",
    "        ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n",
    "    if history['val_acc']:\n",
    "        ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(f\"Final Training Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Training Accuracy: {history['train_acc'][-1]:.2f}%\")\n",
    "    if history['val_loss']:\n",
    "        print(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\")\n",
    "        print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.2f}%\")\n",
    "\n",
    "plot_training_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Optimizers: SGD vs AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different optimizers\n",
    "def compare_optimizers():\n",
    "    \"\"\"Compare SGD, Adam, and AdamW optimizers\"\"\"\n",
    "    \n",
    "    # Create identical models\n",
    "    models = {\n",
    "        'SGD': SimpleClassifier(20, 128, 5),\n",
    "        'Adam': SimpleClassifier(20, 128, 5),\n",
    "        'AdamW': SimpleClassifier(20, 128, 5)\n",
    "    }\n",
    "    \n",
    "    # Make sure they start with identical weights\n",
    "    state_dict = models['SGD'].state_dict()\n",
    "    for model in models.values():\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Create optimizers\n",
    "    optimizers = {\n",
    "        'SGD': optim.SGD(models['SGD'].parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4),\n",
    "        'Adam': optim.Adam(models['Adam'].parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4),\n",
    "        'AdamW': optim.AdamW(models['AdamW'].parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "    }\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Track progress for each optimizer\n",
    "    histories = {name: {'loss': [], 'acc': []} for name in models.keys()}\n",
    "    \n",
    "    num_epochs = 15\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for name in models.keys():\n",
    "            model = models[name]\n",
    "            optimizer = optimizers[name]\n",
    "            \n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for data, target in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            accuracy = 100.0 * correct / total\n",
    "            \n",
    "            histories[name]['loss'].append(avg_loss)\n",
    "            histories[name]['acc'].append(accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:2d}:\")\n",
    "            for name in models.keys():\n",
    "                loss = histories[name]['loss'][-1]\n",
    "                acc = histories[name]['acc'][-1]\n",
    "                print(f\"  {name:5s}: Loss = {loss:.4f}, Acc = {acc:.2f}%\")\n",
    "    \n",
    "    return histories\n",
    "\n",
    "# Compare optimizers\n",
    "print(\"Comparing optimizers...\")\n",
    "optimizer_histories = compare_optimizers()\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs = range(1, len(optimizer_histories['SGD']['loss']) + 1)\n",
    "colors = {'SGD': 'blue', 'Adam': 'green', 'AdamW': 'red'}\n",
    "\n",
    "# Loss comparison\n",
    "for name, history in optimizer_histories.items():\n",
    "    ax1.plot(epochs, history['loss'], color=colors[name], label=f'{name}', marker='o', markersize=3)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Optimizer Comparison: Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "for name, history in optimizer_histories.items():\n",
    "    ax2.plot(epochs, history['acc'], color=colors[name], label=f'{name}', marker='o', markersize=3)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Optimizer Comparison: Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final comparison\n",
    "print(\"\\nFinal Results:\")\n",
    "for name, history in optimizer_histories.items():\n",
    "    final_loss = history['loss'][-1]\n",
    "    final_acc = history['acc'][-1]\n",
    "    print(f\"{name:5s}: Loss = {final_loss:.4f}, Accuracy = {final_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nKey differences:\")\n",
    "print(\"• SGD: Simple, requires learning rate tuning, benefits from momentum\")\n",
    "print(\"• Adam: Adaptive learning rates, fast convergence, can overfit\")\n",
    "print(\"• AdamW: Adam with decoupled weight decay, better generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Common Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different loss functions\n",
    "print(\"=== Common Loss Functions Demo ===\")\n",
    "\n",
    "# 1. Classification: CrossEntropyLoss\n",
    "print(\"\\n1. CrossEntropyLoss (Multi-class Classification)\")\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create sample data\n",
    "batch_size, num_classes = 4, 5\n",
    "logits = torch.randn(batch_size, num_classes)  # Raw scores from model\n",
    "targets = torch.randint(0, num_classes, (batch_size,))  # Class indices\n",
    "\n",
    "loss_ce = ce_loss(logits, targets)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "print(f\"Targets: {targets}\")\n",
    "print(f\"CrossEntropyLoss: {loss_ce.item():.4f}\")\n",
    "\n",
    "# Show what happens with perfect predictions\n",
    "perfect_logits = torch.zeros_like(logits)\n",
    "for i, target in enumerate(targets):\n",
    "    perfect_logits[i, target] = 10.0  # High score for correct class\n",
    "perfect_loss = ce_loss(perfect_logits, targets)\n",
    "print(f\"Perfect prediction loss: {perfect_loss.item():.4f}\")\n",
    "\n",
    "# 2. Regression: MSELoss\n",
    "print(\"\\n2. MSELoss (Regression)\")\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "predictions = torch.randn(batch_size, 1)\n",
    "true_values = torch.randn(batch_size, 1)\n",
    "\n",
    "loss_mse = mse_loss(predictions, true_values)\n",
    "print(f\"Predictions: {predictions.squeeze()}\")\n",
    "print(f\"True values: {true_values.squeeze()}\")\n",
    "print(f\"MSELoss: {loss_mse.item():.4f}\")\n",
    "\n",
    "# Show difference between MSE and MAE\n",
    "mae_loss = nn.L1Loss()\n",
    "loss_mae = mae_loss(predictions, true_values)\n",
    "print(f\"MAE (L1) Loss: {loss_mae.item():.4f}\")\n",
    "\n",
    "# 3. Binary Classification: BCEWithLogitsLoss\n",
    "print(\"\\n3. BCEWithLogitsLoss (Binary Classification)\")\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "binary_logits = torch.randn(batch_size, 1)  # Raw logits\n",
    "binary_targets = torch.randint(0, 2, (batch_size, 1)).float()  # 0 or 1\n",
    "\n",
    "loss_bce = bce_loss(binary_logits, binary_targets)\n",
    "print(f\"Binary logits: {binary_logits.squeeze()}\")\n",
    "print(f\"Binary targets: {binary_targets.squeeze()}\")\n",
    "print(f\"BCEWithLogitsLoss: {loss_bce.item():.4f}\")\n",
    "\n",
    "# Compare with manual BCE calculation\n",
    "probs = torch.sigmoid(binary_logits)\n",
    "print(f\"Converted to probabilities: {probs.squeeze()}\")\n",
    "\n",
    "# 4. Negative Log Likelihood: NLLLoss\n",
    "print(\"\\n4. NLLLoss (when you already have log probabilities)\")\n",
    "nll_loss = nn.NLLLoss()\n",
    "\n",
    "# NLLLoss expects log probabilities\n",
    "log_probs = F.log_softmax(logits, dim=1)\n",
    "loss_nll = nll_loss(log_probs, targets)\n",
    "\n",
    "print(f\"Log probabilities shape: {log_probs.shape}\")\n",
    "print(f\"NLLLoss: {loss_nll.item():.4f}\")\n",
    "print(f\"Note: CrossEntropyLoss = LogSoftmax + NLLLoss\")\n",
    "print(f\"Verification - CE loss: {loss_ce.item():.4f}, NLL loss: {loss_nll.item():.4f}\")\n",
    "print(f\"Match: {abs(loss_ce.item() - loss_nll.item()) < 1e-6}\")\n",
    "\n",
    "# 5. Multi-label classification: BCEWithLogitsLoss with multiple outputs\n",
    "print(\"\\n5. Multi-label Classification\")\n",
    "num_labels = 3\n",
    "multi_logits = torch.randn(batch_size, num_labels)\n",
    "multi_targets = torch.randint(0, 2, (batch_size, num_labels)).float()  # Multiple binary labels\n",
    "\n",
    "multi_bce_loss = nn.BCEWithLogitsLoss()\n",
    "loss_multi = multi_bce_loss(multi_logits, multi_targets)\n",
    "\n",
    "print(f\"Multi-label logits shape: {multi_logits.shape}\")\n",
    "print(f\"Multi-label targets shape: {multi_targets.shape}\")\n",
    "print(f\"Multi-label BCE loss: {loss_multi.item():.4f}\")\n",
    "print(f\"Sample targets: {multi_targets[0]}  (can have multiple 1s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Train vs Eval Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate train vs eval mode differences\n",
    "print(\"=== Train vs Eval Mode Demo ===\")\n",
    "\n",
    "# Create model with dropout and batch norm\n",
    "class ModelWithDropoutAndBN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 50% dropout\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size//2)\n",
    "        self.dropout2 = nn.Dropout(0.3)  # 30% dropout\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size//2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ModelWithDropoutAndBN(20, 128, 5)\n",
    "x = torch.randn(10, 20)  # Batch of 10 samples\n",
    "\n",
    "print(\"1. Dropout behavior:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Training mode: dropout is active\n",
    "model.train()\n",
    "print(\"Training mode (dropout active):\")\n",
    "out1 = model(x)\n",
    "out2 = model(x)  # Same input, different output due to dropout\n",
    "\n",
    "print(f\"Output 1 mean: {out1.mean().item():.4f}\")\n",
    "print(f\"Output 2 mean: {out2.mean().item():.4f}\")\n",
    "print(f\"Outputs are different: {not torch.allclose(out1, out2)}\")\n",
    "\n",
    "# Evaluation mode: dropout is disabled\n",
    "model.eval()\n",
    "print(\"\\nEvaluation mode (dropout disabled):\")\n",
    "out3 = model(x)\n",
    "out4 = model(x)  # Same input, same output\n",
    "\n",
    "print(f\"Output 3 mean: {out3.mean().item():.4f}\")\n",
    "print(f\"Output 4 mean: {out4.mean().item():.4f}\")\n",
    "print(f\"Outputs are identical: {torch.allclose(out3, out4)}\")\n",
    "\n",
    "print(\"\\n2. BatchNorm behavior:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# BatchNorm tracks running statistics differently in train vs eval\n",
    "def check_bn_stats(model, mode_name):\n",
    "    bn_layer = model.bn1\n",
    "    print(f\"{mode_name} mode:\")\n",
    "    print(f\"  Running mean: {bn_layer.running_mean[:5]}\")\n",
    "    print(f\"  Running var:  {bn_layer.running_var[:5]}\")\n",
    "    print(f\"  Training: {bn_layer.training}\")\n",
    "\n",
    "# Reset batch norm statistics\n",
    "def reset_bn_stats(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm1d):\n",
    "            module.reset_running_stats()\n",
    "\n",
    "reset_bn_stats(model)\n",
    "\n",
    "# Training mode: updates running statistics\n",
    "model.train()\n",
    "_ = model(x)\n",
    "check_bn_stats(model, \"Training\")\n",
    "\n",
    "# Evaluation mode: uses fixed running statistics\n",
    "model.eval()\n",
    "old_running_mean = model.bn1.running_mean.clone()\n",
    "_ = model(x)\n",
    "check_bn_stats(model, \"Evaluation\")\n",
    "\n",
    "print(f\"Running mean changed in eval: {not torch.allclose(old_running_mean, model.bn1.running_mean)}\")\n",
    "\n",
    "print(\"\\n3. Practical implications:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"✓ Always call model.train() before training\")\n",
    "print(\"✓ Always call model.eval() before inference\")\n",
    "print(\"✓ Use torch.no_grad() during inference to save memory\")\n",
    "print(\"✓ Dropout provides regularization during training\")\n",
    "print(\"✓ BatchNorm uses batch statistics in training, running stats in eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate correct inference pattern\n",
    "def inference_example():\n",
    "    \"\"\"Show proper inference setup\"\"\"\n",
    "    \n",
    "    # Assume we have a trained model\n",
    "    model = SimpleClassifier(20, 64, 5)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Sample test data\n",
    "    test_data = torch.randn(100, 20)\n",
    "    \n",
    "    print(\"Inference setup:\")\n",
    "    \n",
    "    # Method 1: Basic inference\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        predictions = model(test_data)\n",
    "        probabilities = F.softmax(predictions, dim=1)\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    \n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    print(f\"Sample probabilities: {probabilities[0]}\")\n",
    "    print(f\"Predicted classes: {predicted_classes[:10]}\")\n",
    "    \n",
    "    # Method 2: Batch processing for large datasets\n",
    "    def batch_inference(model, data, batch_size=32):\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                batch = data[i:i+batch_size]\n",
    "                batch_pred = model(batch)\n",
    "                all_predictions.append(batch_pred)\n",
    "        \n",
    "        return torch.cat(all_predictions, dim=0)\n",
    "    \n",
    "    # Process large dataset in batches\n",
    "    large_test_data = torch.randn(1000, 20)\n",
    "    batch_predictions = batch_inference(model, large_test_data, batch_size=64)\n",
    "    \n",
    "    print(f\"\\nBatch inference on {len(large_test_data)} samples:\")\n",
    "    print(f\"Output shape: {batch_predictions.shape}\")\n",
    "    \n",
    "    # Memory usage comparison\n",
    "    import torch.cuda\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nMemory usage patterns:\")\n",
    "        print(\"• torch.no_grad() reduces memory usage by ~2x\")\n",
    "        print(\"• Batch processing prevents OOM on large datasets\")\n",
    "        print(\"• model.eval() ensures consistent results\")\n",
    "\n",
    "inference_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate learning rate scheduling\n",
    "print(\"=== Learning Rate Scheduling Demo ===\")\n",
    "\n",
    "def demonstrate_lr_scheduling():\n",
    "    \"\"\"Show different learning rate scheduling strategies\"\"\"\n",
    "    \n",
    "    # Create a simple model and optimizer\n",
    "    model = SimpleClassifier(20, 64, 5)\n",
    "    base_lr = 0.1\n",
    "    optimizer = optim.SGD(model.parameters(), lr=base_lr)\n",
    "    \n",
    "    num_epochs = 50\n",
    "    \n",
    "    # Different schedulers\n",
    "    schedulers = {\n",
    "        'StepLR': optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5),\n",
    "        'ExponentialLR': optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95),\n",
    "        'CosineAnnealing': optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.001),\n",
    "        'ReduceLROnPlateau': optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
    "    }\n",
    "    \n",
    "    # Track learning rates for each scheduler\n",
    "    lr_histories = {name: [] for name in schedulers.keys()}\n",
    "    lr_histories['No Scheduling'] = []\n",
    "    \n",
    "    # Simulate training with different schedulers\n",
    "    for name, scheduler in schedulers.items():\n",
    "        # Reset optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=base_lr)\n",
    "        if name != 'ReduceLROnPlateau':\n",
    "            scheduler = type(scheduler)(optimizer, **scheduler.state_dict())\n",
    "        else:\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            lr_histories[name].append(current_lr)\n",
    "            \n",
    "            # For ReduceLROnPlateau, we need to provide a metric\n",
    "            if name == 'ReduceLROnPlateau':\n",
    "                # Simulate a loss that decreases then plateaus\n",
    "                fake_loss = 1.0 * np.exp(-epoch/10) + 0.1 + 0.05 * np.random.random()\n",
    "                if epoch > 20:  # Start plateauing\n",
    "                    fake_loss = 0.15 + 0.02 * np.random.random()\n",
    "                scheduler.step(fake_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "    \n",
    "    # Add no scheduling baseline\n",
    "    lr_histories['No Scheduling'] = [base_lr] * num_epochs\n",
    "    \n",
    "    return lr_histories\n",
    "\n",
    "# Generate learning rate histories\n",
    "lr_histories = demonstrate_lr_scheduling()\n",
    "\n",
    "# Plot learning rate schedules\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "epochs = range(len(lr_histories['No Scheduling']))\n",
    "colors = ['blue', 'green', 'red', 'orange', 'purple']\n",
    "\n",
    "for i, (name, lr_history) in enumerate(lr_histories.items()):\n",
    "    plt.plot(epochs, lr_history, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Scheduling Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale to see all schedules clearly\n",
    "plt.show()\n",
    "\n",
    "# Explain each scheduler\n",
    "print(\"\\nScheduler explanations:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"StepLR: Reduces LR by factor γ every step_size epochs\")\n",
    "print(\"ExponentialLR: Multiplies LR by γ each epoch\")\n",
    "print(\"CosineAnnealing: Follows cosine curve from max to min LR\")\n",
    "print(\"ReduceLROnPlateau: Reduces LR when metric stops improving\")\n",
    "print(\"No Scheduling: Constant learning rate\")\n",
    "\n",
    "print(\"\\nWhen to use each:\")\n",
    "print(\"• StepLR: Simple, works well with SGD\")\n",
    "print(\"• ExponentialLR: Smooth decay, good for long training\")\n",
    "print(\"• CosineAnnealing: Popular for transformers, smooth restart\")\n",
    "print(\"• ReduceLROnPlateau: Adaptive, responds to actual performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Learning rate warmup\n",
    "class WarmupScheduler:\n",
    "    \"\"\"Learning rate warmup followed by decay\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_epochs, max_lr, total_epochs):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_lr = max_lr\n",
    "        self.total_epochs = total_epochs\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def step(self):\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            lr = self.max_lr * (self.current_epoch + 1) / self.warmup_epochs\n",
    "        else:\n",
    "            # Cosine decay after warmup\n",
    "            progress = (self.current_epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n",
    "            lr = 0.5 * self.max_lr * (1 + np.cos(np.pi * progress))\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        self.current_epoch += 1\n",
    "        return lr\n",
    "\n",
    "# Demonstrate warmup scheduling\n",
    "model = SimpleClassifier(20, 64, 5)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)  # Will be overridden\n",
    "\n",
    "warmup_scheduler = WarmupScheduler(\n",
    "    optimizer=optimizer,\n",
    "    warmup_epochs=10,\n",
    "    max_lr=0.01,\n",
    "    total_epochs=100\n",
    ")\n",
    "\n",
    "# Track warmup schedule\n",
    "warmup_lrs = []\n",
    "for epoch in range(100):\n",
    "    lr = warmup_scheduler.step()\n",
    "    warmup_lrs.append(lr)\n",
    "\n",
    "# Plot warmup schedule\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(100), warmup_lrs, 'b-', linewidth=2, label='Warmup + Cosine Decay')\n",
    "plt.axvline(x=10, color='r', linestyle='--', alpha=0.7, label='End of Warmup')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Warmup + Cosine Decay')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Warmup benefits:\")\n",
    "print(\"• Prevents early training instability\")\n",
    "print(\"• Especially important for large batch sizes\")\n",
    "print(\"• Common in transformer training\")\n",
    "print(\"• Allows higher maximum learning rates\")\n",
    "\n",
    "print(\"\\n🎉 Optimization & Training exploration completed!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"• Use a systematic training loop structure\")\n",
    "print(\"• AdamW is generally a good default optimizer\")\n",
    "print(\"• Match loss function to your task type\")\n",
    "print(\"• Always set model.train()/model.eval() appropriately\")\n",
    "print(\"• Learning rate scheduling can significantly improve results\")\n",
    "print(\"• Monitor both training and validation metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}