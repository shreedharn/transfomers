{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PyTorch Optimization & Training Loops\n",
    "\n",
    "This notebook covers optimization, loss functions, and training patterns in PyTorch.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Canonical Training Loop](#canonical-training-loop)\n",
    "2. [Optimizers: SGD vs AdamW](#optimizers-sgd-vs-adamw)\n",
    "3. [Common Loss Functions](#common-loss-functions)\n",
    "4. [Train vs Eval Modes](#train-vs-eval-modes)\n",
    "5. [Learning Rate Scheduling](#learning-rate-scheduling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Canonical Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal training loop skeleton\n",
    "def train_model(model, train_loader, val_loader=None, num_epochs=5, lr=0.001, device='cpu'):\n",
    "    \"\"\"Universal training loop that works for MLPs, RNNs, and Transformers\"\"\"\n",
    "    \n",
    "    # Setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ===== TRAINING PHASE =====\n",
    "        model.train()  # Set to training mode\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Zero gradients (important!)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optional: gradient clipping (especially important for RNNs)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # Print progress occasionally\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx:3d}: Loss = {loss.item():.4f}')\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = 100.0 * train_correct / train_total\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        \n",
    "        # ===== VALIDATION PHASE =====\n",
    "        if val_loader is not None:\n",
    "            model.eval()  # Set to evaluation mode\n",
    "            \n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():  # Disable gradient computation\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    \n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    val_total += target.size(0)\n",
    "                    val_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = 100.0 * val_correct / val_total\n",
    "            \n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_acc'].append(val_accuracy)\n",
    "            \n",
    "            print(f'Epoch {epoch+1:2d}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
    "                  f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "        else:\n",
    "            print(f'Epoch {epoch+1:2d}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Create a simple model for demonstration\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.fc3 = nn.Linear(hidden_size//2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Generate synthetic dataset\n",
    "def create_classification_dataset(n_samples=1000, n_features=20, n_classes=5):\n",
    "    \"\"\"Create synthetic classification dataset\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Generate features\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    \n",
    "    # Create separable classes\n",
    "    # Use linear combination with some non-linearity\n",
    "    weights = torch.randn(n_features, n_classes)\n",
    "    logits = X @ weights + 0.1 * torch.randn(n_samples, n_classes)\n",
    "    y = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "X, y = create_classification_dataset(n_samples=1000, n_features=20, n_classes=5)\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Class distribution: {torch.bincount(y)}\")\n",
    "\n",
    "# Split into train/validation\n",
    "train_size = 800\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using our universal training loop\n",
    "model = SimpleClassifier(input_size=20, hidden_size=128, num_classes=5)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=10,\n",
    "    lr=0.001,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "def plot_training_curves(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    if history['val_loss']:\n",
    "        ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n",
    "    if history['val_acc']:\n",
    "        ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(f\"Final Training Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Training Accuracy: {history['train_acc'][-1]:.2f}%\")\n",
    "    if history['val_loss']:\n",
    "        print(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\")\n",
    "        print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.2f}%\")\n",
    "\n",
    "plot_training_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Optimizers: SGD vs AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different optimizers\n",
    "def compare_optimizers():\n",
    "    \"\"\"Compare SGD, Adam, and AdamW optimizers\"\"\"\n",
    "    \n",
    "    # Create identical models\n",
    "    models = {\n",
    "        'SGD': SimpleClassifier(20, 128, 5),\n",
    "        'Adam': SimpleClassifier(20, 128, 5),\n",
    "        'AdamW': SimpleClassifier(20, 128, 5)\n",
    "    }\n",
    "    \n",
    "    # Make sure they start with identical weights\n",
    "    state_dict = models['SGD'].state_dict()\n",
    "    for model in models.values():\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Create optimizers\n",
    "    optimizers = {\n",
    "        'SGD': optim.SGD(models['SGD'].parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4),\n",
    "        'Adam': optim.Adam(models['Adam'].parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4),\n",
    "        'AdamW': optim.AdamW(models['AdamW'].parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "    }\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Track progress for each optimizer\n",
    "    histories = {name: {'loss': [], 'acc': []} for name in models.keys()}\n",
    "    \n",
    "    num_epochs = 15\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for name in models.keys():\n",
    "            model = models[name]\n",
    "            optimizer = optimizers[name]\n",
    "            \n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for data, target in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            accuracy = 100.0 * correct / total\n",
    "            \n",
    "            histories[name]['loss'].append(avg_loss)\n",
    "            histories[name]['acc'].append(accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:2d}:\")\n",
    "            for name in models.keys():\n",
    "                loss = histories[name]['loss'][-1]\n",
    "                acc = histories[name]['acc'][-1]\n",
    "                print(f\"  {name:5s}: Loss = {loss:.4f}, Acc = {acc:.2f}%\")\n",
    "    \n",
    "    return histories\n",
    "\n",
    "# Compare optimizers\n",
    "print(\"Comparing optimizers...\")\n",
    "optimizer_histories = compare_optimizers()\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs = range(1, len(optimizer_histories['SGD']['loss']) + 1)\n",
    "colors = {'SGD': 'blue', 'Adam': 'green', 'AdamW': 'red'}\n",
    "\n",
    "# Loss comparison\n",
    "for name, history in optimizer_histories.items():\n",
    "    ax1.plot(epochs, history['loss'], color=colors[name], label=f'{name}', marker='o', markersize=3)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Optimizer Comparison: Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "for name, history in optimizer_histories.items():\n",
    "    ax2.plot(epochs, history['acc'], color=colors[name], label=f'{name}', marker='o', markersize=3)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Optimizer Comparison: Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final comparison\n",
    "print(\"\\nFinal Results:\")\n",
    "for name, history in optimizer_histories.items():\n",
    "    final_loss = history['loss'][-1]\n",
    "    final_acc = history['acc'][-1]\n",
    "    print(f\"{name:5s}: Loss = {final_loss:.4f}, Accuracy = {final_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nKey differences:\")\n",
    "print(\"• SGD: Simple, requires learning rate tuning, benefits from momentum\")\n",
    "print(\"• Adam: Adaptive learning rates, fast convergence, can overfit\")\n",
    "print(\"• AdamW: Adam with decoupled weight decay, better generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Common Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different loss functions\n",
    "print(\"=== Common Loss Functions Demo ===\")\n",
    "\n",
    "# 1. Classification: CrossEntropyLoss\n",
    "print(\"\\n1. CrossEntropyLoss (Multi-class Classification)\")\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create sample data\n",
    "batch_size, num_classes = 4, 5\n",
    "logits = torch.randn(batch_size, num_classes)  # Raw scores from model\n",
    "targets = torch.randint(0, num_classes, (batch_size,))  # Class indices\n",
    "\n",
    "loss_ce = ce_loss(logits, targets)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "print(f\"Targets: {targets}\")\n",
    "print(f\"CrossEntropyLoss: {loss_ce.item():.4f}\")\n",
    "\n",
    "# Show what happens with perfect predictions\n",
    "perfect_logits = torch.zeros_like(logits)\n",
    "for i, target in enumerate(targets):\n",
    "    perfect_logits[i, target] = 10.0  # High score for correct class\n",
    "perfect_loss = ce_loss(perfect_logits, targets)\n",
    "print(f\"Perfect prediction loss: {perfect_loss.item():.4f}\")\n",
    "\n",
    "# 2. Regression: MSELoss\n",
    "print(\"\\n2. MSELoss (Regression)\")\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "predictions = torch.randn(batch_size, 1)\n",
    "true_values = torch.randn(batch_size, 1)\n",
    "\n",
    "loss_mse = mse_loss(predictions, true_values)\n",
    "print(f\"Predictions: {predictions.squeeze()}\")\n",
    "print(f\"True values: {true_values.squeeze()}\")\n",
    "print(f\"MSELoss: {loss_mse.item():.4f}\")\n",
    "\n",
    "# Show difference between MSE and MAE\n",
    "mae_loss = nn.L1Loss()\n",
    "loss_mae = mae_loss(predictions, true_values)\n",
    "print(f\"MAE (L1) Loss: {loss_mae.item():.4f}\")\n",
    "\n",
    "# 3. Binary Classification: BCEWithLogitsLoss\n",
    "print(\"\\n3. BCEWithLogitsLoss (Binary Classification)\")\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "binary_logits = torch.randn(batch_size, 1)  # Raw logits\n",
    "binary_targets = torch.randint(0, 2, (batch_size, 1)).float()  # 0 or 1\n",
    "\n",
    "loss_bce = bce_loss(binary_logits, binary_targets)\n",
    "print(f\"Binary logits: {binary_logits.squeeze()}\")\n",
    "print(f\"Binary targets: {binary_targets.squeeze()}\")\n",
    "print(f\"BCEWithLogitsLoss: {loss_bce.item():.4f}\")\n",
    "\n",
    "# Compare with manual BCE calculation\n",
    "probs = torch.sigmoid(binary_logits)\n",
    "print(f\"Converted to probabilities: {probs.squeeze()}\")\n",
    "\n",
    "# 4. Negative Log Likelihood: NLLLoss\n",
    "print(\"\\n4. NLLLoss (when you already have log probabilities)\")\n",
    "nll_loss = nn.NLLLoss()\n",
    "\n",
    "# NLLLoss expects log probabilities\n",
    "log_probs = F.log_softmax(logits, dim=1)\n",
    "loss_nll = nll_loss(log_probs, targets)\n",
    "\n",
    "print(f\"Log probabilities shape: {log_probs.shape}\")\n",
    "print(f\"NLLLoss: {loss_nll.item():.4f}\")\n",
    "print(f\"Note: CrossEntropyLoss = LogSoftmax + NLLLoss\")\n",
    "print(f\"Verification - CE loss: {loss_ce.item():.4f}, NLL loss: {loss_nll.item():.4f}\")\n",
    "print(f\"Match: {abs(loss_ce.item() - loss_nll.item()) < 1e-6}\")\n",
    "\n",
    "# 5. Multi-label classification: BCEWithLogitsLoss with multiple outputs\n",
    "print(\"\\n5. Multi-label Classification\")\n",
    "num_labels = 3\n",
    "multi_logits = torch.randn(batch_size, num_labels)\n",
    "multi_targets = torch.randint(0, 2, (batch_size, num_labels)).float()  # Multiple binary labels\n",
    "\n",
    "multi_bce_loss = nn.BCEWithLogitsLoss()\n",
    "loss_multi = multi_bce_loss(multi_logits, multi_targets)\n",
    "\n",
    "print(f\"Multi-label logits shape: {multi_logits.shape}\")\n",
    "print(f\"Multi-label targets shape: {multi_targets.shape}\")\n",
    "print(f\"Multi-label BCE loss: {loss_multi.item():.4f}\")\n",
    "print(f\"Sample targets: {multi_targets[0]}  (can have multiple 1s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Train vs Eval Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate train vs eval mode differences\n",
    "print(\"=== Train vs Eval Mode Demo ===\")\n",
    "\n",
    "# Create model with dropout and batch norm\n",
    "class ModelWithDropoutAndBN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 50% dropout\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size//2)\n",
    "        self.dropout2 = nn.Dropout(0.3)  # 30% dropout\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size//2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ModelWithDropoutAndBN(20, 128, 5)\n",
    "x = torch.randn(10, 20)  # Batch of 10 samples\n",
    "\n",
    "print(\"1. Dropout behavior:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Training mode: dropout is active\n",
    "model.train()\n",
    "print(\"Training mode (dropout active):\")\n",
    "out1 = model(x)\n",
    "out2 = model(x)  # Same input, different output due to dropout\n",
    "\n",
    "print(f\"Output 1 mean: {out1.mean().item():.4f}\")\n",
    "print(f\"Output 2 mean: {out2.mean().item():.4f}\")\n",
    "print(f\"Outputs are different: {not torch.allclose(out1, out2)}\")\n",
    "\n",
    "# Evaluation mode: dropout is disabled\n",
    "model.eval()\n",
    "print(\"\\nEvaluation mode (dropout disabled):\")\n",
    "out3 = model(x)\n",
    "out4 = model(x)  # Same input, same output\n",
    "\n",
    "print(f\"Output 3 mean: {out3.mean().item():.4f}\")\n",
    "print(f\"Output 4 mean: {out4.mean().item():.4f}\")\n",
    "print(f\"Outputs are identical: {torch.allclose(out3, out4)}\")\n",
    "\n",
    "print(\"\\n2. BatchNorm behavior:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# BatchNorm tracks running statistics differently in train vs eval\n",
    "def check_bn_stats(model, mode_name):\n",
    "    bn_layer = model.bn1\n",
    "    print(f\"{mode_name} mode:\")\n",
    "    print(f\"  Running mean: {bn_layer.running_mean[:5]}\")\n",
    "    print(f\"  Running var:  {bn_layer.running_var[:5]}\")\n",
    "    print(f\"  Training: {bn_layer.training}\")\n",
    "\n",
    "# Reset batch norm statistics\n",
    "def reset_bn_stats(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm1d):\n",
    "            module.reset_running_stats()\n",
    "\n",
    "reset_bn_stats(model)\n",
    "\n",
    "# Training mode: updates running statistics\n",
    "model.train()\n",
    "_ = model(x)\n",
    "check_bn_stats(model, \"Training\")\n",
    "\n",
    "# Evaluation mode: uses fixed running statistics\n",
    "model.eval()\n",
    "old_running_mean = model.bn1.running_mean.clone()\n",
    "_ = model(x)\n",
    "check_bn_stats(model, \"Evaluation\")\n",
    "\n",
    "print(f\"Running mean changed in eval: {not torch.allclose(old_running_mean, model.bn1.running_mean)}\")\n",
    "\n",
    "print(\"\\n3. Practical implications:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"✓ Always call model.train() before training\")\n",
    "print(\"✓ Always call model.eval() before inference\")\n",
    "print(\"✓ Use torch.no_grad() during inference to save memory\")\n",
    "print(\"✓ Dropout provides regularization during training\")\n",
    "print(\"✓ BatchNorm uses batch statistics in training, running stats in eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate correct inference pattern\n",
    "def inference_example():\n",
    "    \"\"\"Show proper inference setup\"\"\"\n",
    "    \n",
    "    # Assume we have a trained model\n",
    "    model = SimpleClassifier(20, 64, 5)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Sample test data\n",
    "    test_data = torch.randn(100, 20)\n",
    "    \n",
    "    print(\"Inference setup:\")\n",
    "    \n",
    "    # Method 1: Basic inference\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        predictions = model(test_data)\n",
    "        probabilities = F.softmax(predictions, dim=1)\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    \n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    print(f\"Sample probabilities: {probabilities[0]}\")\n",
    "    print(f\"Predicted classes: {predicted_classes[:10]}\")\n",
    "    \n",
    "    # Method 2: Batch processing for large datasets\n",
    "    def batch_inference(model, data, batch_size=32):\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                batch = data[i:i+batch_size]\n",
    "                batch_pred = model(batch)\n",
    "                all_predictions.append(batch_pred)\n",
    "        \n",
    "        return torch.cat(all_predictions, dim=0)\n",
    "    \n",
    "    # Process large dataset in batches\n",
    "    large_test_data = torch.randn(1000, 20)\n",
    "    batch_predictions = batch_inference(model, large_test_data, batch_size=64)\n",
    "    \n",
    "    print(f\"\\nBatch inference on {len(large_test_data)} samples:\")\n",
    "    print(f\"Output shape: {batch_predictions.shape}\")\n",
    "    \n",
    "    # Memory usage comparison\n",
    "    import torch.cuda\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nMemory usage patterns:\")\n",
    "        print(\"• torch.no_grad() reduces memory usage by ~2x\")\n",
    "        print(\"• Batch processing prevents OOM on large datasets\")\n",
    "        print(\"• model.eval() ensures consistent results\")\n",
    "\n",
    "inference_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate learning rate scheduling\n",
    "print(\"=== Learning Rate Scheduling Demo ===\")\n",
    "\n",
    "def demonstrate_lr_scheduling():\n",
    "    \"\"\"Show different learning rate scheduling strategies\"\"\"\n",
    "    \n",
    "    # Create a simple model and optimizer\n",
    "    model = SimpleClassifier(20, 64, 5)\n",
    "    base_lr = 0.1\n",
    "    optimizer = optim.SGD(model.parameters(), lr=base_lr)\n",
    "    \n",
    "    num_epochs = 50\n",
    "    \n",
    "    # Different schedulers\n",
    "    schedulers = {\n",
    "        'StepLR': optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5),\n",
    "        'ExponentialLR': optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95),\n",
    "        'CosineAnnealing': optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.001),\n",
    "        'ReduceLROnPlateau': optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
    "    }\n",
    "    \n",
    "    # Track learning rates for each scheduler\n",
    "    lr_histories = {name: [] for name in schedulers.keys()}\n",
    "    lr_histories['No Scheduling'] = []\n",
    "    \n",
    "    # Simulate training with different schedulers\n",
    "    for name, scheduler in schedulers.items():\n",
    "        # Reset optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=base_lr)\n",
    "        if name != 'ReduceLROnPlateau':\n",
    "            scheduler = type(scheduler)(optimizer, **scheduler.state_dict())\n",
    "        else:\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            lr_histories[name].append(current_lr)\n",
    "            \n",
    "            # For ReduceLROnPlateau, we need to provide a metric\n",
    "            if name == 'ReduceLROnPlateau':\n",
    "                # Simulate a loss that decreases then plateaus\n",
    "                fake_loss = 1.0 * np.exp(-epoch/10) + 0.1 + 0.05 * np.random.random()\n",
    "                if epoch > 20:  # Start plateauing\n",
    "                    fake_loss = 0.15 + 0.02 * np.random.random()\n",
    "                scheduler.step(fake_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "    \n",
    "    # Add no scheduling baseline\n",
    "    lr_histories['No Scheduling'] = [base_lr] * num_epochs\n",
    "    \n",
    "    return lr_histories\n",
    "\n",
    "# Generate learning rate histories\n",
    "lr_histories = demonstrate_lr_scheduling()\n",
    "\n",
    "# Plot learning rate schedules\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "epochs = range(len(lr_histories['No Scheduling']))\n",
    "colors = ['blue', 'green', 'red', 'orange', 'purple']\n",
    "\n",
    "for i, (name, lr_history) in enumerate(lr_histories.items()):\n",
    "    plt.plot(epochs, lr_history, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Scheduling Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale to see all schedules clearly\n",
    "plt.show()\n",
    "\n",
    "# Explain each scheduler\n",
    "print(\"\\nScheduler explanations:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"StepLR: Reduces LR by factor γ every step_size epochs\")\n",
    "print(\"ExponentialLR: Multiplies LR by γ each epoch\")\n",
    "print(\"CosineAnnealing: Follows cosine curve from max to min LR\")\n",
    "print(\"ReduceLROnPlateau: Reduces LR when metric stops improving\")\n",
    "print(\"No Scheduling: Constant learning rate\")\n",
    "\n",
    "print(\"\\nWhen to use each:\")\n",
    "print(\"• StepLR: Simple, works well with SGD\")\n",
    "print(\"• ExponentialLR: Smooth decay, good for long training\")\n",
    "print(\"• CosineAnnealing: Popular for transformers, smooth restart\")\n",
    "print(\"• ReduceLROnPlateau: Adaptive, responds to actual performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Learning rate warmup\n",
    "class WarmupScheduler:\n",
    "    \"\"\"Learning rate warmup followed by decay\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_epochs, max_lr, total_epochs):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_lr = max_lr\n",
    "        self.total_epochs = total_epochs\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def step(self):\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            lr = self.max_lr * (self.current_epoch + 1) / self.warmup_epochs\n",
    "        else:\n",
    "            # Cosine decay after warmup\n",
    "            progress = (self.current_epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n",
    "            lr = 0.5 * self.max_lr * (1 + np.cos(np.pi * progress))\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        self.current_epoch += 1\n",
    "        return lr\n",
    "\n",
    "# Demonstrate warmup scheduling\n",
    "model = SimpleClassifier(20, 64, 5)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)  # Will be overridden\n",
    "\n",
    "warmup_scheduler = WarmupScheduler(\n",
    "    optimizer=optimizer,\n",
    "    warmup_epochs=10,\n",
    "    max_lr=0.01,\n",
    "    total_epochs=100\n",
    ")\n",
    "\n",
    "# Track warmup schedule\n",
    "warmup_lrs = []\n",
    "for epoch in range(100):\n",
    "    lr = warmup_scheduler.step()\n",
    "    warmup_lrs.append(lr)\n",
    "\n",
    "# Plot warmup schedule\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(100), warmup_lrs, 'b-', linewidth=2, label='Warmup + Cosine Decay')\n",
    "plt.axvline(x=10, color='r', linestyle='--', alpha=0.7, label='End of Warmup')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Warmup + Cosine Decay')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Warmup benefits:\")\n",
    "print(\"• Prevents early training instability\")\n",
    "print(\"• Especially important for large batch sizes\")\n",
    "print(\"• Common in transformer training\")\n",
    "print(\"• Allows higher maximum learning rates\")\n",
    "\n",
    "print(\"\\n🎉 Optimization & Training exploration completed!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"• Use a systematic training loop structure\")\n",
    "print(\"• AdamW is generally a good default optimizer\")\n",
    "print(\"• Match loss function to your task type\")\n",
    "print(\"• Always set model.train()/model.eval() appropriately\")\n",
    "print(\"• Learning rate scheduling can significantly improve results\")\n",
    "print(\"• Monitor both training and validation metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}