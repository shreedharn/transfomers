{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# PyTorch Debugging & Common Gotchas: Avoiding Training Disasters\n\n## üéØ Introduction\n\nWelcome to your PyTorch debugging survival guide! This notebook will arm you with the knowledge to identify, diagnose, and fix the most common PyTorch mistakes that can derail your training. Every deep learning practitioner faces these issues - the difference is knowing how to solve them quickly.\n\n### üß† What You'll Learn to Debug\n\nThis essential guide covers:\n- **Shape mismatches**: The #1 source of PyTorch errors and how to fix them\n- **Training vs eval modes**: Why your model behaves differently during inference\n- **Memory issues**: GPU out-of-memory errors and optimization strategies  \n- **Gradient problems**: Vanishing, exploding, and missing gradients\n- **Common training failures**: Why your loss isn't decreasing and how to fix it\n\n### üéì Prerequisites\n\n- Experience with PyTorch tensors, modules, and basic training loops\n- Familiarity with common neural network architectures\n- Basic understanding of backpropagation and gradient descent\n\n### üöÄ Why Debugging Skills Matter\n\nEffective debugging enables:\n- **Faster development**: Spend time building, not hunting bugs\n- **Reliable training**: Catch issues before they waste compute time\n- **Better models**: Identify and fix performance bottlenecks\n- **Confidence**: Know your models are working as intended\n- **Scalability**: Debug issues that only appear at scale\n\n---\n\n## üìö Table of Contents\n\n1. **[Shape Debugging Mastery](#shape-debugging-mastery)** - Conquering tensor dimension mismatches\n2. **[Training vs Eval Mode Gotchas](#training-vs-eval-mode-gotchas)** - Understanding model state behavior\n3. **[Memory Management & GPU Issues](#memory-management-gpu-issues)** - Handling out-of-memory errors\n4. **[Gradient Flow Problems](#gradient-flow-problems)** - Diagnosing backpropagation issues\n5. **[Training Failure Patterns](#training-failure-patterns)** - When your loss won't decrease"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Shape Debugging Mastery\n\n### üîç The #1 Source of PyTorch Errors\n\nShape mismatches cause more PyTorch frustration than any other issue. Let's learn to diagnose and fix them systematically, plus develop techniques to prevent them in the first place!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# SHAPE DEBUGGING SURVIVAL GUIDE\n# =============================================================================\n\nprint(\"üîç Shape Debugging Mastery\")\nprint(\"=\" * 50)\n\n# Common shape error scenarios and how to debug them\nprint(\"Scenario 1: Matrix Multiplication Dimension Mismatch\")\nprint(\"-\" * 30)\n\ntry:\n    # This will fail - inner dimensions don't match\n    A = torch.randn(3, 4)  # [3, 4]\n    B = torch.randn(2, 5)  # [2, 5] - can't multiply with A\n    \n    print(f\"Tensor A shape: {A.shape}\")\n    print(f\"Tensor B shape: {B.shape}\")\n    print(\"Attempting A @ B...\")\n    \n    result = A @ B  # This will raise an error\n    \nexcept RuntimeError as e:\n    print(f\"‚ùå Error: {e}\")\n    print(f\"\\nüîß Debugging approach:\")\n    print(f\"1. Check dimensions: A is {A.shape}, B is {B.shape}\")\n    print(f\"2. For A @ B, need A[..., n] and B[n, ...]\")\n    print(f\"3. A has last dim {A.shape[-1]}, B has first dim {B.shape[0]}\")\n    print(f\"4. {A.shape[-1]} ‚â† {B.shape[0]}, so multiplication fails\")\n    \n    print(f\"\\n‚úÖ Fix: Transpose B or create compatible tensors\")\n    B_fixed = torch.randn(4, 5)  # [4, 5] - now compatible with A\n    result = A @ B_fixed  # [3, 4] @ [4, 5] = [3, 5]\n    print(f\"Fixed: A {A.shape} @ B_fixed {B_fixed.shape} = result {result.shape}\")\n\nprint(f\"\\nScenario 2: Broadcasting Confusion\")\nprint(\"-\" * 30)\n\n# Broadcasting can be tricky to debug\nA = torch.randn(3, 1, 4)    # [3, 1, 4]\nB = torch.randn(2, 4)       # [2, 4]\n\nprint(f\"Tensor A shape: {A.shape}\")\nprint(f\"Tensor B shape: {B.shape}\")\n\ntry:\n    result = A + B\n    print(f\"‚úÖ Success: A + B = {result.shape}\")\n    print(f\"Broadcasting rule: dimensions aligned from right:\")\n    print(f\"  A: [3, 1, 4]\")\n    print(f\"  B:    [2, 4]\")\n    print(f\"  ‚Üí:  [3, 2, 4] (B expands in dim 0, A expands in dim 1)\")\n    \nexcept RuntimeError as e:\n    print(f\"‚ùå Error: {e}\")\n\n# Show a case that fails\nprint(f\"\\nBroadcasting failure example:\")\nA = torch.randn(3, 4)       # [3, 4] \nB = torch.randn(3, 5)       # [3, 5]\n\nprint(f\"Tensor A shape: {A.shape}\")\nprint(f\"Tensor B shape: {B.shape}\")\n\ntry:\n    result = A + B\nexcept RuntimeError as e:\n    print(f\"‚ùå Error: {e}\")\n    print(f\"üîß Problem: Last dimensions must match or be 1 for broadcasting\")\n    print(f\"  A last dim: {A.shape[-1]}\")\n    print(f\"  B last dim: {B.shape[-1]}\")\n    print(f\"  Neither is 1, and {A.shape[-1]} ‚â† {B.shape[-1]}\")\n\nprint(f\"\\nScenario 3: Neural Network Layer Mismatches\")\nprint(\"-\" * 30)\n\n# Common neural network shape errors\nclass DebugModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(10, 5)   # Expects input: [..., 10]\n        self.fc2 = nn.Linear(5, 1)    # Expects input: [..., 5]\n    \n    def forward(self, x):\n        print(f\"  Input shape: {x.shape}\")\n        x = self.fc1(x)\n        print(f\"  After fc1: {x.shape}\")\n        x = self.fc2(x)\n        print(f\"  After fc2: {x.shape}\")\n        return x\n\nmodel = DebugModel()\n\n# Correct usage\nprint(\"‚úÖ Correct usage:\")\ncorrect_input = torch.randn(3, 10)  # Batch size 3, 10 features\noutput = model(correct_input)\n\n# Common mistake: wrong feature dimension\nprint(f\"\\n‚ùå Wrong feature dimension:\")\ntry:\n    wrong_input = torch.randn(3, 8)  # Wrong: 8 features instead of 10\n    print(f\"Input shape: {wrong_input.shape}\")\n    output = model(wrong_input)\nexcept RuntimeError as e:\n    print(f\"Error: {e}\")\n    print(f\"üîß Fix: Ensure input features match layer expectation\")\n    print(f\"  Model expects: [..., 10]\")\n    print(f\"  You provided: [..., 8]\")\n\nprint(f\"\\nüõ†Ô∏è Essential Shape Debugging Tools\")\nprint(\"=\" * 50)\n\ndef debug_shapes(*tensors, names=None):\n    \"\"\"Utility function to debug tensor shapes.\"\"\"\n    if names is None:\n        names = [f\"tensor_{i}\" for i in range(len(tensors))]\n    \n    print(\"Shape Debug Report:\")\n    print(\"-\" * 20)\n    for name, tensor in zip(names, tensors):\n        print(f\"{name:12}: {tensor.shape} | dtype: {tensor.dtype} | device: {tensor.device}\")\n    \n    # Check for common operations\n    if len(tensors) == 2:\n        A, B = tensors\n        print(f\"\\nCompatibility Check:\")\n        print(f\"A @ B possible: {A.shape[-1] == B.shape[0] if A.ndim >= 1 and B.ndim >= 1 else False}\")\n        \n        # Broadcasting check\n        try:\n            broadcast_shape = torch.broadcast_shapes(A.shape, B.shape)\n            print(f\"A + B broadcasts to: {broadcast_shape}\")\n        except RuntimeError:\n            print(f\"A + B: Broadcasting not possible\")\n\n# Demonstrate the debugging utility\nprint(\"Using debug utility:\")\nA = torch.randn(2, 3, 4)\nB = torch.randn(4, 5)\ndebug_shapes(A, B, names=['A', 'B'])\n\nprint(f\"\\nüí° Pro Shape Debugging Tips\")\nprint(\"=\" * 50)\nprint(\"1. **Always print shapes**: Add print(f'Shape: {tensor.shape}') everywhere\")\nprint(\"2. **Use .shape, not .size()**: .shape is clearer and more pythonic\")\nprint(\"3. **Check device and dtype**: Mismatches cause subtle errors\")\nprint(\"4. **Use named dimensions**: Comment what each dimension represents\")\nprint(\"5. **Test with small tensors**: Debug with tiny shapes first\")\nprint(\"6. **Use torch.einsum()**: More explicit than @ for complex operations\")\n\nprint(f\"\\nüéØ Shape Pattern Recognition\")\nprint(\"=\" * 50)\nprint(\"Common PyTorch shape patterns:\")\nprint(\"‚Ä¢ Batch dimension first: [batch_size, ...]\")\nprint(\"‚Ä¢ Images: [batch, channels, height, width]\")\nprint(\"‚Ä¢ Sequences: [batch, seq_len, features]\")\nprint(\"‚Ä¢ Linear layers: [..., input_features] ‚Üí [..., output_features]\")\nprint(\"‚Ä¢ Conv2d: [batch, in_channels, H, W] ‚Üí [batch, out_channels, H', W']\")\nprint(\"‚Ä¢ Attention: [batch, seq_len, d_model] for all Q, K, V\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate batch norm issues\n",
    "print(\"\\n=== Batch Norm Mode Issues ===\")\n",
    "\n",
    "# Create model with batch norm\n",
    "model_bn = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.BatchNorm1d(20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ")\n",
    "\n",
    "# ‚ùå GOTCHA: Using batch norm with very small batches\n",
    "print(\"\\nBatch norm with different batch sizes:\")\n",
    "\n",
    "model_bn.train()\n",
    "\n",
    "# Large batch - works fine\n",
    "large_batch = torch.randn(32, 10)\n",
    "out_large = model_bn(large_batch)\n",
    "print(f\"Large batch (32): mean={out_large.mean().item():.4f}, std={out_large.std().item():.4f}\")\n",
    "\n",
    "# Small batch - can be unstable\n",
    "small_batch = torch.randn(2, 10)  # Very small batch\n",
    "out_small = model_bn(small_batch)\n",
    "print(f\"Small batch (2):  mean={out_small.mean().item():.4f}, std={out_small.std().item():.4f}\")\n",
    "\n",
    "# Single sample - will error!\n",
    "try:\n",
    "    single_sample = torch.randn(1, 10)\n",
    "    out_single = model_bn(single_sample)\n",
    "except Exception as e:\n",
    "    print(f\"Single sample error: {e}\")\n",
    "\n",
    "print(\"\\nüí° Solution: Use LayerNorm for small batches or add .unsqueeze(0) for single samples\")\n",
    "\n",
    "# Alternative: Use LayerNorm instead\n",
    "model_ln = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.LayerNorm(20),  # LayerNorm works with any batch size\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ")\n",
    "\n",
    "single_sample = torch.randn(1, 10)\n",
    "out_ln = model_ln(single_sample)\n",
    "print(f\"LayerNorm with single sample: {out_ln.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Autograd Gotchas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Autograd Gotchas ===\")\n",
    "\n",
    "# ‚ùå GOTCHA 1: Forgetting to zero gradients\n",
    "print(\"\\n1. Forgetting to zero gradients\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "model = nn.Linear(5, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "x = torch.randn(10, 5)\n",
    "y_true = torch.randn(10, 1)\n",
    "\n",
    "print(\"Training without zero_grad():\")\n",
    "for epoch in range(3):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    loss.backward()  # Gradients accumulate!\n",
    "    \n",
    "    # Check gradient magnitude\n",
    "    grad_norm = model.weight.grad.norm().item()\n",
    "    print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, Grad norm = {grad_norm:.4f}\")\n",
    "    \n",
    "    optimizer.step()  # ‚ùå No zero_grad()!\n",
    "\n",
    "# ‚úÖ CORRECT: Always zero gradients\n",
    "print(\"\\nTraining WITH zero_grad():\")\n",
    "model = nn.Linear(5, 1)  # Reset model\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(3):\n",
    "    optimizer.zero_grad()  # ‚úÖ Clear gradients first\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    loss.backward()\n",
    "    \n",
    "    grad_norm = model.weight.grad.norm().item()\n",
    "    print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, Grad norm = {grad_norm:.4f}\")\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "print(\"\\nüìù Notice how gradient norms are much smaller with proper zero_grad()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå GOTCHA 2: In-place operations breaking autograd\n",
    "print(\"\\n2. In-place operations breaking autograd\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# This will cause problems\n",
    "try:\n",
    "    x = torch.randn(5, requires_grad=True)\n",
    "    y = x * 2\n",
    "    x[0] = 0  # ‚ùå In-place modification!\n",
    "    loss = y.sum()\n",
    "    loss.backward()  # This might error or give wrong gradients\n",
    "except RuntimeError as e:\n",
    "    print(f\"In-place operation error: {e}\")\n",
    "\n",
    "# ‚úÖ CORRECT: Use non-in-place operations\n",
    "x = torch.randn(5, requires_grad=True)\n",
    "y = x * 2\n",
    "\n",
    "# Option 1: Clone and modify\n",
    "x_modified = x.clone()\n",
    "x_modified[0] = 0\n",
    "y_modified = x_modified * 2\n",
    "\n",
    "# Option 2: Create mask for modification\n",
    "mask = torch.ones_like(x)\n",
    "mask[0] = 0\n",
    "y_masked = y * mask\n",
    "\n",
    "loss = y_masked.sum()\n",
    "loss.backward()\n",
    "print(f\"Gradients computed successfully: {x.grad is not None}\")\n",
    "print(f\"Gradient: {x.grad}\")\n",
    "\n",
    "print(\"\\nüí° Avoid in-place operations on tensors with requires_grad=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå GOTCHA 3: Gradient accumulation confusion\n",
    "print(\"\\n3. Gradient accumulation behavior\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Sometimes gradient accumulation is intentional (for large effective batch sizes)\n",
    "model = nn.Linear(3, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Simulate gradient accumulation over 4 mini-batches\n",
    "mini_batches = [\n",
    "    (torch.randn(2, 3), torch.randn(2, 1)),\n",
    "    (torch.randn(2, 3), torch.randn(2, 1)),\n",
    "    (torch.randn(2, 3), torch.randn(2, 1)),\n",
    "    (torch.randn(2, 3), torch.randn(2, 1))\n",
    "]\n",
    "\n",
    "print(\"Intentional gradient accumulation (effective batch size = 8):\")\n",
    "accumulation_steps = 4\n",
    "total_loss = 0\n",
    "\n",
    "for i, (x_batch, y_batch) in enumerate(mini_batches):\n",
    "    y_pred = model(x_batch)\n",
    "    loss = F.mse_loss(y_pred, y_batch)\n",
    "    \n",
    "    # Scale loss by accumulation steps (important!)\n",
    "    loss = loss / accumulation_steps\n",
    "    loss.backward()\n",
    "    \n",
    "    total_loss += loss.item() * accumulation_steps\n",
    "    \n",
    "    print(f\"  Mini-batch {i}: Loss = {loss.item() * accumulation_steps:.4f}\")\n",
    "\n",
    "# Update after accumulating gradients from all mini-batches\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(f\"Total accumulated loss: {total_loss:.4f}\")\n",
    "print(\"\\nüí° Scale gradients by 1/accumulation_steps to maintain equivalent learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Data Type and Shape Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Data Type and Shape Issues ===\")\n",
    "\n",
    "# ‚ùå GOTCHA 1: Wrong target dtype for CrossEntropyLoss\n",
    "print(\"\\n1. Wrong target dtype for CrossEntropyLoss\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "logits = torch.randn(4, 5)  # 4 samples, 5 classes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ‚ùå Wrong: Float targets\n",
    "try:\n",
    "    targets_wrong = torch.tensor([0., 1., 2., 3.])  # Float\n",
    "    loss = criterion(logits, targets_wrong)\n",
    "except Exception as e:\n",
    "    print(f\"Float targets error: {type(e).__name__}: {e}\")\n",
    "\n",
    "# ‚úÖ Correct: Long tensor targets\n",
    "targets_correct = torch.tensor([0, 1, 2, 3])  # Long\n",
    "loss = criterion(logits, targets_correct)\n",
    "print(f\"Correct loss with Long targets: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nLogits dtype: {logits.dtype}\")\n",
    "print(f\"Correct targets dtype: {targets_correct.dtype}\")\n",
    "\n",
    "# ‚ùå GOTCHA 2: Shape mismatches\n",
    "print(\"\\n2. Common shape mismatches\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Problem: Unexpected dimension removal\n",
    "x = torch.randn(1, 10)  # Shape: [1, 10]\n",
    "print(f\"Original shape: {x.shape}\")\n",
    "\n",
    "# ‚ùå Dangerous: squeeze() without arguments\n",
    "x_squeezed = x.squeeze()  # Removes ALL dimensions of size 1\n",
    "print(f\"After squeeze(): {x_squeezed.shape}\")\n",
    "\n",
    "# Now if we try to use this with a model expecting 2D input:\n",
    "model = nn.Linear(10, 5)\n",
    "try:\n",
    "    output = model(x_squeezed)  # Will error - expects 2D\n",
    "except Exception as e:\n",
    "    print(f\"Shape error: {e}\")\n",
    "\n",
    "# ‚úÖ Better: Be specific about which dimensions to squeeze\n",
    "x_safe = x.squeeze(0) if x.size(0) == 1 else x  # Only squeeze batch dim if size 1\n",
    "print(f\"Safe squeeze result: {x_safe.shape}\")\n",
    "\n",
    "# Or keep original shape\n",
    "output = model(x)  # Works fine with [1, 10]\n",
    "print(f\"Model output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå GOTCHA 3: Broadcasting surprises\n",
    "print(\"\\n3. Unexpected broadcasting\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# This might not do what you expect\n",
    "a = torch.randn(3, 1)  # Shape: [3, 1]\n",
    "b = torch.randn(4)     # Shape: [4]\n",
    "\n",
    "c = a + b  # Broadcasting: [3, 1] + [4] -> [3, 4]\n",
    "print(f\"a.shape: {a.shape}, b.shape: {b.shape}\")\n",
    "print(f\"Result shape: {c.shape}\")\n",
    "print(\"This creates a 3x4 tensor - might not be intended!\")\n",
    "\n",
    "# ‚úÖ Be explicit about your intentions\n",
    "print(\"\\nBetter: Be explicit about dimensions\")\n",
    "a = torch.randn(3, 1)\n",
    "b = torch.randn(1, 4)  # Make the broadcast intention clear\n",
    "c = a + b  # Now clearly [3, 1] + [1, 4] -> [3, 4]\n",
    "print(f\"a.shape: {a.shape}, b.shape: {b.shape} -> result: {c.shape}\")\n",
    "\n",
    "# ‚ùå GOTCHA 4: Dimension confusion with batch_first\n",
    "print(\"\\n4. batch_first confusion\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Old-style RNN (batch_first=False) - confusing!\n",
    "rnn_old = nn.LSTM(input_size=10, hidden_size=20, batch_first=False)\n",
    "x_old_style = torch.randn(15, 32, 10)  # [seq_len, batch, features]\n",
    "output_old, _ = rnn_old(x_old_style)\n",
    "print(f\"Old style - input: {x_old_style.shape}, output: {output_old.shape}\")\n",
    "print(\"Confusing: sequence length comes first!\")\n",
    "\n",
    "# ‚úÖ Modern style (batch_first=True) - intuitive\n",
    "rnn_new = nn.LSTM(input_size=10, hidden_size=20, batch_first=True)\n",
    "x_new_style = torch.randn(32, 15, 10)  # [batch, seq_len, features]\n",
    "output_new, _ = rnn_new(x_new_style)\n",
    "print(f\"New style - input: {x_new_style.shape}, output: {output_new.shape}\")\n",
    "print(\"Intuitive: batch comes first, like other layers\")\n",
    "\n",
    "print(\"\\nüí° Always use batch_first=True for consistency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Memory and Device Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Memory and Device Problems ===\")\n",
    "\n",
    "# ‚ùå GOTCHA 1: Model and data on different devices\n",
    "print(\"\\n1. Device mismatches\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "model = nn.Linear(10, 5)\n",
    "x = torch.randn(4, 10)\n",
    "\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Data device: {x.device}\")\n",
    "\n",
    "# If you have CUDA available, demonstrate the error\n",
    "if torch.cuda.is_available():\n",
    "    model_gpu = model.cuda()\n",
    "    try:\n",
    "        output = model_gpu(x)  # x is still on CPU!\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Device mismatch error: {e}\")\n",
    "    \n",
    "    # ‚úÖ Solution: Move both to same device\n",
    "    x_gpu = x.cuda()\n",
    "    output = model_gpu(x_gpu)\n",
    "    print(f\"Success! Output shape: {output.shape}, device: {output.device}\")\nelse:\n",
    "    print(\"No CUDA available - device mismatch demo skipped\")\n",
    "\n",
    "# Better pattern: Use device variable\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "x = x.to(device)\n",
    "output = model(x)\n",
    "print(f\"Using device variable - output device: {output.device}\")\n",
    "\n",
    "# ‚ùå GOTCHA 2: Memory leaks in training loops\n",
    "print(\"\\n2. Memory leak patterns\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# This can cause memory leaks\n",
    "def bad_training_loop():\n",
    "    model = nn.Linear(100, 10)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    losses = []  # ‚ùå Storing loss tensors\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        x = torch.randn(32, 100)\n",
    "        y = torch.randint(0, 10, (32,))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss)  # ‚ùå Keeps computation graph in memory!\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# ‚úÖ Correct: Extract scalar values\n",
    "def good_training_loop():\n",
    "    model = nn.Linear(100, 10)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    losses = []  # ‚úÖ Store scalar values only\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        x = torch.randn(32, 100)\n",
    "        y = torch.randint(0, 10, (32,))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())  # ‚úÖ Extract scalar value\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Compare memory usage (simplified demonstration)\n",
    "bad_losses = bad_training_loop()\n",
    "good_losses = good_training_loop()\n",
    "\n",
    "print(f\"Bad pattern - storing tensors: {type(bad_losses[0])}\")\n",
    "print(f\"Good pattern - storing scalars: {type(good_losses[0])}\")\n",
    "print(\"\\nüí° Always use .item() to extract scalar values for logging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå GOTCHA 3: Inefficient tensor operations\n",
    "print(\"\\n3. Inefficient tensor operations\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# ‚ùå Inefficient: Python loops for tensor operations\n",
    "def slow_sum(tensor):\n",
    "    result = 0\n",
    "    for i in range(tensor.size(0)):\n",
    "        for j in range(tensor.size(1)):\n",
    "            result += tensor[i, j].item()  # ‚ùå Very slow!\n",
    "    return result\n",
    "\n",
    "# ‚úÖ Efficient: Use vectorized operations\n",
    "def fast_sum(tensor):\n",
    "    return tensor.sum().item()  # ‚úÖ Fast!\n",
    "\n",
    "# Compare performance\n",
    "import time\n",
    "\n",
    "test_tensor = torch.randn(100, 100)\n",
    "\n",
    "start = time.time()\n",
    "slow_result = slow_sum(test_tensor)\n",
    "slow_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "fast_result = fast_sum(test_tensor)\n",
    "fast_time = time.time() - start\n",
    "\n",
    "print(f\"Slow method: {slow_time:.4f}s, result: {slow_result:.4f}\")\n",
    "print(f\"Fast method: {fast_time:.6f}s, result: {fast_result:.4f}\")\n",
    "print(f\"Speedup: {slow_time/fast_time:.1f}x faster\")\n",
    "\n",
    "# ‚ùå GOTCHA 4: Creating tensors in loops\n",
    "print(\"\\n4. Tensor creation in loops\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ‚ùå Slow: Creating tensors in loop\n",
    "def slow_batch_creation():\n",
    "    batch = []\n",
    "    for i in range(32):\n",
    "        sample = torch.randn(10)  # ‚ùå Creating tensor in loop\n",
    "        batch.append(sample)\n",
    "    return torch.stack(batch)\n",
    "\n",
    "# ‚úÖ Fast: Create all at once\n",
    "def fast_batch_creation():\n",
    "    return torch.randn(32, 10)  # ‚úÖ Create entire batch at once\n",
    "\n",
    "# Time comparison\n",
    "start = time.time()\n",
    "slow_batch = slow_batch_creation()\n",
    "slow_batch_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "fast_batch = fast_batch_creation()\n",
    "fast_batch_time = time.time() - start\n",
    "\n",
    "print(f\"Slow batch creation: {slow_batch_time:.4f}s\")\n",
    "print(f\"Fast batch creation: {fast_batch_time:.6f}s\")\n",
    "print(f\"Same result: {torch.allclose(slow_batch.sum(), fast_batch.sum(), atol=1e-3)}\")\n",
    "\n",
    "print(\"\\nüí° Avoid Python loops for tensor operations - use vectorization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Debugging Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Debugging Techniques ===\")\n",
    "\n",
    "# 1. Shape debugging\n",
    "print(\"\\n1. Shape Debugging\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "def debug_shapes_example():\n",
    "    \"\"\"Example of how to debug shape issues\"\"\"\n",
    "    \n",
    "    batch_size, seq_len, d_model = 4, 10, 64\n",
    "    \n",
    "    # Input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    \n",
    "    # Linear layer\n",
    "    linear = nn.Linear(d_model, d_model * 2)\n",
    "    x = linear(x)\n",
    "    print(f\"After linear: {x.shape}\")\n",
    "    \n",
    "    # Reshape for multi-head attention\n",
    "    num_heads = 8\n",
    "    head_dim = (d_model * 2) // num_heads\n",
    "    x = x.view(batch_size, seq_len, num_heads, head_dim)\n",
    "    print(f\"After reshape: {x.shape}\")\n",
    "    \n",
    "    # Transpose for attention\n",
    "    x = x.transpose(1, 2)  # [batch, heads, seq, head_dim]\n",
    "    print(f\"After transpose: {x.shape}\")\n",
    "    \n",
    "    return x\n",
    "\n",
    "result = debug_shapes_example()\n",
    "print(\"\\nüí° Always print shapes when debugging complex transformations\")\n",
    "\n",
    "# 2. Gradient checking\n",
    "print(\"\\n2. Gradient Checking\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "def check_gradients(model):\n",
    "    \"\"\"Utility to check gradient flow\"\"\"\n",
    "    print(\"\\nGradient check:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            print(f\"  {name}: grad_norm = {grad_norm:.6f}\")\n",
    "            \n",
    "            # Check for problematic gradients\n",
    "            if grad_norm == 0:\n",
    "                print(f\"    ‚ö†Ô∏è  Zero gradients in {name}\")\n",
    "            elif grad_norm > 10:\n",
    "                print(f\"    ‚ö†Ô∏è  Large gradients in {name} (possible explosion)\")\n",
    "            elif torch.isnan(param.grad).any():\n",
    "                print(f\"    ‚ùå NaN gradients in {name}\")\n",
    "        else:\n",
    "            print(f\"  {name}: No gradients\")\n",
    "\n",
    "# Test gradient checking\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ")\n",
    "\n",
    "x = torch.randn(5, 10)\n",
    "y = torch.randn(5, 1)\n",
    "\n",
    "output = model(x)\n",
    "loss = F.mse_loss(output, y)\n",
    "loss.backward()\n",
    "\n",
    "check_gradients(model)\n",
    "\n",
    "# 3. NaN/Inf detection\n",
    "print(\"\\n3. NaN/Inf Detection\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "def check_tensor_health(tensor, name=\"tensor\"):\n",
    "    \"\"\"Check if tensor contains NaN or Inf values\"\"\"\n",
    "    has_nan = torch.isnan(tensor).any().item()\n",
    "    has_inf = torch.isinf(tensor).any().item()\n",
    "    \n",
    "    if has_nan:\n",
    "        print(f\"‚ùå {name} contains NaN values!\")\n",
    "        return False\n",
    "    if has_inf:\n",
    "        print(f\"‚ùå {name} contains Inf values!\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"‚úÖ {name} is healthy\")\n",
    "    return True\n",
    "\n",
    "# Test with problematic tensor\n",
    "good_tensor = torch.randn(5, 5)\n",
    "bad_tensor = torch.tensor([[1.0, 2.0, float('nan')], [4.0, float('inf'), 6.0]])\n",
    "\n",
    "check_tensor_health(good_tensor, \"good_tensor\")\n",
    "check_tensor_health(bad_tensor, \"bad_tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model surgery - examining intermediate outputs\n",
    "print(\"\\n4. Model Surgery with Hooks\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "class DebuggableModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "        \n",
    "        # Storage for intermediate activations\n",
    "        self.activations = {}\n",
    "        \n",
    "        # Register hooks for debugging\n",
    "        self.fc1.register_forward_hook(self.save_activation('fc1'))\n",
    "        self.fc2.register_forward_hook(self.save_activation('fc2'))\n",
    "        self.fc3.register_forward_hook(self.save_activation('fc3'))\n",
    "    \n",
    "    def save_activation(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.activations[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def print_activation_stats(self):\n",
    "        print(\"Activation statistics:\")\n",
    "        for name, activation in self.activations.items():\n",
    "            mean = activation.mean().item()\n",
    "            std = activation.std().item()\n",
    "            sparsity = (activation == 0).float().mean().item()\n",
    "            print(f\"  {name}: mean={mean:.4f}, std={std:.4f}, sparsity={sparsity:.2%}\")\n",
    "\n",
    "# Test the debuggable model\n",
    "debug_model = DebuggableModel()\n",
    "x = torch.randn(8, 10)\n",
    "output = debug_model(x)\n",
    "\n",
    "debug_model.print_activation_stats()\n",
    "\n",
    "# 5. Quick debugging utilities\n",
    "print(\"\\n5. Quick Debugging Utilities\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "def quick_stats(tensor, name=\"tensor\"):\n",
    "    \"\"\"Print quick statistics about a tensor\"\"\"\n",
    "    print(f\"{name}: shape={tensor.shape}, dtype={tensor.dtype}, device={tensor.device}\")\n",
    "    print(f\"  min={tensor.min().item():.4f}, max={tensor.max().item():.4f}\")\n",
    "    print(f\"  mean={tensor.mean().item():.4f}, std={tensor.std().item():.4f}\")\n",
    "    \n",
    "    # Check for common issues\n",
    "    if tensor.requires_grad:\n",
    "        print(f\"  requires_grad=True\")\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"  ‚ö†Ô∏è Contains NaN values\")\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(f\"  ‚ö†Ô∏è Contains Inf values\")\n",
    "\n",
    "# Test the utility\n",
    "test_tensor = torch.randn(3, 4, requires_grad=True)\n",
    "quick_stats(test_tensor, \"test_tensor\")\n",
    "\n",
    "# 6. Model parameter overview\n",
    "def model_overview(model):\n",
    "    \"\"\"Print comprehensive model overview\"\"\"\n",
    "    print(\"\\nModel Overview:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Model size: {total_params * 4 / 1024**2:.2f} MB (assuming float32)\")\n",
    "    \n",
    "    print(\"\\nLayer details:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Leaf modules only\n",
    "            params = sum(p.numel() for p in module.parameters())\n",
    "            print(f\"  {name}: {module.__class__.__name__}, {params:,} params\")\n",
    "\n",
    "model_overview(debug_model)\n",
    "\n",
    "print(\"\\nüéâ Debugging toolkit complete!\")\n",
    "print(\"\\nKey debugging strategies:\")\n",
    "print(\"‚Ä¢ Always print shapes when developing\")\n",
    "print(\"‚Ä¢ Use hooks to inspect intermediate outputs\")\n",
    "print(\"‚Ä¢ Check for NaN/Inf values regularly\")\n",
    "print(\"‚Ä¢ Monitor gradient norms during training\")\n",
    "print(\"‚Ä¢ Create debugging utilities for common checks\")\n",
    "print(\"‚Ä¢ Use tensor.item() to avoid memory leaks\")\n",
    "print(\"‚Ä¢ Be explicit about device placement\")\n",
    "print(\"‚Ä¢ Understand broadcasting behavior\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}