{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PyTorch Modules, Parameters, Initialization\n",
    "\n",
    "This notebook covers PyTorch's building blocks - modules, parameters, and proper initialization strategies.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Basic Module Structure](#basic-module-structure)\n",
    "2. [Parameter Counting Formulas](#parameter-counting-formulas)\n",
    "3. [Initialization Strategies](#initialization-strategies)\n",
    "4. [Advanced Module Patterns](#advanced-module-patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Basic Module Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()  # Always call parent constructor\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout during training\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create and inspect\n",
    "model = SimpleNet(10, 20, 5)\n",
    "print(\"Model structure:\")\n",
    "print(model)\n",
    "\n",
    "print(\"\\nModel parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape} ({param.numel()} parameters)\")\n",
    "\n",
    "# State dict for saving/loading\n",
    "state_dict = model.state_dict()\n",
    "print(f\"\\nState dict keys: {list(state_dict.keys())}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(3, 10)  # Batch of 3 samples\n",
    "output = model(x)\n",
    "print(f\"\\nInput shape: {x.shape}, Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Parameter Counting Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(layer):\n",
    "    \"\"\"Count parameters in common layer types\"\"\"\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        # Linear(in_features, out_features): in*out + out (bias)\n",
    "        weight_params = layer.in_features * layer.out_features\n",
    "        bias_params = layer.out_features if layer.bias is not None else 0\n",
    "        return weight_params + bias_params\n",
    "    elif isinstance(layer, nn.Embedding):\n",
    "        # Embedding(num_embeddings, embedding_dim): num*dim\n",
    "        return layer.num_embeddings * layer.embedding_dim\n",
    "    elif isinstance(layer, nn.LSTM):\n",
    "        # LSTM has 4 gates, each with input and hidden weights + bias\n",
    "        input_size, hidden_size = layer.input_size, layer.hidden_size\n",
    "        num_layers = layer.num_layers\n",
    "        bidirectional = 2 if layer.bidirectional else 1\n",
    "        \n",
    "        # Per layer: 4 gates * (input_weights + hidden_weights + 2*bias)\n",
    "        # Note: LSTM has input bias and hidden bias for each gate\n",
    "        per_layer = 4 * (input_size * hidden_size + hidden_size * hidden_size + 2 * hidden_size)\n",
    "        \n",
    "        # First layer uses input_size, subsequent layers use hidden_size as input\n",
    "        if num_layers > 1:\n",
    "            first_layer = 4 * (input_size * hidden_size + hidden_size * hidden_size + 2 * hidden_size)\n",
    "            other_layers = (num_layers - 1) * 4 * (hidden_size * hidden_size + hidden_size * hidden_size + 2 * hidden_size)\n",
    "            return (first_layer + other_layers) * bidirectional\n",
    "        else:\n",
    "            return per_layer * bidirectional\n",
    "    else:\n",
    "        return sum(p.numel() for p in layer.parameters())\n",
    "\n",
    "# Examples with verification\n",
    "print(\"=== Parameter Counting Examples ===\")\n",
    "\n",
    "# Linear layer\n",
    "linear = nn.Linear(100, 50)\n",
    "calculated = count_parameters(linear)\n",
    "actual = sum(p.numel() for p in linear.parameters())\n",
    "print(f\"Linear(100, 50):\")\n",
    "print(f\"  Calculated: {calculated} (100*50 + 50 = {100*50 + 50})\")\n",
    "print(f\"  Actual: {actual}\")\n",
    "print(f\"  Match: {calculated == actual}\")\n",
    "\n",
    "# Embedding layer\n",
    "embedding = nn.Embedding(1000, 128)\n",
    "calculated = count_parameters(embedding)\n",
    "actual = sum(p.numel() for p in embedding.parameters())\n",
    "print(f\"\\nEmbedding(1000, 128):\")\n",
    "print(f\"  Calculated: {calculated} (1000*128 = {1000*128})\")\n",
    "print(f\"  Actual: {actual}\")\n",
    "print(f\"  Match: {calculated == actual}\")\n",
    "\n",
    "# LSTM layer\n",
    "lstm = nn.LSTM(64, 32, num_layers=1, batch_first=True)\n",
    "calculated = count_parameters(lstm)\n",
    "actual = sum(p.numel() for p in lstm.parameters())\n",
    "print(f\"\\nLSTM(64, 32, layers=1):\")\n",
    "print(f\"  Calculated: {calculated}\")\n",
    "print(f\"  Actual: {actual}\")\n",
    "print(f\"  Match: {calculated == actual}\")\n",
    "\n",
    "# Multi-layer LSTM\n",
    "lstm_multi = nn.LSTM(64, 32, num_layers=2, batch_first=True)\n",
    "calculated_multi = count_parameters(lstm_multi)\n",
    "actual_multi = sum(p.numel() for p in lstm_multi.parameters())\n",
    "print(f\"\\nLSTM(64, 32, layers=2):\")\n",
    "print(f\"  Calculated: {calculated_multi}\")\n",
    "print(f\"  Actual: {actual_multi}\")\n",
    "print(f\"  Match: {calculated_multi == actual_multi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter analysis for complete model\n",
    "def analyze_model_parameters(model):\n",
    "    \"\"\"Detailed parameter analysis\"\"\"\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    \n",
    "    print(\"Parameter breakdown:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            trainable_params += param_count\n",
    "            status = \"Trainable\"\n",
    "        else:\n",
    "            status = \"Frozen\"\n",
    "        \n",
    "        print(f\"{name:20s}: {str(param.shape):15s} {param_count:8,d} ({status})\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Total parameters:':<20s} {total_params:>15,d}\")\n",
    "    print(f\"{'Trainable parameters:':<20s} {trainable_params:>15,d}\")\n",
    "    print(f\"{'Non-trainable:':<20s} {total_params - trainable_params:>15,d}\")\n",
    "    \n",
    "    # Memory estimation (rough)\n",
    "    param_memory_mb = total_params * 4 / (1024**2)  # 4 bytes per float32\n",
    "    print(f\"{'Estimated memory:':<20s} {param_memory_mb:>12.1f} MB\")\n",
    "\n",
    "# Analyze our simple model\n",
    "analyze_model_parameters(model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Create a more complex model for comparison\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(5000, 128)\n",
    "        self.lstm = nn.LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        self.attention = nn.MultiheadAttention(256, 8, batch_first=True)\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        x = self.classifier(x.mean(dim=1))  # Global average pooling\n",
    "        return x\n",
    "\n",
    "complex_model = ComplexModel()\n",
    "print(\"Complex model analysis:\")\n",
    "analyze_model_parameters(complex_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Initialization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different initialization strategies\n",
    "def xavier_init(m):\n",
    "    \"\"\"Xavier (Glorot) initialization - good for tanh/sigmoid activations\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def kaiming_init(m):\n",
    "    \"\"\"Kaiming (He) initialization - good for ReLU activations\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.normal_(m.weight, std=0.1)\n",
    "\n",
    "def normal_init(m):\n",
    "    \"\"\"Simple normal initialization\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.normal_(m.weight, std=0.1)\n",
    "\n",
    "# Compare initialization effects\n",
    "def compare_initializations():\n",
    "    \"\"\"Compare different initialization strategies\"\"\"\n",
    "    \n",
    "    # Create three identical models\n",
    "    model_xavier = SimpleNet(100, 50, 10)\n",
    "    model_kaiming = SimpleNet(100, 50, 10)\n",
    "    model_normal = SimpleNet(100, 50, 10)\n",
    "    \n",
    "    # Apply different initializations\n",
    "    model_xavier.apply(xavier_init)\n",
    "    model_kaiming.apply(kaiming_init)\n",
    "    model_normal.apply(normal_init)\n",
    "    \n",
    "    models = {\n",
    "        'Xavier (Glorot)': model_xavier,\n",
    "        'Kaiming (He)': model_kaiming,\n",
    "        'Normal (0.01)': model_normal\n",
    "    }\n",
    "    \n",
    "    # Test with random input\n",
    "    x = torch.randn(32, 100)  # Batch of 32 samples\n",
    "    \n",
    "    print(\"Initialization comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.eval()  # Set to eval mode for consistent comparison\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(x)\n",
    "            \n",
    "        # Analyze weight statistics\n",
    "        fc1_weight = model.fc1.weight\n",
    "        fc2_weight = model.fc2.weight\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  FC1 weight stats: mean={fc1_weight.mean().item():.6f}, std={fc1_weight.std().item():.6f}\")\n",
    "        print(f\"  FC2 weight stats: mean={fc2_weight.mean().item():.6f}, std={fc2_weight.std().item():.6f}\")\n",
    "        print(f\"  Output stats: mean={output.mean().item():.6f}, std={output.std().item():.6f}\")\n",
    "        print(f\"  Output range: [{output.min().item():.6f}, {output.max().item():.6f}]\")\n",
    "\n",
    "compare_initializations()\n",
    "\n",
    "# Demonstrate the impact of bad initialization\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Effect of bad initialization:\")\n",
    "\n",
    "def bad_init(m):\n",
    "    \"\"\"Intentionally bad initialization\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.constant_(m.weight, 10.0)  # Too large!\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "model_bad = SimpleNet(10, 20, 5)\n",
    "model_bad.apply(bad_init)\n",
    "\n",
    "x = torch.randn(5, 10)\n",
    "with torch.no_grad():\n",
    "    output_bad = model_bad(x)\n",
    "    \n",
    "print(f\"Bad initialization output stats:\")\n",
    "print(f\"  Mean: {output_bad.mean().item():.2f}\")\n",
    "print(f\"  Std: {output_bad.std().item():.2f}\")\n",
    "print(f\"  Range: [{output_bad.min().item():.2f}, {output_bad.max().item():.2f}]\")\n",
    "print(f\"  Contains NaN: {torch.isnan(output_bad).any().item()}\")\n",
    "print(\"\\nNote: Large values can lead to vanishing/exploding gradients!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight distributions after initialization\n",
    "def visualize_weight_distributions():\n",
    "    # Create models with different initializations\n",
    "    model_xavier = nn.Linear(100, 100)\n",
    "    model_kaiming = nn.Linear(100, 100)\n",
    "    model_normal = nn.Linear(100, 100)\n",
    "    \n",
    "    nn.init.xavier_uniform_(model_xavier.weight)\n",
    "    nn.init.kaiming_uniform_(model_kaiming.weight, nonlinearity='relu')\n",
    "    nn.init.normal_(model_normal.weight, std=0.01)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    weights = {\n",
    "        'Xavier': model_xavier.weight.detach().numpy().flatten(),\n",
    "        'Kaiming': model_kaiming.weight.detach().numpy().flatten(),\n",
    "        'Normal (0.01)': model_normal.weight.detach().numpy().flatten()\n",
    "    }\n",
    "    \n",
    "    for i, (name, weight) in enumerate(weights.items()):\n",
    "        axes[i].hist(weight, bins=50, alpha=0.7, density=True)\n",
    "        axes[i].set_title(f'{name} Initialization')\n",
    "        axes[i].set_xlabel('Weight Value')\n",
    "        axes[i].set_ylabel('Density')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean, std = weight.mean(), weight.std()\n",
    "        axes[i].axvline(mean, color='red', linestyle='--', label=f'Mean: {mean:.4f}')\n",
    "        axes[i].legend()\n",
    "        axes[i].set_title(f'{name}\\nMean: {mean:.4f}, Std: {std:.4f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key insights:\")\n",
    "    print(\"• Xavier: Balanced for tanh/sigmoid activations\")\n",
    "    print(\"• Kaiming: Wider distribution for ReLU activations\")\n",
    "    print(\"• Normal (0.01): Very narrow, might cause vanishing gradients\")\n",
    "\n",
    "visualize_weight_distributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Advanced Module Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom module with learnable parameters\n",
    "class CustomLinear(nn.Module):\n",
    "    \"\"\"Custom linear layer to demonstrate parameter creation\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Create learnable parameters\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        else:\n",
    "            # Register as None so it doesn't appear in parameters()\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Manual implementation of linear transformation\n",
    "        output = torch.matmul(x, self.weight.t())\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        return output\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        # Custom string representation\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'\n",
    "\n",
    "# Test custom linear layer\n",
    "import math\n",
    "\n",
    "custom_linear = CustomLinear(10, 5)\n",
    "builtin_linear = nn.Linear(10, 5)\n",
    "\n",
    "print(\"Custom linear layer:\")\n",
    "print(custom_linear)\n",
    "print(f\"Parameters: {sum(p.numel() for p in custom_linear.parameters())}\")\n",
    "\n",
    "# Test they produce similar results\n",
    "x = torch.randn(3, 10)\n",
    "output_custom = custom_linear(x)\n",
    "output_builtin = builtin_linear(x)\n",
    "\n",
    "print(f\"\\nOutput shapes - Custom: {output_custom.shape}, Built-in: {output_builtin.shape}\")\n",
    "print(\"Custom and built-in linear layers work equivalently!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module with submodules and parameter sharing\n",
    "class ModularNet(nn.Module):\n",
    "    \"\"\"Demonstrate modular architecture and parameter sharing\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Shared transformation (parameter sharing)\n",
    "        self.shared_transform = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Layer-specific transformations\n",
    "        self.layer_transforms = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization for each layer\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input projection\n",
    "        x = F.relu(self.input_proj(x))\n",
    "        \n",
    "        # Process through layers\n",
    "        for i in range(self.num_layers):\n",
    "            residual = x\n",
    "            \n",
    "            # Apply shared transformation (parameter sharing across layers)\n",
    "            x = self.shared_transform(x)\n",
    "            \n",
    "            # Apply layer-specific transformation\n",
    "            x = self.layer_transforms[i](x)\n",
    "            \n",
    "            # Residual connection and layer norm\n",
    "            x = self.layer_norms[i](x + residual)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.output_proj(x)\n",
    "        return x\n",
    "\n",
    "# Create and analyze modular network\n",
    "modular_net = ModularNet(input_size=64, hidden_size=128, num_layers=4)\n",
    "\n",
    "print(\"Modular Network Architecture:\")\n",
    "print(modular_net)\n",
    "\n",
    "print(\"\\nParameter analysis:\")\n",
    "total_params = 0\n",
    "for name, param in modular_net.named_parameters():\n",
    "    print(f\"{name:30s}: {str(param.shape):20s} {param.numel():>8,d}\")\n",
    "    total_params += param.numel()\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Note: shared_transform parameters are used across all layers\n",
    "print(f\"\\nShared parameters (used {modular_net.num_layers} times): {modular_net.shared_transform.weight.numel() + modular_net.shared_transform.bias.numel():,}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(5, 64)\n",
    "output = modular_net(x)\n",
    "print(f\"\\nForward pass: {x.shape} -> {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hooks for monitoring activations and gradients\n",
    "class MonitoredNet(nn.Module):\n",
    "    \"\"\"Network with built-in monitoring capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Storage for activations and gradients\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "        \n",
    "        # Register hooks\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        def save_activation(name):\n",
    "            def hook(module, input, output):\n",
    "                self.activations[name] = output.detach()\n",
    "            return hook\n",
    "        \n",
    "        def save_gradient(name):\n",
    "            def hook(module, grad_input, grad_output):\n",
    "                if grad_output[0] is not None:\n",
    "                    self.gradients[name] = grad_output[0].detach()\n",
    "            return hook\n",
    "        \n",
    "        # Register forward and backward hooks\n",
    "        self.fc1.register_forward_hook(save_activation('fc1'))\n",
    "        self.fc2.register_forward_hook(save_activation('fc2'))\n",
    "        self.fc3.register_forward_hook(save_activation('fc3'))\n",
    "        \n",
    "        self.fc1.register_backward_hook(save_gradient('fc1'))\n",
    "        self.fc2.register_backward_hook(save_gradient('fc2'))\n",
    "        self.fc3.register_backward_hook(save_gradient('fc3'))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def get_activation_stats(self):\n",
    "        \"\"\"Get statistics about activations\"\"\"\n",
    "        stats = {}\n",
    "        for name, activation in self.activations.items():\n",
    "            stats[name] = {\n",
    "                'mean': activation.mean().item(),\n",
    "                'std': activation.std().item(),\n",
    "                'min': activation.min().item(),\n",
    "                'max': activation.max().item(),\n",
    "                'zeros': (activation == 0).float().mean().item()  # Sparsity for ReLU\n",
    "            }\n",
    "        return stats\n",
    "    \n",
    "    def get_gradient_stats(self):\n",
    "        \"\"\"Get statistics about gradients\"\"\"\n",
    "        stats = {}\n",
    "        for name, gradient in self.gradients.items():\n",
    "            stats[name] = {\n",
    "                'mean': gradient.mean().item(),\n",
    "                'std': gradient.std().item(),\n",
    "                'norm': gradient.norm().item()\n",
    "            }\n",
    "        return stats\n",
    "\n",
    "# Test monitored network\n",
    "monitored_net = MonitoredNet(20, 50, 5)\n",
    "optimizer = optim.SGD(monitored_net.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Forward and backward pass\n",
    "x = torch.randn(10, 20)\n",
    "y_true = torch.randn(10, 5)\n",
    "\n",
    "# Forward pass\n",
    "y_pred = monitored_net(x)\n",
    "loss = criterion(y_pred, y_true)\n",
    "\n",
    "# Backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Analyze activations and gradients\n",
    "print(\"Activation Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "activation_stats = monitored_net.get_activation_stats()\n",
    "for layer, stats in activation_stats.items():\n",
    "    print(f\"{layer}:\")\n",
    "    print(f\"  Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n",
    "    print(f\"  Range: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
    "    print(f\"  Sparsity (zeros): {stats['zeros']:.2%}\")\n",
    "\n",
    "print(\"\\nGradient Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "gradient_stats = monitored_net.get_gradient_stats()\n",
    "for layer, stats in gradient_stats.items():\n",
    "    print(f\"{layer}:\")\n",
    "    print(f\"  Mean: {stats['mean']:.6f}, Std: {stats['std']:.6f}\")\n",
    "    print(f\"  Norm: {stats['norm']:.6f}\")\n",
    "\n",
    "print(\"\\n🎉 Module exploration completed!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"• Use nn.Module as base class for all models\")\n",
    "print(\"• Parameters are automatically tracked when using nn.Parameter\")\n",
    "print(\"• Proper initialization is crucial for training success\")\n",
    "print(\"• ModuleList and ModuleDict help organize complex architectures\")\n",
    "print(\"• Hooks enable monitoring and debugging of model internals\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}