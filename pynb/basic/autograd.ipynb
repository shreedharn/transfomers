{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch Autograd: Automatic Differentiation\n\n## ðŸŽ¯ Introduction\n\nWelcome to the magic of automatic differentiation! Autograd is PyTorch's secret weapon that makes neural network training possible. Without it, we'd be stuck computing gradients by hand for every parameter in our models - imagine doing that for GPT-3's 175 billion parameters!\n\n### ðŸ§  What You'll Learn\n\nThis notebook will teach you:\n- **Gradient computation**: How PyTorch automatically computes derivatives\n- **Training integration**: How autograd powers neural network optimization\n- **Memory management**: When to use `torch.no_grad()` and `.detach()`\n- **Gradient clipping**: Preventing exploding gradients in deep networks\n- **Advanced patterns**: Higher-order derivatives and optimization tricks\n\n### ðŸŽ“ Prerequisites\n\n- Basic calculus (derivatives, chain rule)\n- Understanding of tensors from the previous notebook\n- Basic knowledge of neural network training concepts\n\n### ðŸš€ Why Autograd Matters\n\nAutograd is revolutionary because:\n- **Eliminates manual differentiation**: No more error-prone hand calculations\n- **Handles any computation graph**: Works with any combination of operations\n- **Efficient backpropagation**: Optimized for the computations you actually use\n- **Dynamic graphs**: Change your network structure on-the-fly\n\n---\n\n## ðŸ“š Table of Contents\n\n1. **[Basic Gradient Computation](#basic-gradient-computation)** - Understanding the fundamentals\n2. **[Optimizer Integration](#optimizer-integration)** - How gradients drive learning\n3. **[No-Grad Context and Detach](#no-grad-context-and-detach)** - Memory optimization techniques\n4. **[Gradient Clipping Demo](#gradient-clipping-demo)** - Preventing training instabilities"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function: f(x) = x^2 + 2x + 1\n",
    "# Derivative: f'(x) = 2x + 2\n",
    "\n",
    "x = torch.tensor(3.0, requires_grad=True)  # Enable gradient computation\n",
    "print(f\"x = {x.item()}, requires_grad = {x.requires_grad}\")\n",
    "\n",
    "# Forward pass\n",
    "y = x**2 + 2*x + 1\n",
    "print(f\"y = f(x) = x^2 + 2x + 1 = {y.item()}\")\n",
    "\n",
    "# Backward pass (compute gradient)\n",
    "y.backward()\n",
    "\n",
    "print(f\"dy/dx = {x.grad.item()}\")\n",
    "print(f\"Analytical derivative at x=3: 2*3 + 2 = {2*3 + 2}\")\n",
    "print(f\"Match: {abs(x.grad.item() - (2*3 + 2)) < 1e-6}\")\n",
    "\n",
    "# Multi-variable function\n",
    "print(\"\\n=== Multi-variable Example ===\")\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# f(x,y) = x^2*y + x*y^2\n",
    "z = x**2 * y + x * y**2\n",
    "print(f\"f(1,2) = 1^2*2 + 1*2^2 = {z.item()}\")\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(f\"âˆ‚f/âˆ‚x = {x.grad.item()} (should be 2xy + y^2 = 2*1*2 + 2^2 = {2*1*2 + 2**2})\")\n",
    "print(f\"âˆ‚f/âˆ‚y = {y.grad.item()} (should be x^2 + 2xy = 1^2 + 2*1*2 = {1**2 + 2*1*2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain rule in action\n",
    "print(\"=== Chain Rule Demo ===\")\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Composite function: f(g(h(x))) where h(x)=x^2, g(u)=u+1, f(v)=v^3\n",
    "h = x**2        # h(x) = x^2, h'(x) = 2x\n",
    "g = h + 1       # g(h) = h + 1, g'(h) = 1\n",
    "f = g**3        # f(g) = g^3, f'(g) = 3g^2\n",
    "\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"h(x) = x^2 = {h.item()}\")\n",
    "print(f\"g(h) = h + 1 = {g.item()}\")\n",
    "print(f\"f(g) = g^3 = {f.item()}\")\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(f\"\\nComputed gradient: df/dx = {x.grad.item()}\")\n",
    "\n",
    "# Manual chain rule: df/dx = (df/dg) * (dg/dh) * (dh/dx)\n",
    "# df/dg = 3g^2 = 3*(x^2 + 1)^2\n",
    "# dg/dh = 1\n",
    "# dh/dx = 2x\n",
    "# So df/dx = 3*(x^2 + 1)^2 * 1 * 2x = 6x*(x^2 + 1)^2\n",
    "manual_grad = 6 * x.item() * (x.item()**2 + 1)**2\n",
    "print(f\"Manual chain rule: df/dx = 6x(x^2 + 1)^2 = {manual_grad}\")\n",
    "print(f\"Match: {abs(x.grad.item() - manual_grad) < 1e-6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple optimization example: minimize f(x) = (x - 5)^2\n",
    "print(\"=== Optimization Example ===\")\n",
    "print(\"Minimizing f(x) = (x - 5)^2 using gradient descent\")\n",
    "\n",
    "# Starting point\n",
    "x = torch.tensor(0.0, requires_grad=True)\n",
    "optimizer = optim.SGD([x], lr=0.1)\n",
    "\n",
    "# Track progress\n",
    "history = []\n",
    "\n",
    "for step in range(20):\n",
    "    # Zero gradients (important!)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    loss = (x - 5)**2\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track progress\n",
    "    history.append((step, x.item(), loss.item()))\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step:2d}: x = {x.item():.4f}, loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal result: x = {x.item():.4f} (target: 5.0)\")\n",
    "print(f\"Error: {abs(x.item() - 5.0):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization progress\n",
    "steps, x_vals, losses = zip(*history)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot parameter evolution\n",
    "ax1.plot(steps, x_vals, 'b-o', markersize=4)\n",
    "ax1.axhline(y=5.0, color='r', linestyle='--', label='Target (x=5)')\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('x value')\n",
    "ax1.set_title('Parameter Convergence')\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot loss evolution\n",
    "ax2.plot(steps, losses, 'r-o', markersize=4)\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Loss Convergence')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Optimization completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No-Grad Context and Detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== No-Grad Context ===\")\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Normal operation (gradients tracked)\n",
    "y1 = x**2\n",
    "print(f\"y1 = x^2, requires_grad = {y1.requires_grad}\")\n",
    "\n",
    "# Using torch.no_grad() context\n",
    "with torch.no_grad():\n",
    "    y2 = x**2\n",
    "    print(f\"y2 = x^2 (in no_grad), requires_grad = {y2.requires_grad}\")\n",
    "\n",
    "# Using .detach() method\n",
    "y3 = x.detach()**2\n",
    "print(f\"y3 = x.detach()^2, requires_grad = {y3.requires_grad}\")\n",
    "\n",
    "# Practical example: evaluation mode\n",
    "print(\"\\n=== Practical Example: Model Evaluation ===\")\n",
    "\n",
    "model = nn.Linear(5, 1)\n",
    "x_data = torch.randn(10, 5)\n",
    "\n",
    "# During training (gradients needed)\n",
    "model.train()\n",
    "output_train = model(x_data)\n",
    "print(f\"Training mode - output requires_grad: {output_train.requires_grad}\")\n",
    "\n",
    "# During evaluation (no gradients needed, saves memory)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_eval = model(x_data)\n",
    "    print(f\"Eval mode - output requires_grad: {output_eval.requires_grad}\")\n",
    "\n",
    "print(\"\\nUsing no_grad during evaluation:\")\n",
    "print(\"âœ“ Saves memory (no gradient computation graph)\")\n",
    "print(\"âœ“ Faster inference\")\n",
    "print(\"âœ“ Prevents accidental gradient updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Gradient Clipping Demo ===\")\n",
    "\n",
    "# Create a simple model that might have exploding gradients\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        # Initialize with large weights to cause gradient explosion\n",
    "        nn.init.constant_(self.linear.weight, 10.0)\n",
    "        nn.init.constant_(self.linear.bias, 0.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Create some data that will cause large gradients\n",
    "x = torch.tensor([[10.0, 10.0], [5.0, 5.0], [-10.0, -10.0]])\n",
    "y_true = torch.tensor([[1.0], [0.5], [-1.0]])\n",
    "\n",
    "print(\"Training without gradient clipping:\")\n",
    "gradient_norms = []\n",
    "\n",
    "for step in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = nn.functional.mse_loss(y_pred, y_true)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Calculate gradient norm before clipping\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    gradient_norms.append(total_norm)\n",
    "    \n",
    "    print(f\"Step {step}: Loss = {loss.item():.4f}, Grad norm = {total_norm:.4f}\")\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"\\nAverage gradient norm: {np.mean(gradient_norms):.4f}\")\n",
    "\n",
    "# Reset model\n",
    "model = SimpleModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(\"\\nTraining WITH gradient clipping (max_norm=1.0):\")\n",
    "clipped_gradient_norms = []\n",
    "\n",
    "for step in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = nn.functional.mse_loss(y_pred, y_true)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    # Calculate gradient norm after clipping\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    clipped_gradient_norms.append(total_norm)\n",
    "    \n",
    "    print(f\"Step {step}: Loss = {loss.item():.4f}, Clipped grad norm = {total_norm:.4f}\")\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"\\nAverage clipped gradient norm: {np.mean(clipped_gradient_norms):.4f}\")\n",
    "print(\"\\nGradient clipping benefits:\")\n",
    "print(\"âœ“ Prevents exploding gradients\")\n",
    "print(\"âœ“ Stabilizes training\")\n",
    "print(\"âœ“ Allows higher learning rates\")\n",
    "print(\"âœ“ Essential for RNNs and deep networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced autograd example: Higher-order derivatives\n",
    "print(\"=== Higher-Order Derivatives ===\")\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# f(x) = x^4\n",
    "y = x**4\n",
    "\n",
    "# First derivative: f'(x) = 4x^3\n",
    "grad1 = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "print(f\"f(x) = x^4 = {y.item()}\")\n",
    "print(f\"f'(x) = 4x^3 = {grad1.item()} (analytical: {4 * x.item()**3})\")\n",
    "\n",
    "# Second derivative: f''(x) = 12x^2\n",
    "grad2 = torch.autograd.grad(grad1, x, create_graph=True)[0]\n",
    "print(f\"f''(x) = 12x^2 = {grad2.item()} (analytical: {12 * x.item()**2})\")\n",
    "\n",
    "# Third derivative: f'''(x) = 24x\n",
    "grad3 = torch.autograd.grad(grad2, x, create_graph=True)[0]\n",
    "print(f\"f'''(x) = 24x = {grad3.item()} (analytical: {24 * x.item()})\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Autograd exploration completed!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"â€¢ requires_grad=True enables gradient tracking\")\n",
    "print(\"â€¢ .backward() computes gradients\")\n",
    "print(\"â€¢ optimizer.zero_grad() clears old gradients\")\n",
    "print(\"â€¢ torch.no_grad() disables gradient computation\")\n",
    "print(\"â€¢ Gradient clipping prevents exploding gradients\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}